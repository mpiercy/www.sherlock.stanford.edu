{
    "docs": [
        {
            "location": "/docs/", 
            "text": "Sherlock \ndocumentation\n#\n\n\n\n\n\n\n@media only screen and (max-width: 720px) {\n    #logo_head {\n        display: none;\n    }\n}\n#logo_head {\n    margin-top: -50px;\n}\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nThis guide is a work in progress and is not complete yet. We are actively\nworking on adding more content and information.\n\n\n\n\nWelcome to Sherlock!\n#\n\n\nSherlock is a High-Performance Computing (HPC) cluster, operated by the\n\nStanford Research Computing Center\n to provide computing resources\nto the Stanford community at large. You'll find all the documentation, tips,\nFAQs and information about Sherlock among these pages.\n\n\n\n\nSherlock 1.0\n\n\nThese pages refer to Sherlock 2.0, the new iteration of the Sherlock\ncluster.  For anything specific to Sherlock 1.0, please see the previous\n\nwiki\n.\n\n\nThe older Sherlock system will remain in production until all of its nodes\nhave been merged into Sherlock 2.0.  Until then, the two systems will\ncoexist for an extended transition period. The \nSherlock 2.0 transition\nguide\n includes material intended to help users manage\ntheir transition between the two systems: differences, temporary\nconditions, etc.\n\n\n\n\nFeel free to explore the different sections. If some information is missing,\nplease \ncontact us\n to suggest additions or modifications.\n\n\nInformation sources\n#\n\n\n\n\nSearching the docs\n\n\nIf you're looking for information on a specific topic, the Search feature\nof this site will allow you to quickly find the page you're looking for.\nJust press \nS\n or \nF\n to open the Search bar and start typing.\n\n\n\n\nTo help users take their first steps on Sherlock, we provide documentation and\ninformation through various channels:\n\n\n\n\n\n\n\n\nChannel\n\n\nURL\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nDocumentation\n \nYou are here\n\n\nwww.sherlock.stanford.edu/docs\n\n\ninformation to help new users start on Sherlock, and more in-depth documentation for users already familiar with the environment.\n\n\n\n\n\n\nChangelog\n\n\nnews.sherlock.stanford.edu\n\n\nannounces, news and updates about Sherlock.\n\n\n\n\n\n\nDashboard\n\n\nstatus.sherlock.stanford.edu\n\n\nstatus of Sherlock's main components and services, outages, maintenances.\n\n\n\n\n\n\n\n\nTo get started, you can take a look at the \nconcepts\n and\n\nglossary\n pages to get familiar with the terminology used\nthroughout the documentation pages. Then, we recommend going through the\nfollowing sections:\n\n\n\n\nPrerequisites\n\n\nConnecting to the cluster\n\n\nSubmitting jobs\n\n\n\n\nAcknowledgment / citation\n#\n\n\nIt is important and expected that publications resulting from computations\nperformed on Sherlock acknowledge this. The following wording is suggested:\n\n\n\n\nAcknowledgment\n\n\nSome of the computing for this project was performed on the Sherlock\ncluster.  We would like to thank Stanford University and the Stanford\nResearch Computing Center for providing computational resources and support\nthat contributed to these research results.\n\n\n\n\nSupport\n#\n\n\nEmail \n(recommended)\n#\n\n\nResearch Computing support can be reached by sending an email to\n\n and \nmentioning Sherlock\n.\n\n\n\n\nHow to submit effective support requests\n\n\nTo ensure a timely and relevant response, please make sure to include some\nadditional details, such as job ids, commands executed and error messages\nreceived, so we can help you better.  For more details, see the\n\nTroubleshooting\n page.\n\n\n\n\nAs a member of the Sherlock community, you're also automatically subscribed to:\n\n\n\n\nthe \nsherlock-announce\n mailing-list, which is only used by the\n  SRCC team to send   important announcements about Sherlock,\n\n\nthe \nsherlock-users\n mailing list\n1\n, which is used to\n  connect Sherlock users together. If you have a general\n  question about software used on Sherlock, want to reach out to the other\n  Sherlock users to share tips and good practices, tutorials or other info,\n  please feel free to do so through this mailing-list.\n\n\n\n\nOffice hours\n#\n\n\n\n\nWe can't accommodate walk-ins\n\n\nWe're unfortunately not staffed to accommodate walk-ins, so please make\nsure that you're planning to stop by \nduring\n office hours. We may not be\nable to help you otherwise.\n\n\n\n\nThe SRCC holds office hours twice a week, in \nPolya Hall\n, room 255\n(2\nnd\n floor):\n\n\n\n\nTuesdays, 10-11am\n\n\nThursdays 3-4pm\n\n\n\n\nPlease feel free to stop by if you have any question or trouble using Sherlock,\nwe'll be happy to help you.\n\n\nQuick Start\n#\n\n\nIf you're in a rush, here's a 3-step ultra-quick start:\n\n\n\n\n\n\nconnect to Sherlock\n   \n$ ssh login.sherlock.stanford.edu\n\n\n\n\n\n\nget an interactive session on a compute node\n   \n[kilian@sh-ln01 login! ~]$ sdev\n\n\n\n\n\n\nrun a command\n   \n[kilian@sh-101-58 ~]$ module load python\n[kilian@sh-101-58 ~]$ python -c \nprint('Hello Sherlock')\n\nHello Sherlock\n\n\n\n\n\n\nCongrats! You ran your first job on Sherlock!\n\n\nHere's what it looks like in motion:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis mailing-list is moderated.", 
            "title": "Introduction"
        }, 
        {
            "location": "/docs/#sherlock-documentation", 
            "text": "@media only screen and (max-width: 720px) {\n    #logo_head {\n        display: none;\n    }\n}\n#logo_head {\n    margin-top: -50px;\n}     Warning  This guide is a work in progress and is not complete yet. We are actively\nworking on adding more content and information.", 
            "title": "Sherlock documentation"
        }, 
        {
            "location": "/docs/#welcome-to-sherlock", 
            "text": "Sherlock is a High-Performance Computing (HPC) cluster, operated by the Stanford Research Computing Center  to provide computing resources\nto the Stanford community at large. You'll find all the documentation, tips,\nFAQs and information about Sherlock among these pages.   Sherlock 1.0  These pages refer to Sherlock 2.0, the new iteration of the Sherlock\ncluster.  For anything specific to Sherlock 1.0, please see the previous wiki .  The older Sherlock system will remain in production until all of its nodes\nhave been merged into Sherlock 2.0.  Until then, the two systems will\ncoexist for an extended transition period. The  Sherlock 2.0 transition\nguide  includes material intended to help users manage\ntheir transition between the two systems: differences, temporary\nconditions, etc.   Feel free to explore the different sections. If some information is missing,\nplease  contact us  to suggest additions or modifications.", 
            "title": "Welcome to Sherlock!"
        }, 
        {
            "location": "/docs/#information-sources", 
            "text": "Searching the docs  If you're looking for information on a specific topic, the Search feature\nof this site will allow you to quickly find the page you're looking for.\nJust press  S  or  F  to open the Search bar and start typing.   To help users take their first steps on Sherlock, we provide documentation and\ninformation through various channels:     Channel  URL  Purpose      Documentation   You are here  www.sherlock.stanford.edu/docs  information to help new users start on Sherlock, and more in-depth documentation for users already familiar with the environment.    Changelog  news.sherlock.stanford.edu  announces, news and updates about Sherlock.    Dashboard  status.sherlock.stanford.edu  status of Sherlock's main components and services, outages, maintenances.     To get started, you can take a look at the  concepts  and glossary  pages to get familiar with the terminology used\nthroughout the documentation pages. Then, we recommend going through the\nfollowing sections:   Prerequisites  Connecting to the cluster  Submitting jobs", 
            "title": "Information sources"
        }, 
        {
            "location": "/docs/#acknowledgment-citation", 
            "text": "It is important and expected that publications resulting from computations\nperformed on Sherlock acknowledge this. The following wording is suggested:   Acknowledgment  Some of the computing for this project was performed on the Sherlock\ncluster.  We would like to thank Stanford University and the Stanford\nResearch Computing Center for providing computational resources and support\nthat contributed to these research results.", 
            "title": "Acknowledgment / citation"
        }, 
        {
            "location": "/docs/#support", 
            "text": "", 
            "title": "Support"
        }, 
        {
            "location": "/docs/#email-recommended", 
            "text": "Research Computing support can be reached by sending an email to  and  mentioning Sherlock .   How to submit effective support requests  To ensure a timely and relevant response, please make sure to include some\nadditional details, such as job ids, commands executed and error messages\nreceived, so we can help you better.  For more details, see the Troubleshooting  page.   As a member of the Sherlock community, you're also automatically subscribed to:   the  sherlock-announce  mailing-list, which is only used by the\n  SRCC team to send   important announcements about Sherlock,  the  sherlock-users  mailing list 1 , which is used to\n  connect Sherlock users together. If you have a general\n  question about software used on Sherlock, want to reach out to the other\n  Sherlock users to share tips and good practices, tutorials or other info,\n  please feel free to do so through this mailing-list.", 
            "title": "Email (recommended)"
        }, 
        {
            "location": "/docs/#office-hours", 
            "text": "We can't accommodate walk-ins  We're unfortunately not staffed to accommodate walk-ins, so please make\nsure that you're planning to stop by  during  office hours. We may not be\nable to help you otherwise.   The SRCC holds office hours twice a week, in  Polya Hall , room 255\n(2 nd  floor):   Tuesdays, 10-11am  Thursdays 3-4pm   Please feel free to stop by if you have any question or trouble using Sherlock,\nwe'll be happy to help you.", 
            "title": "Office hours"
        }, 
        {
            "location": "/docs/#quick-start", 
            "text": "If you're in a rush, here's a 3-step ultra-quick start:    connect to Sherlock\n    $ ssh login.sherlock.stanford.edu    get an interactive session on a compute node\n    [kilian@sh-ln01 login! ~]$ sdev    run a command\n    [kilian@sh-101-58 ~]$ module load python\n[kilian@sh-101-58 ~]$ python -c  print('Hello Sherlock') \nHello Sherlock    Congrats! You ran your first job on Sherlock!  Here's what it looks like in motion:       This mailing-list is moderated.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/docs/overview/concepts/", 
            "text": "Sherlock, a shared resource\n#\n\n\nSherlock is a shared compute cluster available for use by all Stanford faculty\nmembers and their research teams to support sponsored research.\n\n\n\n\nSherlock is a resource for research\n\n\nSherlock is not suitable for course work, class assignments or general-use\ntraining sessions.\n\n\n\n\nIt is open to the Stanford community as a computing resource to support\ndepartmental or sponsored research, thus a faculty member's sponsorship is\nrequired for all user accounts.\n\n\n\n\nUsage policy\n\n\nPlease note that your use of this system falls under the \"Computer and\nNetwork Usage Policy\", as described in the \nStanford Administrative\nGuide\n. In particular, sharing authentication credentials is\nstrictly prohibited.  Violation of this policy will result in termination\nof access to Sherlock.\n\n\n\n\nSherlock has been designed, deployed, and is maintained and operated by the\n\nStanford Research Computing Center (SRCC)\n staff. The SRCC is a\njoint effort of the \nDean of Research\n and \nIT Services\n to\nbuild and support a comprehensive program to advance computational research at\nStanford.\n\n\nSherlock has been initially purchased and supported with seed funding from\nStanford's \nProvost\n. It comprises a set of freely available\ncompute nodes, a few specific resources such as large-memory machines and GPU\nservers, as well as the associated networking equipment and storage.  These\nresources can be used to run computational codes and programs, and are managed\nthrough a job scheduler using a \nfair-share algorithm\n.\n\n\nData risk classification\n#\n\n\n\n\nLow and Moderate Risk data\n\n\nSherlock is approved for computing with Low and Moderate Risk data only.\n\n\n\n\n\n\nHigh Risk data\n\n\nSherlock is \nNOT\n \nHIPAA\n compliant and shouldn't be used to\nprocess \nPHI\n nor \nPII\n.  The system is approved for\ncomputing with Low and Moderate Risk data only, and is \nnot suitable to\nprocess High Risk data\n.  For more information about data risk\nclassifications, see the \nInformation Security Risk Classification\npage\n.\n\n\n\n\nWhat's a cluster?\n#\n\n\nA computing cluster is a federation of multiple compute nodes (independent\ncomputers), most commonly linked together through a high-performance\ninterconnect network.\n\n\nWhat makes it a \"super-computer\" is the ability for a program to address\nresources (such as memory, CPU cores) located in different compute nodes,\nthrough the high-performance interconnect network.\n\n\n\n\nOn a computing cluster, users typically connect to \nlogin nodes\n,\nusing a secure remote login protocol such as \nSSH\n. Unlike in\ntraditional interactive environments, users then need to prepare \ncompute\njobs\n to submit to the \nresource scheduler\n. Based on a\n\n\nThe condominium model\n#\n\n\nFor users who need more than casual access to a shared computing environment,\nSRCC also offers faculty members the possibility to purchase additional\ndedicated resources to augment Sherlock, by becoming Sherlock \nowners\n.\nChoosing from a standard set of server configurations supported by SRCC\nstaff (known as the Sherlock \ncatalog\n), principal investigators (PIs) can\npurchase their own servers to add to the cluster.\n\n\nThe vast majority of Sherlock's compute nodes are actually owners nodes, and PI\npurchases are the main driver behind the rapid expansion of the cluster, which\nwent from 120 nodes in early 2014, to more than 1,000 nodes mid-2017. An order\nof magnitude increase in about 3 years.\n\n\nThis model, often referred to as the \nCondo model\n, allows Sherlock owners to\nbenefit from the scale of the cluster and give them access to more compute\nnodes than their individual purchase.  This provides owners with much greater\nflexibility than owning a standalone cluster.\n\n\nThe resource scheduler configuration works like this:\n\n\n\n\nowners and their research teams have priority use of the resources they\n  purchase,\n\n\nwhen those resources are idle, other owners can use them,\n\n\nwhen the purchasing owner wants to use his/her resources, other jobs will be\n  killed\n\n\n\n\nThis provides a way to get more resources to run less important jobs in the\nbackground, while making sure that an owner always gets immediate access to\nhis/her own nodes.\n\n\nParticipating owners also have shared access to the original base Sherlock\nnodes, along with everyone else.\n\n\nBenefits\n#\n\n\nBenefits to owners include:\n\n\n\n\nData center hosting, including backup power and cooling,\n\n\nAccess to high-performance, large parallel scratch disk space,\n\n\nPriority access to nodes that they own,\n\n\nBackground access to any owner nodes that are not in use,\n\n\nSystem configuration and administration,\n\n\nUser support,\n\n\nStandard software stack, appropriate for a range of research needs,\n\n\nPossibility for users to install additional software applications as needed,\n\n\n\n\nHow to become an owner\n#\n\n\nFor administrative reasons, SRCC offers PIs the possibility to purchase\nSherlock nodes on a quarterly basis. Large orders could be accommodated at any\ntime, though.\n\n\n\n\nMinimum purchase\n\n\nPlease note that the minimum purchase per PI is one physical server. We\ncannot accommodate multiple PIs pooling funds for a single node.\n\n\n\n\nIf you are interested in becoming an owner, you can find the latest information\nabout ordering Sherlock nodes on the \nSherlock ordering page\n\n(SUNet ID login required). Feel free to \ncontact us\n is you have any\nadditional question.\n\n\nCluster generations\n#\n\n\nThe research computing landscape evolves very quickly, and to both accommodate\ngrowth and technological advances, it's necessary to adapt the Sherlock\nenvironment to these evolutions.\n\n\nEvery year or so, a new generation of processors is released, which is why,\nover a span of several years, multiple generations of CPUs and GPUs make their\nway into Sherlock. This provides users with access to the latest features and\nperformance enhancements, but it also adds some heterogeneity to the cluster,\nwhich is important to keep in mind when compiling software and requesting\nresources to run them.\n\n\nAnother key component of Sherlock is the interconnect network that links all of\nSherlock's compute nodes together and act as a backbone for the whole cluster.\nThis network fabric is of finite capacity, and based on the individual\nnetworking switches characteristics and the typical research computing\nworkflows, it can accommodate up to about 850 compute nodes.\n\n\nAs nodes get added to Sherlock, the number of available ports decreases, and at\nsome point, the fabric gets full and no more nodes can be added. Sherlock\nreached that stage in late 2016, which prompted the installation of a whole new\nfabric, to continue adding nodes to the cluster.\n\n\nThis kind of evolution is the perfect opportunity to upgrade other components\ntoo: management software, ancillary services architecture and\nuser applications were completely overhauled and a new, completely separate\ncluster was kick-started. Sherlock 2.0 is a complete refresh of Sherlock, using\na different set of hardware and software, while conserving the same storage\ninfrastructure, to ease the transition process.\n\n\n\n\nTransition to Sherlock 2.0\n\n\nFor users who are already familiar with Sherlock, a \ntransition\nguide\n lists all the differences and the important\ninformation for starting on Sherlock 2.0.\n\n\n\n\nAfter a transition period, the older Sherlock hardware, compute and login\nnodes, will be merged in the new cluster, and from a logical perspective\n(connection, job scheduling and computing resources), both will form a single\ncluster again.\n\n\nAs Sherlock continues to evolve and grow, the new fabric will certainly get\nfull again, and the same process will happen again to start the next generation\nof Sherlock.\n\n\nMaintenances and upgrades\n#\n\n\nThe SRCC institutes a monthly scheduled maintenance window on Sherlock, to\nensure optimal operation, avoid potential issues and prepare for future\nexpansions.  This window will be used to make hardware repairs, software and\nfirmware updates, and perform general manufacturer recommended maintenance on\nour environment.\n\n\n\n\nMaintenance schedule\n\n\nAs often as possible, maintenances will take place on the \nfirst Tuesday\n of every month, from 8am to 12am\n, and will be announced 2 weeks in\n advance, through the usual communication channels.\n\n\n\n\nIn case an exceptional amount of work is required, the maintenance window could\nbe extended to 10 hours (from 8am to 6pm).\n\n\nDuring these times, access to Sherlock will be unavailable, login will be\ndisabled and jobs won't run. A reservation will be placed in the scheduler so\nrunning jobs can finish before the maintenance, and jobs that wouldn't finish\nby the maintenance window would be pushed after it.\n\n\nCommon questions\n#\n\n\nWhy doing maintenances at all?\nDue to growth in our compute environment and the increasing complexity of\nthe systems we deploy, we felt it prudent to arrange for a regular time\nwhen we could comfortably and without pressure fix problems or update\nfacilities with minimal impact to our customers. Most, if not all, major\nHPC centers have regular maintenance schedules.  We also need to enforce\nthe \nMinimum Security\n rules edicted by the Stanford\nInformation Security Office, which mandate deployment of security patches\nin a timely manner.\nWhy Tuesdays 8am-12am? Why not do this late at night?\nWe have observed that the least busy time for our services is at the\nbeginning of the week in the morning hours. Using this time period should\nnot interrupt most of our users. If the remote possibility of a problem\nthat extends past the scheduled downtime occurs, we would have our full\nstaff fresh and available to assist in repairs and quickly restore service.\nI have jobs running, what will happen to them?\nFor long-running jobs, we strongly recommend checkpointing your results on\na periodic basis. Besides, we will place a reservation in the scheduler for\neach maintenance that would prevent jobs to run past it. This means that\nthe scheduler will only allow jobs to run if they can finish by the time\nthe maintenance starts. If you submit a long job soon before the\nmaintenance, it will be delayed until after the maintenance. That will\nensure that no work is lost when the maintence starts.", 
            "title": "Concepts"
        }, 
        {
            "location": "/docs/overview/concepts/#sherlock-a-shared-resource", 
            "text": "Sherlock is a shared compute cluster available for use by all Stanford faculty\nmembers and their research teams to support sponsored research.   Sherlock is a resource for research  Sherlock is not suitable for course work, class assignments or general-use\ntraining sessions.   It is open to the Stanford community as a computing resource to support\ndepartmental or sponsored research, thus a faculty member's sponsorship is\nrequired for all user accounts.   Usage policy  Please note that your use of this system falls under the \"Computer and\nNetwork Usage Policy\", as described in the  Stanford Administrative\nGuide . In particular, sharing authentication credentials is\nstrictly prohibited.  Violation of this policy will result in termination\nof access to Sherlock.   Sherlock has been designed, deployed, and is maintained and operated by the Stanford Research Computing Center (SRCC)  staff. The SRCC is a\njoint effort of the  Dean of Research  and  IT Services  to\nbuild and support a comprehensive program to advance computational research at\nStanford.  Sherlock has been initially purchased and supported with seed funding from\nStanford's  Provost . It comprises a set of freely available\ncompute nodes, a few specific resources such as large-memory machines and GPU\nservers, as well as the associated networking equipment and storage.  These\nresources can be used to run computational codes and programs, and are managed\nthrough a job scheduler using a  fair-share algorithm .", 
            "title": "Sherlock, a shared resource"
        }, 
        {
            "location": "/docs/overview/concepts/#data-risk-classification", 
            "text": "Low and Moderate Risk data  Sherlock is approved for computing with Low and Moderate Risk data only.    High Risk data  Sherlock is  NOT   HIPAA  compliant and shouldn't be used to\nprocess  PHI  nor  PII .  The system is approved for\ncomputing with Low and Moderate Risk data only, and is  not suitable to\nprocess High Risk data .  For more information about data risk\nclassifications, see the  Information Security Risk Classification\npage .", 
            "title": "Data risk classification"
        }, 
        {
            "location": "/docs/overview/concepts/#whats-a-cluster", 
            "text": "A computing cluster is a federation of multiple compute nodes (independent\ncomputers), most commonly linked together through a high-performance\ninterconnect network.  What makes it a \"super-computer\" is the ability for a program to address\nresources (such as memory, CPU cores) located in different compute nodes,\nthrough the high-performance interconnect network.   On a computing cluster, users typically connect to  login nodes ,\nusing a secure remote login protocol such as  SSH . Unlike in\ntraditional interactive environments, users then need to prepare  compute\njobs  to submit to the  resource scheduler . Based on a", 
            "title": "What's a cluster?"
        }, 
        {
            "location": "/docs/overview/concepts/#the-condominium-model", 
            "text": "For users who need more than casual access to a shared computing environment,\nSRCC also offers faculty members the possibility to purchase additional\ndedicated resources to augment Sherlock, by becoming Sherlock  owners .\nChoosing from a standard set of server configurations supported by SRCC\nstaff (known as the Sherlock  catalog ), principal investigators (PIs) can\npurchase their own servers to add to the cluster.  The vast majority of Sherlock's compute nodes are actually owners nodes, and PI\npurchases are the main driver behind the rapid expansion of the cluster, which\nwent from 120 nodes in early 2014, to more than 1,000 nodes mid-2017. An order\nof magnitude increase in about 3 years.  This model, often referred to as the  Condo model , allows Sherlock owners to\nbenefit from the scale of the cluster and give them access to more compute\nnodes than their individual purchase.  This provides owners with much greater\nflexibility than owning a standalone cluster.  The resource scheduler configuration works like this:   owners and their research teams have priority use of the resources they\n  purchase,  when those resources are idle, other owners can use them,  when the purchasing owner wants to use his/her resources, other jobs will be\n  killed   This provides a way to get more resources to run less important jobs in the\nbackground, while making sure that an owner always gets immediate access to\nhis/her own nodes.  Participating owners also have shared access to the original base Sherlock\nnodes, along with everyone else.", 
            "title": "The condominium model"
        }, 
        {
            "location": "/docs/overview/concepts/#benefits", 
            "text": "Benefits to owners include:   Data center hosting, including backup power and cooling,  Access to high-performance, large parallel scratch disk space,  Priority access to nodes that they own,  Background access to any owner nodes that are not in use,  System configuration and administration,  User support,  Standard software stack, appropriate for a range of research needs,  Possibility for users to install additional software applications as needed,", 
            "title": "Benefits"
        }, 
        {
            "location": "/docs/overview/concepts/#how-to-become-an-owner", 
            "text": "For administrative reasons, SRCC offers PIs the possibility to purchase\nSherlock nodes on a quarterly basis. Large orders could be accommodated at any\ntime, though.   Minimum purchase  Please note that the minimum purchase per PI is one physical server. We\ncannot accommodate multiple PIs pooling funds for a single node.   If you are interested in becoming an owner, you can find the latest information\nabout ordering Sherlock nodes on the  Sherlock ordering page \n(SUNet ID login required). Feel free to  contact us  is you have any\nadditional question.", 
            "title": "How to become an owner"
        }, 
        {
            "location": "/docs/overview/concepts/#cluster-generations", 
            "text": "The research computing landscape evolves very quickly, and to both accommodate\ngrowth and technological advances, it's necessary to adapt the Sherlock\nenvironment to these evolutions.  Every year or so, a new generation of processors is released, which is why,\nover a span of several years, multiple generations of CPUs and GPUs make their\nway into Sherlock. This provides users with access to the latest features and\nperformance enhancements, but it also adds some heterogeneity to the cluster,\nwhich is important to keep in mind when compiling software and requesting\nresources to run them.  Another key component of Sherlock is the interconnect network that links all of\nSherlock's compute nodes together and act as a backbone for the whole cluster.\nThis network fabric is of finite capacity, and based on the individual\nnetworking switches characteristics and the typical research computing\nworkflows, it can accommodate up to about 850 compute nodes.  As nodes get added to Sherlock, the number of available ports decreases, and at\nsome point, the fabric gets full and no more nodes can be added. Sherlock\nreached that stage in late 2016, which prompted the installation of a whole new\nfabric, to continue adding nodes to the cluster.  This kind of evolution is the perfect opportunity to upgrade other components\ntoo: management software, ancillary services architecture and\nuser applications were completely overhauled and a new, completely separate\ncluster was kick-started. Sherlock 2.0 is a complete refresh of Sherlock, using\na different set of hardware and software, while conserving the same storage\ninfrastructure, to ease the transition process.   Transition to Sherlock 2.0  For users who are already familiar with Sherlock, a  transition\nguide  lists all the differences and the important\ninformation for starting on Sherlock 2.0.   After a transition period, the older Sherlock hardware, compute and login\nnodes, will be merged in the new cluster, and from a logical perspective\n(connection, job scheduling and computing resources), both will form a single\ncluster again.  As Sherlock continues to evolve and grow, the new fabric will certainly get\nfull again, and the same process will happen again to start the next generation\nof Sherlock.", 
            "title": "Cluster generations"
        }, 
        {
            "location": "/docs/overview/concepts/#maintenances-and-upgrades", 
            "text": "The SRCC institutes a monthly scheduled maintenance window on Sherlock, to\nensure optimal operation, avoid potential issues and prepare for future\nexpansions.  This window will be used to make hardware repairs, software and\nfirmware updates, and perform general manufacturer recommended maintenance on\nour environment.   Maintenance schedule  As often as possible, maintenances will take place on the  first Tuesday\n of every month, from 8am to 12am , and will be announced 2 weeks in\n advance, through the usual communication channels.   In case an exceptional amount of work is required, the maintenance window could\nbe extended to 10 hours (from 8am to 6pm).  During these times, access to Sherlock will be unavailable, login will be\ndisabled and jobs won't run. A reservation will be placed in the scheduler so\nrunning jobs can finish before the maintenance, and jobs that wouldn't finish\nby the maintenance window would be pushed after it.", 
            "title": "Maintenances and upgrades"
        }, 
        {
            "location": "/docs/overview/concepts/#common-questions", 
            "text": "Why doing maintenances at all? Due to growth in our compute environment and the increasing complexity of\nthe systems we deploy, we felt it prudent to arrange for a regular time\nwhen we could comfortably and without pressure fix problems or update\nfacilities with minimal impact to our customers. Most, if not all, major\nHPC centers have regular maintenance schedules.  We also need to enforce\nthe  Minimum Security  rules edicted by the Stanford\nInformation Security Office, which mandate deployment of security patches\nin a timely manner. Why Tuesdays 8am-12am? Why not do this late at night? We have observed that the least busy time for our services is at the\nbeginning of the week in the morning hours. Using this time period should\nnot interrupt most of our users. If the remote possibility of a problem\nthat extends past the scheduled downtime occurs, we would have our full\nstaff fresh and available to assist in repairs and quickly restore service. I have jobs running, what will happen to them? For long-running jobs, we strongly recommend checkpointing your results on\na periodic basis. Besides, we will place a reservation in the scheduler for\neach maintenance that would prevent jobs to run past it. This means that\nthe scheduler will only allow jobs to run if they can finish by the time\nthe maintenance starts. If you submit a long job soon before the\nmaintenance, it will be delayed until after the maintenance. That will\nensure that no work is lost when the maintence starts.", 
            "title": "Common questions"
        }, 
        {
            "location": "/docs/overview/glossary/", 
            "text": "Cluster components\n#\n\n\nThe terms that are typically used to describe cluster components could be\nconfusing, so in an effort to clarify things, here's a schema of the most\nimportant ones, and their definition.\n\n\n\nCPU\n#\n\n\n\n\nA Central Processing Unit (CPU), or core, or CPU core, is the smallest unit\nin a microprocessor that can carry out computational tasks, that is, run\nprograms. Modern processors typically have multiple cores.\n\n\n\n\nSocket\n#\n\n\n\n\nA socket is the connector that houses the microprocessor. By extension, it\nrepresents the physical package of a processor, that typically contains\nmultiple cores.\n\n\n\n\nNode\n#\n\n\n\n\nA node is a physical, stand-alone computer, that can handle computing tasks\nand run jobs. It's connected to other compute nodes via a fast network\ninterconnect, and contains CPUs, memory and devices managed by an operating\nsystem.\n\n\n\n\nCluster\n#\n\n\n\n\nA cluster is the complete collection of nodes with networking and file\nstorage facilities. It's usually a group of independent computers connected via\na fast network interconnect, managed by a resource manager, which acts as a\nlarge parallel computer.\n\n\n\n\nOther commonly used terms\n#\n\n\nTo make this documentation more accessible, we try to explain key terms in a\nnon-technical way. When reading these pages, please keep in mind the following\ndefinitions, presented in alphabetical order:\n\n\nApplication\n#\n\n\n\n\nAn application is a computer program designed to perform a group of\ncoordinated functions, tasks, or activities for the benefit of the user. In the\ncontext of scientific computing, an application typically performs computations\nrelated to a scientific goal (molecular dynamics simulations, genome assembly,\ncompuational fluid dynamics simulations, etc).\n\n\n\n\nBackfill\n#\n\n\n\n\nBackfill scheduling is a method that a scheduler can use in order to maximize\nutilization. It allows smaller (both in terms of size and time requirements),\nlower priority jobs to start before larger, higher priority ones, as long as\ndoing so doesn't push back the higher-priority jobs expected start time.\n\n\n\n\nExecutable\n#\n\n\n\n\nA binary (or executable) program refers to the machine-code compiled version\nof an application. This is  which is a binary file that a computer can execute\ndirectly. As opposed to the application source code, which is the\nhuman-readable version of the application internal instructions, and which\nneeds to be compiled by a compiler to produce the executable binary.\n\n\n\n\nFairshare\n#\n\n\n\n\nA resource scheduler ranks jobs by priority for execution. Each job's\npriority in queue is determined by multiple factors, among which\none being the user's fairshare score.  A user's fairshare score is computed\nbased on a target (the given portion of the resources that this user should be\nable to use) and the user's effetive usage, \nie\n the amount of resources (s)he\neffectively used in the past.  As a result, the more resources past jobs have\nused, the lower the priority of the next jobs will be.  Past usage is computed\nbased on a sliding window and progressively forgotten over time.  This enables\nall users on a shared resource to get a fair portion of it for their own use,\nby giving higher priorty to users who have been underserved in the past.\n\n\n\n\nFLOPS\n#\n\n\n\n\nFloating-point Operations Per Second (FLOPS) are a measure of computing\nperformance, and represent the number of floating-point operations that a CPU\ncan perform each second. Modern CPUs and GPUs are capable of doing TeraFLOPS\n(10^12 floating-point operations per second), depending on the precision of\nthose operations (half-precision: 16 bits, single-precision: 32 bits,\ndouble-precision: 64 bits).\n\n\n\n\nGPU\n#\n\n\n\n\nA Graphical Processing Unit (GPU) is a specialized device initially designed\nto generate graphical output.  On modern computing architecture, they are used\nto accelerate certain types of computation, which they are much faster than\nCPUs at. GPUs have their own memory, and are attached to CPUs, within a node.\nEach compute node can host one or more GPUs.\n\n\n\n\nHPC\n#\n\n\n\n\nHigh Performance Computing (HPC) refers to the practice of aggregating\ncomputing power to achieve higher performance that would be possible by using a\ntypical computer.\n\n\n\n\nInfiniband\n#\n\n\n\n\nInfiniband is a networking standard that features high bandwidth and low\nlatency. The current Infiniband devices are capable of transferring data at up\nto 100 Gbits/sec with less than a microsecond latency. As of this writing, the\npopular Infiniband versions are FDR (Fourteen Data Rate) with 56 Gbits/sec and\nEDR (Enhanced Data Rate) with 100 Gbits/sec.\n\n\n\n\nIOPS\n#\n\n\n\n\nInput/output operations per second (IOPS, pronounced eye-ops) is an\ninput/output performance measurement used to characterize computer storage\nsystem performance.\n\n\n\n\nJob\n#\n\n\n\n\nA job, or batch job, is the scheduler\u2019s base unit of computing by which\nresources are allocated to a user for a specified amount of time. Users create\njob submission scripts to ask the scheduler for resources such as cores,\nmemory, runtime, etc. The scheduler puts the requests in a queue and allocates\nrequested resources based on jobs\u2019 priority.\n\n\n\n\nJob step\n#\n\n\n\n\nJob steps are sets of (possibly parallel) tasks within a job\n\n\n\n\nLogin nodes\n#\n\n\n\n\n\n\nLogin nodes are points of access to a compute cluster. Users usually connect\nto login nodes via SSH to compile and debug their code, review their results,\ndo some simple tests, and submit their batch jobs to the parallel computer.\n\n\n\n\nLogin nodes are not for computing\n\n\nLogin nodes are usually shared among many users and therefore must not\nbe used to run computationally intensive tasks. Those should be\nsubmitted to the scheduler which will dispatch them on compute nodes.\n\n\n\n\n\n\n\n\nModules\n#\n\n\n\n\nEnvironment modules, or software modules, are a type of software management\ntool used on in most HPC environments. Using modules enable users to\nselectively pick the software that they want to use and add them to their\nenvironment. This allows to switch between different versions or flavors of the\nsame software, pick compilers, libraries and software components and avoid\nconflicts between them.\n\n\n\n\nMPI\n#\n\n\n\n\nMessage Passing Interface (MPI) is a standardized and portable\nmessage-passing system designed to exchange information between processes\nrunning on different nodes. There are several implementations of the MPI\nstandard, which is the most common way used to scale parallel applications\nbeyond a single compute node.\n\n\n\n\nOpenMP\n#\n\n\n\n\nOpen Multi Processing (OpenMP) is a parallel programming model\ndesigned for shared memory architecture. It's based on pragmas that can be\nadded in applications to let the compiler generate a code that\ncan run on multiple cores, within the same node.\n\n\n\n\nPartition\n#\n\n\n\n\nA partition is a set of compute nodes within a cluster with a\ncommon feature. For example, compute nodes with GPU, or compute nodes belonging\nto same owner,  could form a partition.\n\n\n\n\nQOS\n#\n\n\n\n\nA Quality Of Service (QOS) is the set of rules and limitations that apply to\na categories of job. The combination of a partition (set of machines where a\njob can run) and QOS (set of rules that applies to that job) makes what is\noften referred to as a scheduler \nqueue\n.\n\n\n\n\nRun time\n#\n\n\n\n\nThe run time, or walltime, of a job is the time required to finish its\nexecution.\n\n\n\n\nScheduler\n#\n\n\n\n\nThe goal of a job scheduler is to find the appropriate resources to run a set\nof computational tasks in the most efficient manner. Based on resource\nrequirements and job descriptions, it will prioritize those jobs, allocate\nresources (nodes, CPUs, memory) and schedule their execution.\n\n\n\n\nSlurm\n#\n\n\n\n\nSimple Linux Utility for Resource Management (SLURM) is a software that\nmanages computing resources and schedule tasks on them. Slurm coordinates\nrunning of many programs on a shared facility and makes sure that resources are\nused in an optimal manner.\n\n\n\n\nSSH\n#\n\n\n\n\nSecure Shell (SSH) is a protocol to securely access remote computers.\nBased on the client-server model, multiple users with an SSH client can access\na remote computer. Some operating systems such as Linux and Mac OS have a\nbuilt-in SSH client and others can use one of many publicly available clients.\n\n\n\n\nThread\n#\n\n\n\n\nA process, in the simplest terms, is an executing program. One or more\nthreads run in the context of the process. A thread is the basic unit to which\nthe operating system allocates processor time. A thread can execute any part of\nthe process code, including parts currently being executed by another thread.\nThreads are co-located on the same node.\n\n\n\n\nTask\n#\n\n\n\n\nIn the Slurm context, a task is to be understood as a process. A\nmulti-process program is made of several tasks. A task is typically used to\nschedule a MPI process, that in turn can use several CPUs.  By contrast, a\nmulti-threaded program is composed of only one task, which uses several CPUs.", 
            "title": "Glossary"
        }, 
        {
            "location": "/docs/overview/glossary/#cluster-components", 
            "text": "The terms that are typically used to describe cluster components could be\nconfusing, so in an effort to clarify things, here's a schema of the most\nimportant ones, and their definition.", 
            "title": "Cluster components"
        }, 
        {
            "location": "/docs/overview/glossary/#cpu", 
            "text": "A Central Processing Unit (CPU), or core, or CPU core, is the smallest unit\nin a microprocessor that can carry out computational tasks, that is, run\nprograms. Modern processors typically have multiple cores.", 
            "title": "CPU"
        }, 
        {
            "location": "/docs/overview/glossary/#socket", 
            "text": "A socket is the connector that houses the microprocessor. By extension, it\nrepresents the physical package of a processor, that typically contains\nmultiple cores.", 
            "title": "Socket"
        }, 
        {
            "location": "/docs/overview/glossary/#node", 
            "text": "A node is a physical, stand-alone computer, that can handle computing tasks\nand run jobs. It's connected to other compute nodes via a fast network\ninterconnect, and contains CPUs, memory and devices managed by an operating\nsystem.", 
            "title": "Node"
        }, 
        {
            "location": "/docs/overview/glossary/#cluster", 
            "text": "A cluster is the complete collection of nodes with networking and file\nstorage facilities. It's usually a group of independent computers connected via\na fast network interconnect, managed by a resource manager, which acts as a\nlarge parallel computer.", 
            "title": "Cluster"
        }, 
        {
            "location": "/docs/overview/glossary/#other-commonly-used-terms", 
            "text": "To make this documentation more accessible, we try to explain key terms in a\nnon-technical way. When reading these pages, please keep in mind the following\ndefinitions, presented in alphabetical order:", 
            "title": "Other commonly used terms"
        }, 
        {
            "location": "/docs/overview/glossary/#application", 
            "text": "An application is a computer program designed to perform a group of\ncoordinated functions, tasks, or activities for the benefit of the user. In the\ncontext of scientific computing, an application typically performs computations\nrelated to a scientific goal (molecular dynamics simulations, genome assembly,\ncompuational fluid dynamics simulations, etc).", 
            "title": "Application"
        }, 
        {
            "location": "/docs/overview/glossary/#backfill", 
            "text": "Backfill scheduling is a method that a scheduler can use in order to maximize\nutilization. It allows smaller (both in terms of size and time requirements),\nlower priority jobs to start before larger, higher priority ones, as long as\ndoing so doesn't push back the higher-priority jobs expected start time.", 
            "title": "Backfill"
        }, 
        {
            "location": "/docs/overview/glossary/#executable", 
            "text": "A binary (or executable) program refers to the machine-code compiled version\nof an application. This is  which is a binary file that a computer can execute\ndirectly. As opposed to the application source code, which is the\nhuman-readable version of the application internal instructions, and which\nneeds to be compiled by a compiler to produce the executable binary.", 
            "title": "Executable"
        }, 
        {
            "location": "/docs/overview/glossary/#fairshare", 
            "text": "A resource scheduler ranks jobs by priority for execution. Each job's\npriority in queue is determined by multiple factors, among which\none being the user's fairshare score.  A user's fairshare score is computed\nbased on a target (the given portion of the resources that this user should be\nable to use) and the user's effetive usage,  ie  the amount of resources (s)he\neffectively used in the past.  As a result, the more resources past jobs have\nused, the lower the priority of the next jobs will be.  Past usage is computed\nbased on a sliding window and progressively forgotten over time.  This enables\nall users on a shared resource to get a fair portion of it for their own use,\nby giving higher priorty to users who have been underserved in the past.", 
            "title": "Fairshare"
        }, 
        {
            "location": "/docs/overview/glossary/#flops", 
            "text": "Floating-point Operations Per Second (FLOPS) are a measure of computing\nperformance, and represent the number of floating-point operations that a CPU\ncan perform each second. Modern CPUs and GPUs are capable of doing TeraFLOPS\n(10^12 floating-point operations per second), depending on the precision of\nthose operations (half-precision: 16 bits, single-precision: 32 bits,\ndouble-precision: 64 bits).", 
            "title": "FLOPS"
        }, 
        {
            "location": "/docs/overview/glossary/#gpu", 
            "text": "A Graphical Processing Unit (GPU) is a specialized device initially designed\nto generate graphical output.  On modern computing architecture, they are used\nto accelerate certain types of computation, which they are much faster than\nCPUs at. GPUs have their own memory, and are attached to CPUs, within a node.\nEach compute node can host one or more GPUs.", 
            "title": "GPU"
        }, 
        {
            "location": "/docs/overview/glossary/#hpc", 
            "text": "High Performance Computing (HPC) refers to the practice of aggregating\ncomputing power to achieve higher performance that would be possible by using a\ntypical computer.", 
            "title": "HPC"
        }, 
        {
            "location": "/docs/overview/glossary/#infiniband", 
            "text": "Infiniband is a networking standard that features high bandwidth and low\nlatency. The current Infiniband devices are capable of transferring data at up\nto 100 Gbits/sec with less than a microsecond latency. As of this writing, the\npopular Infiniband versions are FDR (Fourteen Data Rate) with 56 Gbits/sec and\nEDR (Enhanced Data Rate) with 100 Gbits/sec.", 
            "title": "Infiniband"
        }, 
        {
            "location": "/docs/overview/glossary/#iops", 
            "text": "Input/output operations per second (IOPS, pronounced eye-ops) is an\ninput/output performance measurement used to characterize computer storage\nsystem performance.", 
            "title": "IOPS"
        }, 
        {
            "location": "/docs/overview/glossary/#job", 
            "text": "A job, or batch job, is the scheduler\u2019s base unit of computing by which\nresources are allocated to a user for a specified amount of time. Users create\njob submission scripts to ask the scheduler for resources such as cores,\nmemory, runtime, etc. The scheduler puts the requests in a queue and allocates\nrequested resources based on jobs\u2019 priority.", 
            "title": "Job"
        }, 
        {
            "location": "/docs/overview/glossary/#job-step", 
            "text": "Job steps are sets of (possibly parallel) tasks within a job", 
            "title": "Job step"
        }, 
        {
            "location": "/docs/overview/glossary/#login-nodes", 
            "text": "Login nodes are points of access to a compute cluster. Users usually connect\nto login nodes via SSH to compile and debug their code, review their results,\ndo some simple tests, and submit their batch jobs to the parallel computer.   Login nodes are not for computing  Login nodes are usually shared among many users and therefore must not\nbe used to run computationally intensive tasks. Those should be\nsubmitted to the scheduler which will dispatch them on compute nodes.", 
            "title": "Login nodes"
        }, 
        {
            "location": "/docs/overview/glossary/#modules", 
            "text": "Environment modules, or software modules, are a type of software management\ntool used on in most HPC environments. Using modules enable users to\nselectively pick the software that they want to use and add them to their\nenvironment. This allows to switch between different versions or flavors of the\nsame software, pick compilers, libraries and software components and avoid\nconflicts between them.", 
            "title": "Modules"
        }, 
        {
            "location": "/docs/overview/glossary/#mpi", 
            "text": "Message Passing Interface (MPI) is a standardized and portable\nmessage-passing system designed to exchange information between processes\nrunning on different nodes. There are several implementations of the MPI\nstandard, which is the most common way used to scale parallel applications\nbeyond a single compute node.", 
            "title": "MPI"
        }, 
        {
            "location": "/docs/overview/glossary/#openmp", 
            "text": "Open Multi Processing (OpenMP) is a parallel programming model\ndesigned for shared memory architecture. It's based on pragmas that can be\nadded in applications to let the compiler generate a code that\ncan run on multiple cores, within the same node.", 
            "title": "OpenMP"
        }, 
        {
            "location": "/docs/overview/glossary/#partition", 
            "text": "A partition is a set of compute nodes within a cluster with a\ncommon feature. For example, compute nodes with GPU, or compute nodes belonging\nto same owner,  could form a partition.", 
            "title": "Partition"
        }, 
        {
            "location": "/docs/overview/glossary/#qos", 
            "text": "A Quality Of Service (QOS) is the set of rules and limitations that apply to\na categories of job. The combination of a partition (set of machines where a\njob can run) and QOS (set of rules that applies to that job) makes what is\noften referred to as a scheduler  queue .", 
            "title": "QOS"
        }, 
        {
            "location": "/docs/overview/glossary/#run-time", 
            "text": "The run time, or walltime, of a job is the time required to finish its\nexecution.", 
            "title": "Run time"
        }, 
        {
            "location": "/docs/overview/glossary/#scheduler", 
            "text": "The goal of a job scheduler is to find the appropriate resources to run a set\nof computational tasks in the most efficient manner. Based on resource\nrequirements and job descriptions, it will prioritize those jobs, allocate\nresources (nodes, CPUs, memory) and schedule their execution.", 
            "title": "Scheduler"
        }, 
        {
            "location": "/docs/overview/glossary/#slurm", 
            "text": "Simple Linux Utility for Resource Management (SLURM) is a software that\nmanages computing resources and schedule tasks on them. Slurm coordinates\nrunning of many programs on a shared facility and makes sure that resources are\nused in an optimal manner.", 
            "title": "Slurm"
        }, 
        {
            "location": "/docs/overview/glossary/#ssh", 
            "text": "Secure Shell (SSH) is a protocol to securely access remote computers.\nBased on the client-server model, multiple users with an SSH client can access\na remote computer. Some operating systems such as Linux and Mac OS have a\nbuilt-in SSH client and others can use one of many publicly available clients.", 
            "title": "SSH"
        }, 
        {
            "location": "/docs/overview/glossary/#thread", 
            "text": "A process, in the simplest terms, is an executing program. One or more\nthreads run in the context of the process. A thread is the basic unit to which\nthe operating system allocates processor time. A thread can execute any part of\nthe process code, including parts currently being executed by another thread.\nThreads are co-located on the same node.", 
            "title": "Thread"
        }, 
        {
            "location": "/docs/overview/glossary/#task", 
            "text": "In the Slurm context, a task is to be understood as a process. A\nmulti-process program is made of several tasks. A task is typically used to\nschedule a MPI process, that in turn can use several CPUs.  By contrast, a\nmulti-threaded program is composed of only one task, which uses several CPUs.", 
            "title": "Task"
        }, 
        {
            "location": "/docs/overview/transition/", 
            "text": "Sherlock 2.0 \ntransition guide\n#\n\n\nThis transition guide is intended for users already familiar with Sherlock, to\nhelp them moving to Sherlock 2.0: differences with the existing, temporary\nconditions, etc.\n\n\n\n\nSherlock 2.0 orders\n\n\nIf you are looking to buy into Sherlock 2.0, please see the \nSherlock\nQuarterly Order\n page (\nSUNet ID required\n).\n\n\n\n\nWhy Sherlock 2.0?\n#\n\n\nSherlock started in 2014 with 120 general use compute nodes and their\nassociated networking (high-performance interconnect) and storage (Lustre\nscratch filesystem) infrastructure. Those nodes have been consistently heavily\nused.  An additional 730+ nodes were purchased by a wide variety of\n\nStanford\n and \nSLAC\n researchers and added to the\ninitial base.\n\n\nHowever, the Infiniband fabric (the supporting network that interconnects all\nof the nodes and storage) eventually reached capacity, effectively putting a\nhalt to Sherlock's growth. With the help of new recurring funding from the\nUniversity, a new, completely separate cluster has been kick-started, with its\nown Infiniband fabric, new management servers, and a set of more\ncapable compute nodes. These new nodes, along with an updated software\nenvironment, forms the basis of Sherlock 2.0.\n\n\nFor the sake of simplicity, we'll refer to the new environment as Sherlock\n2.0, and to the existing one as Sherlock 1.0.\n\n\nTransition process\n#\n\n\n\n\nStorage\n\n\nFor the whole duration of the transition process, storage will be shared\nbetween Sherlock 1.0 and Sherlock 2.0 and will be accessible from both,\nmeaning that users will be able to access and work on the the same files\nwhatever cluster they connect to.\n\n\n\n\nTo allow every user to smoothly transition to the new system, a 3-phase\nprocess will take place.\n\n\n\n\nPhase 1\n: coexistence\n\n\n\n\nFor the duration of the transition period, both Sherlock 1.0 and Sherlock 2.0\nwill coexist. As two separate clusters, they will use distinct login nodes and\nprovide access to different hardware and software.\n\n\n\n\nTwo isolated clusters\n\n\nSpecific compute nodes will only be accessible from one\ncluster at a time: the new compute nodes from Sherlock 2.0, and the old\nnodes from Sherlock 1.0. It also means that jobs submitted on Sherlock\n1.0 will not visible from Sherlock 2.0, and vice-versa.\n\n\n\n\n\n\nPhase 2\n: merge\n\n\n\n\nAfter sufficient time for the majority of users to validate and migrate to\nthe new environment, the original Sherlock nodes will be merged into the new\ncluster. We'll work with individual owners to determine the best time frame for\ntheir own nodes migration, and will announce a date for the migration of all\nthe general nodes from the public partitions.\n\n\n\n\nPhase 3\n: retirement\n\n\n\n\nAs they age out, compute nodes older than 5 years will be retired, to\ncomply with the data center usage guidelines. When all the remaining Sherlock\n1.0 nodes have been merged into Sherlock 2.0, the Sherlock 1.0 and 2.0\ndistinction will disappear, and Sherlock will be a single system again.\n\n\n\n\n\n\nWhat changes?\n#\n\n\nQuite a lot!\n\n\n\n\nMost notable upgrades\n\n\nAdditional cores per CPU, increased Infiniband throughput, twice the amount\nof memory per node and higher memory/core ratio, increased power\nredundancy, a new OS and software stack, and mandatory 2FA.\n\n\n\n\nDocumentation\n#\n\n\nAs part of our documentation and communication effort, we're announcing\nthree new web resources for Sherlock:\n\n\n\n\na \nchangelog\n for news and updates, to get a better sense of\n  what's happening on Sherlock, beyond general email announces,\n\n\na \nstatus dashboard\n, where you'll find up-to-date information\n  about the status of the main Sherlock components and services. You could also\n  subscribe to receive notification in case of events or outages,\n\n\nand of course the very \ndocumentation\n you're reading right now.\n\n\n\n\nLogin\n#\n\n\nThe most notable changes are visible right from the connection process.\n\n\n\n\n\n\n\n\nMain changes\n\n\nSherlock 1.0\n\n\nSherlock 2.0\n\n\n\n\n\n\n\n\n\n\nLogin nodes\n\n\nsherlock.stanford.edu\n\n\nlogin.sherlock.stanford.edu\n\n\n\n\n\n\nAuthentication scheme\n\n\nGSSAPI (Kerberos)\n\n\nSUNet ID password or GSSAPI\n\n\n\n\n\n\nTwo-step authentication\n\n\nno\n\n\nyes\n\n\n\n\n\n\n\n\n\n\nNew login hostname\n\n\n\n\nSherlock 2.0 uses a new set of login nodes, under a different load-balanced\n  hostname. To connect, you'll need to use:\n  \n$ ssh \nsunet\n@login.sherlock.stanford.edu\n\n  (note the \nlogin\n part of the hostname)\n\n\n\n\nNew server fingerprint\n\n\nWhen you connect to the new login nodes for the first time, you'll be\npresented with the new servers' fingerprint (host keys). Please see the\n\nConnecting\n page for more details.\n\n\n\n\n\n\nNew authentication scheme\n\n\n\n\nGSSAPI (Kerberos) is not required anymore. To make up for the next item, we\n  relaxed the requirement on Kerberos authentication for Sherlock 2.0. Meaning\n  that you will now be able to connect using your SUNet ID password, without\n  having to get a Kerberos ticket via \nkinit\n first.  We hope this will also\n  ease the connection process from clients and OS which don't correctly support\n  GSSAPI. And please note that if you still want to use Kerberos for\n  authentication, you can.\n\n\n\n\nTwo-step authentication\n\n\n\n\nTwo-step (or two-factor) authentication (2FA) this is now a requirement in\n  the\u00a0\nMinimum Security Standards\n mandated by the \nInformation\n  Security Office\n, and that we're implementing on Sherlock. Upon\n  connection, you will be prompted for your password (if you're not using\n  GSSAPI) and a two-factor prompt like this:\n  \nDuo two-factor login for johndoe\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-4425\n 2. Phone call to XXX-XXX-4425\n 3. SMS passcodes to XXX-XXX-4425 (next code starts with: 1)\n\nPasscode or option (1-3):\n\n    Upon validation of the passcode, you'll be connected to Sherlock's login\n    nodes as usual.\n\n\n\n\n\n\nFor more details about the login procedure, see the \nConnecting\n\npage.\n\n\nHardware\n#\n\n\nA number of changes differentiate Sherlock 2.0 from the first generation:\ntechnology has evolved and those changes are reflected in the new hardware\nspecifications. Here are the main changes on typical compute nodes:\n\n\n\n\n\n\n\n\nMain changes\n\n\nSherlock 1.0\n\n\nSherlock 2.0\n\n\n\n\n\n\n\n\n\n\nNodes\n\n\n1U Dell R630 servers\n\n\n0.5U C6320 servers in a 2U chassis\n\n\n\n\n\n\nCPUs\n\n\n2x 8-core (Ivy-Bridge/Haswell)\n\n\n2x 10-core (Broadwell)\n\n\n\n\n\n\nMemory\n\n\n64G (base node)\n\n\n128G (base node)\n\n\n\n\n\n\nLocal storage\n\n\n200G SSD (+ 500G HDD)\n\n\n200G SSD\n\n\n\n\n\n\nInterconnect\n\n\n56G FDR Infiniband\n\n\n100G EDR Infiniband\n\n\n\n\n\n\n\n\n\n\nNodes\n\n\n\n\nBase node model moves from the Dell R630 1U server to the Dell C6320\n  system. More servers will fit in a single rack, which is essential for us to\n  accommodate the ever-increasing demand for servers.  Another change is that\n  the new model provides redundant power supplies, in case one fails. Each\n  C6320 chassis houses four independent servers in only two rack units of\n  space.\n\n\n\n\nCPUs\n\n\n\n\nUpgrade from Intel\n Xeon\n E5-2640v3 (8-core, 2.6GHz) to E5-2640v4\n  (10-core, 2.4GHz), meaning that each CPU gets two additional cores, and that\n  to maintain a 90W power envelope, the clock speed changed from 2.6GHz to\n  2.4GHz.\n\n\n\n\nMemory\n\n\n\n\nBecause of the 2 additional cores per CPU, a total for 4 more per node,\n  having a 64GB minimum RAM requirement was no longer deemed to be a good fit.\n  Originally, 64GB for 16 cores gave a ratio of 4GB/core. With the same 64GB\n  per node, the memory per core ratio on Sherlock 2.0 would have dropped to\n  3.2GB/core.  Usage statistics showed that more RAM per core has been needed\n  for a wide range of applications.  So for Sherlock 2.0, the minimum memory\n  amount per node has be set to 128GB, or 6.4GB/core, a much more appropriate\n  ratio for the typical user needs.\n\n\n\n\nLocal storage\n\n\n\n\nThe slightly smaller form factor of each node has prompted\n  elimination of the 500G spinning hard drive, keeping the 200G SSD.  Sherlock\n  statistics showed that the hard drive was rarely used, as users favored use\n  of the very fast Lustre /scratch file system available to all nodes.\n\n\n\n\nInterconnect\n\n\n\n\nSubstantial network improvement by way of an almost 2x increase in\n  Infiniband (IB) speed. Sherlock 1.0 uses the FDR IB speed of 56Gbps, while\n  Sherlock 2.0 uses EDR IB at 100Gbps.\n\n\n\n\n\n\nSlurm partitions\n#\n\n\nThe same organization of compute nodes will be used on Sherlock 2.0, with the\nsame partition nomenclature, although different hardware characteristics:\n\n\n\n\n\n\n\n\nMain changes\n\n\nSherlock 1.0\n\n\nSherlock 2.0\n\n\n\n\n\n\n\n\n\n\nnormal\n\n\n116x 16c/64GB nodes\n\n\n56x 20c/128GB nodes\n\n\n\n\n\n\ndev\n\n\n2x 16c/64GB nodes\n\n\n2x 20c/128GB nodes\n\n\n\n\n\n\nbigmem\n\n\n2x 32c/1.5TB nodes\n\n\n1x 56c/3.0TB node\n 1x 32c/512GB node\n\n\n\n\n\n\ngpu\n\n\n6x 16c/256GB nodes \n w/ 8x K20/K80/TITAN Black\n\n\n5x 20c/256GB nodes \n w/ 4x P40/P100_PCIE/V100_NVLINK\n\n\n\n\n\n\n\n\nAfter the first coexistence phase of the \ntransition\n,\nSherlock 1.0 nodes will be merged in Sherlock 2.0 partitions, and the resulting\ngeneral partitions will be the combination of both. For instance, the \nnormal\n\npartition will feature 172 nodes.\n\n\nJob submission parameters\n#\n\n\nQOS and limits are slightly different too, because of the different\ncharacteristics of Sherlock 2.0, and the new version of Slurm (cf. below).\n\n\nThe main change will be that your jobs won't generally need to specify any QOS,\nexcept for the \nlong\n one. or in other words, \n--qos=long\n is the only QOS\noption you will need to specify in your jobs. The \nbigmem\n and \ngpu\n QOSes are\nnot used anymore for job submission, and will lead to errors if used. The goal\nis to simplify job submission for new users and limit the number of parameters\nto specify when submitting jobs.\n\n\nSoftware\n#\n\n\n\n\nChanges required\n\n\nSherlock 2.0 runs a newer version of the Linux Operating System (OS) and as\na result, most the software available on Sherlock 1.0 needs to be\nrecompiled and reinstalled to work optimally in the new environment.\n\n\n\n\nSherlock 2.0 runs an updated software stack, from kernel to user-space\napplications, including management software. Most user-facing applications\nare new version of the same software, while the management stack has been\ncompletely overhauled and redesigned from scratch.\n\n\nChanges include:\n\n\n\n\n\n\n\n\nMain changes\n\n\nSherlock 1.0\n\n\nSherlock 2.0\n\n\n\n\n\n\n\n\n\n\nOS\n\n\nCentOS 6.x\n\n\nCentOS 7.x\n\n\n\n\n\n\nScheduler\n\n\nSlurm 16.05.x\n\n\nSlurm 17.02.x\n\n\n\n\n\n\n\n\nWhat stays the same?\n#\n\n\nWe're well aware that transitioning to a new system takes time and effort away\nfrom research, and we wanted to minimize disruption. So we've focused our\nattention on revamping and improving the required elements, but we didn't\nchange it all: we kept the best parts as they are.\n\n\nStorage\n#\n\n\nAll the storage systems (including \n$HOME\n, \n$PI_HOME\n, \n$SCRATCH\n, \n$PI_SCRATCH\n\nand \n$OAK\n) are shared between Sherlock 1.0 and Sherlock 2.0. So all the\nfiles and folders available on Sherlock 1.0 will be available on Sherlock 2.0.\n\n\n\n\nPath changes\n\n\nBe advised that the paths of the \n/home\n and \n/share/PI\n directories have\nchanged on Sherlock 2.0.  They are now respectively \n/home/users\n and\n\n/home/groups\n.\n\n\nWe strongly recommend referring to them through the \n$HOME\n and \n$PI_HOME\n\nenvironment variables instead of using the full paths.\n\n\n\n\nFile transfers\n#\n\n\nTo transfer files, you can continue to use the existing data-transfer node\n(DTN): since all the filesystems are shared between the two clusters, files\nuploaded through the DTN will be immediately available on Sherlock 2.0.\n\n\nFor more information, see the \nData transfer\n page.\n\n\nScheduler\n#\n\n\nIn terms of scheduling, the same principles users are already familiar\nwith on Sherlock 1.0 are conserved:\n\n\n\n\na set of general partitions, available to all users: \nnormal\n, \nbigmem\n,\n  \ngpu\n, \ndev\n,\n\n\nowner-specific partitions, regrouping nodes bought by PI groups,\n\n\na global \nowners\n partition, to allow other owners to use idle nodes,\n\n\nsame backfilling and fair share scheduling rules,\n\n\nsimilar limits on most partitions,\n\n\n\n\n\n\nOwners' partition on Sherlock 2.0\n\n\nWhile the two clusters operate as separate entities (phase 1 of the\n\ntransition process\n), owners on Sherlock 1.0 and\nSherlock 2.0 will be considered two distinct groups. Meaning that Sherlock\n1.0 owners won't be able to use the \nowners\n partition on Sherlock 2.0, and\nvice-versa.\n\n\nAs we move through \nPhase 2\n, and as Sherlock 1.0\nowners nodes are folded into Sherlock 2.0, Sherlock 1.0 owners will gain\naccess to the \nowners\n partition on Sherlock 2.0.\n\n\n\n\nFor more details about the scheduler, see the \nRunning Jobs\n page.\n\n\nSoftware modules\n#\n\n\nPart of the current Sherlock user software stack has been ported over to\nSherlock 2.0, with a refreshed but similar \nmodules\n system.\n\n\nThat software stack on Sherlock is an ongoing effort, and we'll be continuing\nto add new applications over the coming days and weeks.\n\n\nHow to differentiate clusters?\n#\n\n\n\n\nDetermine the Sherlock version\n\n\nTo check which cluster environment you're running on, check the value of\nthe \n$SHERLOCK\n environment variable.\n\n\n\n\nTo ease transition to the new software environment, and to make re-using the\nsame scripts more convenient, we provide a way to check which cluster your\njobs are running on. By checking the value of the \n$SHERLOCK\n environment\nvariable, you'll be able to determine which environment your scripts should be\nexpecting.\n\n\n\n\n\n\n\n\nCommand\n\n\nOutput on Sherlock 1.0\n\n\nOutput on Sherlock 2.0\n\n\n\n\n\n\n\n\n\n\necho $SHERLOCK\n\n\n1\n\n\n2\n\n\n\n\n\n\n\n\nSo, for instance, the following batch script will check that environment\nvariable to determine where it's running, and load the appropriate Python\nmodule:\n\n\n#!/bin/bash\n#\n#SBATCH --job-name=test\n#SBATCH --time 10:00\n#\n\nif [[ \n$SHERLOCK\n == \n1\n ]]; then\n    echo \nRunning on Sherlock 1.0\n\n    module load python/2.7.5\n\nelif [[ \n$SHERLOCK\n == \n2\n ]]; then\n    echo \nRunning on Sherlock 2.0\n\n    module load python/2.7.13\n\nelse\n    echo \nUh-oh, not sure where we are...\n\n    exit 1\nfi\n\npython -c \nprint 'Hello Sherlock'\n\n\n\nThat script could be submitted with \nsbatch\n on either cluster, and will\nproduce the exact same output.\n\n\nThe \n$SHERLOCK\n environment variable will remain available throughout all the\n\ntransition phases\n, and will stay defined even after the\ntwo clusters have been merged.\n\n\nFeedback and support\n#\n\n\nYour feedback is very important to us, especially in the early stages of a new\nsystem. So please tell us about any issues you run into and successes you have.\nFeel free to contact us by email at \n.", 
            "title": "Sherlock 2.0"
        }, 
        {
            "location": "/docs/overview/transition/#sherlock-20-transition-guide", 
            "text": "This transition guide is intended for users already familiar with Sherlock, to\nhelp them moving to Sherlock 2.0: differences with the existing, temporary\nconditions, etc.   Sherlock 2.0 orders  If you are looking to buy into Sherlock 2.0, please see the  Sherlock\nQuarterly Order  page ( SUNet ID required ).", 
            "title": "Sherlock 2.0 transition guide"
        }, 
        {
            "location": "/docs/overview/transition/#why-sherlock-20", 
            "text": "Sherlock started in 2014 with 120 general use compute nodes and their\nassociated networking (high-performance interconnect) and storage (Lustre\nscratch filesystem) infrastructure. Those nodes have been consistently heavily\nused.  An additional 730+ nodes were purchased by a wide variety of Stanford  and  SLAC  researchers and added to the\ninitial base.  However, the Infiniband fabric (the supporting network that interconnects all\nof the nodes and storage) eventually reached capacity, effectively putting a\nhalt to Sherlock's growth. With the help of new recurring funding from the\nUniversity, a new, completely separate cluster has been kick-started, with its\nown Infiniband fabric, new management servers, and a set of more\ncapable compute nodes. These new nodes, along with an updated software\nenvironment, forms the basis of Sherlock 2.0.  For the sake of simplicity, we'll refer to the new environment as Sherlock\n2.0, and to the existing one as Sherlock 1.0.", 
            "title": "Why Sherlock 2.0?"
        }, 
        {
            "location": "/docs/overview/transition/#transition-process", 
            "text": "Storage  For the whole duration of the transition process, storage will be shared\nbetween Sherlock 1.0 and Sherlock 2.0 and will be accessible from both,\nmeaning that users will be able to access and work on the the same files\nwhatever cluster they connect to.   To allow every user to smoothly transition to the new system, a 3-phase\nprocess will take place.   Phase 1 : coexistence   For the duration of the transition period, both Sherlock 1.0 and Sherlock 2.0\nwill coexist. As two separate clusters, they will use distinct login nodes and\nprovide access to different hardware and software.   Two isolated clusters  Specific compute nodes will only be accessible from one\ncluster at a time: the new compute nodes from Sherlock 2.0, and the old\nnodes from Sherlock 1.0. It also means that jobs submitted on Sherlock\n1.0 will not visible from Sherlock 2.0, and vice-versa.    Phase 2 : merge   After sufficient time for the majority of users to validate and migrate to\nthe new environment, the original Sherlock nodes will be merged into the new\ncluster. We'll work with individual owners to determine the best time frame for\ntheir own nodes migration, and will announce a date for the migration of all\nthe general nodes from the public partitions.   Phase 3 : retirement   As they age out, compute nodes older than 5 years will be retired, to\ncomply with the data center usage guidelines. When all the remaining Sherlock\n1.0 nodes have been merged into Sherlock 2.0, the Sherlock 1.0 and 2.0\ndistinction will disappear, and Sherlock will be a single system again.", 
            "title": "Transition process"
        }, 
        {
            "location": "/docs/overview/transition/#what-changes", 
            "text": "Quite a lot!   Most notable upgrades  Additional cores per CPU, increased Infiniband throughput, twice the amount\nof memory per node and higher memory/core ratio, increased power\nredundancy, a new OS and software stack, and mandatory 2FA.", 
            "title": "What changes?"
        }, 
        {
            "location": "/docs/overview/transition/#documentation", 
            "text": "As part of our documentation and communication effort, we're announcing\nthree new web resources for Sherlock:   a  changelog  for news and updates, to get a better sense of\n  what's happening on Sherlock, beyond general email announces,  a  status dashboard , where you'll find up-to-date information\n  about the status of the main Sherlock components and services. You could also\n  subscribe to receive notification in case of events or outages,  and of course the very  documentation  you're reading right now.", 
            "title": "Documentation"
        }, 
        {
            "location": "/docs/overview/transition/#login", 
            "text": "The most notable changes are visible right from the connection process.     Main changes  Sherlock 1.0  Sherlock 2.0      Login nodes  sherlock.stanford.edu  login.sherlock.stanford.edu    Authentication scheme  GSSAPI (Kerberos)  SUNet ID password or GSSAPI    Two-step authentication  no  yes      New login hostname   Sherlock 2.0 uses a new set of login nodes, under a different load-balanced\n  hostname. To connect, you'll need to use:\n   $ ssh  sunet @login.sherlock.stanford.edu \n  (note the  login  part of the hostname)   New server fingerprint  When you connect to the new login nodes for the first time, you'll be\npresented with the new servers' fingerprint (host keys). Please see the Connecting  page for more details.    New authentication scheme   GSSAPI (Kerberos) is not required anymore. To make up for the next item, we\n  relaxed the requirement on Kerberos authentication for Sherlock 2.0. Meaning\n  that you will now be able to connect using your SUNet ID password, without\n  having to get a Kerberos ticket via  kinit  first.  We hope this will also\n  ease the connection process from clients and OS which don't correctly support\n  GSSAPI. And please note that if you still want to use Kerberos for\n  authentication, you can.   Two-step authentication   Two-step (or two-factor) authentication (2FA) this is now a requirement in\n  the\u00a0 Minimum Security Standards  mandated by the  Information\n  Security Office , and that we're implementing on Sherlock. Upon\n  connection, you will be prompted for your password (if you're not using\n  GSSAPI) and a two-factor prompt like this:\n   Duo two-factor login for johndoe\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-4425\n 2. Phone call to XXX-XXX-4425\n 3. SMS passcodes to XXX-XXX-4425 (next code starts with: 1)\n\nPasscode or option (1-3): \n    Upon validation of the passcode, you'll be connected to Sherlock's login\n    nodes as usual.    For more details about the login procedure, see the  Connecting \npage.", 
            "title": "Login"
        }, 
        {
            "location": "/docs/overview/transition/#hardware", 
            "text": "A number of changes differentiate Sherlock 2.0 from the first generation:\ntechnology has evolved and those changes are reflected in the new hardware\nspecifications. Here are the main changes on typical compute nodes:     Main changes  Sherlock 1.0  Sherlock 2.0      Nodes  1U Dell R630 servers  0.5U C6320 servers in a 2U chassis    CPUs  2x 8-core (Ivy-Bridge/Haswell)  2x 10-core (Broadwell)    Memory  64G (base node)  128G (base node)    Local storage  200G SSD (+ 500G HDD)  200G SSD    Interconnect  56G FDR Infiniband  100G EDR Infiniband      Nodes   Base node model moves from the Dell R630 1U server to the Dell C6320\n  system. More servers will fit in a single rack, which is essential for us to\n  accommodate the ever-increasing demand for servers.  Another change is that\n  the new model provides redundant power supplies, in case one fails. Each\n  C6320 chassis houses four independent servers in only two rack units of\n  space.   CPUs   Upgrade from Intel  Xeon  E5-2640v3 (8-core, 2.6GHz) to E5-2640v4\n  (10-core, 2.4GHz), meaning that each CPU gets two additional cores, and that\n  to maintain a 90W power envelope, the clock speed changed from 2.6GHz to\n  2.4GHz.   Memory   Because of the 2 additional cores per CPU, a total for 4 more per node,\n  having a 64GB minimum RAM requirement was no longer deemed to be a good fit.\n  Originally, 64GB for 16 cores gave a ratio of 4GB/core. With the same 64GB\n  per node, the memory per core ratio on Sherlock 2.0 would have dropped to\n  3.2GB/core.  Usage statistics showed that more RAM per core has been needed\n  for a wide range of applications.  So for Sherlock 2.0, the minimum memory\n  amount per node has be set to 128GB, or 6.4GB/core, a much more appropriate\n  ratio for the typical user needs.   Local storage   The slightly smaller form factor of each node has prompted\n  elimination of the 500G spinning hard drive, keeping the 200G SSD.  Sherlock\n  statistics showed that the hard drive was rarely used, as users favored use\n  of the very fast Lustre /scratch file system available to all nodes.   Interconnect   Substantial network improvement by way of an almost 2x increase in\n  Infiniband (IB) speed. Sherlock 1.0 uses the FDR IB speed of 56Gbps, while\n  Sherlock 2.0 uses EDR IB at 100Gbps.", 
            "title": "Hardware"
        }, 
        {
            "location": "/docs/overview/transition/#slurm-partitions", 
            "text": "The same organization of compute nodes will be used on Sherlock 2.0, with the\nsame partition nomenclature, although different hardware characteristics:     Main changes  Sherlock 1.0  Sherlock 2.0      normal  116x 16c/64GB nodes  56x 20c/128GB nodes    dev  2x 16c/64GB nodes  2x 20c/128GB nodes    bigmem  2x 32c/1.5TB nodes  1x 56c/3.0TB node  1x 32c/512GB node    gpu  6x 16c/256GB nodes   w/ 8x K20/K80/TITAN Black  5x 20c/256GB nodes   w/ 4x P40/P100_PCIE/V100_NVLINK     After the first coexistence phase of the  transition ,\nSherlock 1.0 nodes will be merged in Sherlock 2.0 partitions, and the resulting\ngeneral partitions will be the combination of both. For instance, the  normal \npartition will feature 172 nodes.", 
            "title": "Slurm partitions"
        }, 
        {
            "location": "/docs/overview/transition/#job-submission-parameters", 
            "text": "QOS and limits are slightly different too, because of the different\ncharacteristics of Sherlock 2.0, and the new version of Slurm (cf. below).  The main change will be that your jobs won't generally need to specify any QOS,\nexcept for the  long  one. or in other words,  --qos=long  is the only QOS\noption you will need to specify in your jobs. The  bigmem  and  gpu  QOSes are\nnot used anymore for job submission, and will lead to errors if used. The goal\nis to simplify job submission for new users and limit the number of parameters\nto specify when submitting jobs.", 
            "title": "Job submission parameters"
        }, 
        {
            "location": "/docs/overview/transition/#software", 
            "text": "Changes required  Sherlock 2.0 runs a newer version of the Linux Operating System (OS) and as\na result, most the software available on Sherlock 1.0 needs to be\nrecompiled and reinstalled to work optimally in the new environment.   Sherlock 2.0 runs an updated software stack, from kernel to user-space\napplications, including management software. Most user-facing applications\nare new version of the same software, while the management stack has been\ncompletely overhauled and redesigned from scratch.  Changes include:     Main changes  Sherlock 1.0  Sherlock 2.0      OS  CentOS 6.x  CentOS 7.x    Scheduler  Slurm 16.05.x  Slurm 17.02.x", 
            "title": "Software"
        }, 
        {
            "location": "/docs/overview/transition/#what-stays-the-same", 
            "text": "We're well aware that transitioning to a new system takes time and effort away\nfrom research, and we wanted to minimize disruption. So we've focused our\nattention on revamping and improving the required elements, but we didn't\nchange it all: we kept the best parts as they are.", 
            "title": "What stays the same?"
        }, 
        {
            "location": "/docs/overview/transition/#storage", 
            "text": "All the storage systems (including  $HOME ,  $PI_HOME ,  $SCRATCH ,  $PI_SCRATCH \nand  $OAK ) are shared between Sherlock 1.0 and Sherlock 2.0. So all the\nfiles and folders available on Sherlock 1.0 will be available on Sherlock 2.0.   Path changes  Be advised that the paths of the  /home  and  /share/PI  directories have\nchanged on Sherlock 2.0.  They are now respectively  /home/users  and /home/groups .  We strongly recommend referring to them through the  $HOME  and  $PI_HOME \nenvironment variables instead of using the full paths.", 
            "title": "Storage"
        }, 
        {
            "location": "/docs/overview/transition/#file-transfers", 
            "text": "To transfer files, you can continue to use the existing data-transfer node\n(DTN): since all the filesystems are shared between the two clusters, files\nuploaded through the DTN will be immediately available on Sherlock 2.0.  For more information, see the  Data transfer  page.", 
            "title": "File transfers"
        }, 
        {
            "location": "/docs/overview/transition/#scheduler", 
            "text": "In terms of scheduling, the same principles users are already familiar\nwith on Sherlock 1.0 are conserved:   a set of general partitions, available to all users:  normal ,  bigmem ,\n   gpu ,  dev ,  owner-specific partitions, regrouping nodes bought by PI groups,  a global  owners  partition, to allow other owners to use idle nodes,  same backfilling and fair share scheduling rules,  similar limits on most partitions,    Owners' partition on Sherlock 2.0  While the two clusters operate as separate entities (phase 1 of the transition process ), owners on Sherlock 1.0 and\nSherlock 2.0 will be considered two distinct groups. Meaning that Sherlock\n1.0 owners won't be able to use the  owners  partition on Sherlock 2.0, and\nvice-versa.  As we move through  Phase 2 , and as Sherlock 1.0\nowners nodes are folded into Sherlock 2.0, Sherlock 1.0 owners will gain\naccess to the  owners  partition on Sherlock 2.0.   For more details about the scheduler, see the  Running Jobs  page.", 
            "title": "Scheduler"
        }, 
        {
            "location": "/docs/overview/transition/#software-modules", 
            "text": "Part of the current Sherlock user software stack has been ported over to\nSherlock 2.0, with a refreshed but similar  modules  system.  That software stack on Sherlock is an ongoing effort, and we'll be continuing\nto add new applications over the coming days and weeks.", 
            "title": "Software modules"
        }, 
        {
            "location": "/docs/overview/transition/#how-to-differentiate-clusters", 
            "text": "Determine the Sherlock version  To check which cluster environment you're running on, check the value of\nthe  $SHERLOCK  environment variable.   To ease transition to the new software environment, and to make re-using the\nsame scripts more convenient, we provide a way to check which cluster your\njobs are running on. By checking the value of the  $SHERLOCK  environment\nvariable, you'll be able to determine which environment your scripts should be\nexpecting.     Command  Output on Sherlock 1.0  Output on Sherlock 2.0      echo $SHERLOCK  1  2     So, for instance, the following batch script will check that environment\nvariable to determine where it's running, and load the appropriate Python\nmodule:  #!/bin/bash\n#\n#SBATCH --job-name=test\n#SBATCH --time 10:00\n#\n\nif [[  $SHERLOCK  ==  1  ]]; then\n    echo  Running on Sherlock 1.0 \n    module load python/2.7.5\n\nelif [[  $SHERLOCK  ==  2  ]]; then\n    echo  Running on Sherlock 2.0 \n    module load python/2.7.13\n\nelse\n    echo  Uh-oh, not sure where we are... \n    exit 1\nfi\n\npython -c  print 'Hello Sherlock'  That script could be submitted with  sbatch  on either cluster, and will\nproduce the exact same output.  The  $SHERLOCK  environment variable will remain available throughout all the transition phases , and will stay defined even after the\ntwo clusters have been merged.", 
            "title": "How to differentiate clusters?"
        }, 
        {
            "location": "/docs/overview/transition/#feedback-and-support", 
            "text": "Your feedback is very important to us, especially in the early stages of a new\nsystem. So please tell us about any issues you run into and successes you have.\nFeel free to contact us by email at  .", 
            "title": "Feedback and support"
        }, 
        {
            "location": "/docs/overview/specs/", 
            "text": "Technical specifications\n#\n\n\n\n\nNote\n\n\nSherlock is driven by contributions from individual PIs and groups, and as\nsuch, is constantly evolving.  The technical specifications outlined here\nare subject to change, and may not be an accurate representation of the\ncurrent cluster configuration. The numbers provided on this page are as of\n\nJune 2017\n.\n\n\n\n\nIn a nutshell\n#\n\n\nSherlock features more than 1,000 compute nodes, 18,000+ CPU cores, 120TB of\ntotal memory, 400+ GPUs, for a total computing power of more than 1 Petaflop.\nThat would rank it in the Top500 list of the most powerful supercomputers in\nthe world.\n\n\nA parallel high-performance filesystem of more than 3 PB, delivering over\n20GB/s of sustained I/O bandwidth, provides scratch storage for more than 2,300\nusers, and 400 PI groups.\n\n\nComputing\n#\n\n\nThe Sherlock cluster has been initiated in January 2014 with a base of freely\navailable computing resources and the accompanying networking and storage\ninfrastructure. It has since been expanded with additions from multiple PI\ngroups to reach the capacity of its Infinband network in December 2016.\n\n\n\n\nSherlock 2.0\n\n\nA new Infiniband fabric has been installed in early 2017, as the foundation\nfor Sherlock 2.0. The existing nodes will join that new cluster in the\nsecond half of 2017, at which point both clusters will be merged.\n\n\n\n\nSherlock 1.0\n#\n\n\n\n\n\n\n\n\nType\n\n\nQty\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nlogin nodes\n\n\n4\n\n\nsherlock.stanford.edu\n (load-balanced)\n\n\n\n\n\n\ndata transfer node\n\n\n1\n\n\ndedicated bandwidth for large data transfers\n\n\n\n\n\n\ncompute nodes\n\n\n120\n\n\n16 cores\n1\n, 64 GB RAM, 100 GB local SSD\n\n\n\n\n\n\nbigmem\n nodes\n\n\n2\n\n\n32 cores\n2\n, 1.5 TB RAM, 13TB of local storage\n\n\n\n\n\n\nGPU nodes\n\n\n5\n\n\n16 cores\n1\n, 64 GB RAM, 200 GB local SSD, 8 GPUs\nNVIDIA Tesla K20Xm, K80, or GeForce GTX TITAN Black\n\n\n\n\n\n\nowners nodes\n\n\n716\n\n\nvarious CPU/memory configs, \nbigmem\n and GPU nodes\n\n\n\n\n\n\ninterconnect\n\n\n\n\n2:1 oversubscribed FDR Infiniband fabric (56 GB/s)\n\n\n\n\n\n\noperating system\n\n\n\n\nCentOS 6.x\n\n\n\n\n\n\n\n\nSherlock 2.0\n#\n\n\n\n\n\n\n\n\nType\n\n\nQty\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nlogin nodes\n\n\n2\n\n\nlogin.sherlock.stanford.edu\n (load-balanced)\n\n\n\n\n\n\ncompute nodes\n\n\n60\n\n\n20 cores\n3\n, 128 GB RAM, 200 GB local SSD\n\n\n\n\n\n\nbigmem\n nodes\n\n\n2\n\n\n56 cores\n4\n, 3.0 TB RAM, 200 GB local SSD\n32 cores\n5\n, 512 GB RAM, 200GB local SSD\n\n\n\n\n\n\nGPU nodes\n\n\n2\n\n\n20 cores\n3\n, 128 GB RAM, 200 GB local SSD, 4 GPUs\nNVIDIA Tesla P100-PCIE-16GB, Tesla P40\n\n\n\n\n\n\nowners nodes\n\n\n160\n\n\nvarious CPU/memory configs, \nbigmem\n and GPU nodes\n\n\n\n\n\n\ninterconnect\n\n\n\n\n2:1 oversubscribed EDR Infiniband fabric (100 GB/s)\n\n\n\n\n\n\noperating system\n\n\n\n\nCentOS 7.x\n\n\n\n\n\n\n\n\nStorage\n#\n\n\n\n\nMore information\n\n\nFor more information about storage options on Sherlock, please refer to the\n\nStorage section\n of the documentation.\n\n\n\n\nStorage components are common to both clusters, meaning users can find the same\nfiles and directories from both Sherlock 1.0 and Sherlock 2.0 nodes.\n\n\n\n\nHighly-available NFS filesystem for user and group home directories (with hourly\n  snapshots and off-site replication)\n\n\nHigh-performance Lustre scratch filesystem (3 PB, 20+GB/s sustained I/O bandwidth, 18 I/O servers, 1,080 disks)\n\n\nDirect access to SRCC's \nOak\n long-term research data storage system\n\n\n\n\n\n\n\n\n\n\n\n\ntwo-socket Intel\n Xeon\n \nE5-2650v2\n processors (8-core Ivy-Bridge, 2.60 GHz)\n\n\n\n\n\n\nfour-socket Intel\n Xeon\n \nE5-4640\n processors (8-core Sandy-Bridge, 2.40 GHz)\n\n\n\n\n\n\ntwo-socket Intel\n Xeon\n \nE5-2640v4\n processors (10-core Broadwell, 2.40 GHz)\n\n\n\n\n\n\nfour-socket Intel\n Xeon\n \nE5-4650v4\n processors (14-core Broadwell, 2.20 GHz)\n\n\n\n\n\n\ntwo-socket Intel\n Xeon\n \nE5-2697Av4\n processors (16-core Broadwell, 2.60 GHz)", 
            "title": "Tech specs"
        }, 
        {
            "location": "/docs/overview/specs/#technical-specifications", 
            "text": "Note  Sherlock is driven by contributions from individual PIs and groups, and as\nsuch, is constantly evolving.  The technical specifications outlined here\nare subject to change, and may not be an accurate representation of the\ncurrent cluster configuration. The numbers provided on this page are as of June 2017 .", 
            "title": "Technical specifications"
        }, 
        {
            "location": "/docs/overview/specs/#in-a-nutshell", 
            "text": "Sherlock features more than 1,000 compute nodes, 18,000+ CPU cores, 120TB of\ntotal memory, 400+ GPUs, for a total computing power of more than 1 Petaflop.\nThat would rank it in the Top500 list of the most powerful supercomputers in\nthe world.  A parallel high-performance filesystem of more than 3 PB, delivering over\n20GB/s of sustained I/O bandwidth, provides scratch storage for more than 2,300\nusers, and 400 PI groups.", 
            "title": "In a nutshell"
        }, 
        {
            "location": "/docs/overview/specs/#computing", 
            "text": "The Sherlock cluster has been initiated in January 2014 with a base of freely\navailable computing resources and the accompanying networking and storage\ninfrastructure. It has since been expanded with additions from multiple PI\ngroups to reach the capacity of its Infinband network in December 2016.   Sherlock 2.0  A new Infiniband fabric has been installed in early 2017, as the foundation\nfor Sherlock 2.0. The existing nodes will join that new cluster in the\nsecond half of 2017, at which point both clusters will be merged.", 
            "title": "Computing"
        }, 
        {
            "location": "/docs/overview/specs/#sherlock-10", 
            "text": "Type  Qty  Details      login nodes  4  sherlock.stanford.edu  (load-balanced)    data transfer node  1  dedicated bandwidth for large data transfers    compute nodes  120  16 cores 1 , 64 GB RAM, 100 GB local SSD    bigmem  nodes  2  32 cores 2 , 1.5 TB RAM, 13TB of local storage    GPU nodes  5  16 cores 1 , 64 GB RAM, 200 GB local SSD, 8 GPUs NVIDIA Tesla K20Xm, K80, or GeForce GTX TITAN Black    owners nodes  716  various CPU/memory configs,  bigmem  and GPU nodes    interconnect   2:1 oversubscribed FDR Infiniband fabric (56 GB/s)    operating system   CentOS 6.x", 
            "title": "Sherlock 1.0"
        }, 
        {
            "location": "/docs/overview/specs/#sherlock-20", 
            "text": "Type  Qty  Details      login nodes  2  login.sherlock.stanford.edu  (load-balanced)    compute nodes  60  20 cores 3 , 128 GB RAM, 200 GB local SSD    bigmem  nodes  2  56 cores 4 , 3.0 TB RAM, 200 GB local SSD 32 cores 5 , 512 GB RAM, 200GB local SSD    GPU nodes  2  20 cores 3 , 128 GB RAM, 200 GB local SSD, 4 GPUs NVIDIA Tesla P100-PCIE-16GB, Tesla P40    owners nodes  160  various CPU/memory configs,  bigmem  and GPU nodes    interconnect   2:1 oversubscribed EDR Infiniband fabric (100 GB/s)    operating system   CentOS 7.x", 
            "title": "Sherlock 2.0"
        }, 
        {
            "location": "/docs/overview/specs/#storage", 
            "text": "More information  For more information about storage options on Sherlock, please refer to the Storage section  of the documentation.   Storage components are common to both clusters, meaning users can find the same\nfiles and directories from both Sherlock 1.0 and Sherlock 2.0 nodes.   Highly-available NFS filesystem for user and group home directories (with hourly\n  snapshots and off-site replication)  High-performance Lustre scratch filesystem (3 PB, 20+GB/s sustained I/O bandwidth, 18 I/O servers, 1,080 disks)  Direct access to SRCC's  Oak  long-term research data storage system       two-socket Intel  Xeon   E5-2650v2  processors (8-core Ivy-Bridge, 2.60 GHz)    four-socket Intel  Xeon   E5-4640  processors (8-core Sandy-Bridge, 2.40 GHz)    two-socket Intel  Xeon   E5-2640v4  processors (10-core Broadwell, 2.40 GHz)    four-socket Intel  Xeon   E5-4650v4  processors (14-core Broadwell, 2.20 GHz)    two-socket Intel  Xeon   E5-2697Av4  processors (16-core Broadwell, 2.60 GHz)", 
            "title": "Storage"
        }, 
        {
            "location": "/docs/overview/status/", 
            "text": "var statusWidget = new Status.Widget({\n    hostname: \"status.sherlock.stanford.edu\",\n    selector: \"#sh_status\",\n    display: {\n        ledPosition: \"left\",\n    }\n  });\n\n\n\n\n\n.status-widget__state {\n  font-size: 1.6rem;\n}\n.status-widget__led {\n  height: 12px;\n  width:  11px;\n}\n.status-widget__issue {\n  line-height: normal;\n}\n.status-widget__issue__title,\n.status-widget__issue__body {\n  padding: 5px 0;\n}\n\n\n\n\nScheduled maintenances\n#\n\n\nMaintenance operations and upgrades\n are\nscheduled on Sherlock on a regular basis.  Per the University's \nMinimum\nSecurity policies\n, we deploy security patches on Sherlock as\nrequired for compliance.\n\n\nComponents and services\n#\n\n\nSherlock status is \n\n\nFor more details about Sherlock components and services, see the \nstatus\ndashboard\n.\n\n\nCurrent usage\n#", 
            "title": "Status"
        }, 
        {
            "location": "/docs/overview/status/#scheduled-maintenances", 
            "text": "Maintenance operations and upgrades  are\nscheduled on Sherlock on a regular basis.  Per the University's  Minimum\nSecurity policies , we deploy security patches on Sherlock as\nrequired for compliance.", 
            "title": "Scheduled maintenances"
        }, 
        {
            "location": "/docs/overview/status/#components-and-services", 
            "text": "Sherlock status is   For more details about Sherlock components and services, see the  status\ndashboard .", 
            "title": "Components and services"
        }, 
        {
            "location": "/docs/overview/status/#current-usage", 
            "text": "", 
            "title": "Current usage"
        }, 
        {
            "location": "/docs/overview/about/", 
            "text": "About us\n#\n\n\n\nimg[alt=\"logo\"] {\n    width: 400px;\n}\n\n\n\n\n\n\n\n\n\nSRCC\n#\n\n\n\n\n\n\n\n\nThe Stanford Research Computing Center (\nSRCC\n) is a joint effort of\nthe \nDean of Research\n and \nIT Services\n to build and support a\ncomprehensive program to advance computational research at Stanford.  That\nincludes offering and supporting traditional high performance computing (HPC)\nsystems, as well as systems for high throughput and data-intensive computing.\n\n\nThe SRCC also helps researchers transition their analyses and models from the\ndesktop to more capable and plentiful resources, providing the opportunity to\nexplore their data and answer research questions at a scale typically not\npossible on desktops or departmental servers. Partnering with national\ninitiatives like NSF \nXSEDE\n program as well as vendors, the SRCC\noffers training and learning opportunities around HPC tools and technologies.\n\n\nFor more information, please see the \nSRCC website\n\n\nCredits\n#\n\n\nWe would like to thank the following companies for their generous sponsorship,\nand for providing services and resources that help us manage Sherlock every\nday:\n\n\n\n\n\n\nimg[alt=\"favicon\"] {\n    bottom: -3px;\n    height: 18px;\n    position:relative;\n}\nimg[alt=\"screencap\"] {\n    width: 320px;\n    margin: 5px;\n}\n.md-typeset__table{\n    width: 100%;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nZenHub\n\n\n \nGitHub\n\n\n \nTravis CI\n\n\n \nHund\n\n\n \nHeadway\n\n\n \nupdown.io\n\n\n\n\n\n\n\n\nThe Sherlock website and documentation also rely on the following projects:\n\n\n\n\nMkDocs\n\n\nMaterial for MkDocs\n\n\nAgency Bootstrap\n\n\n\n\nWhy the Sherlock name?\n#\n\n\nIf you're curious about where the \nSherlock\n name came from, we always\nconsidered that computing resources in general and HPC clusters in particular\nshould be the catalyst of innovation, be ahead of their time, and spur new\ndiscoveries.\n\n\nAnd what better account of what's happening on a high-performance computing\ncluster than Benedict Cumberbatch \ndescribing\n his role as Sherlock\nHolmes in the BBC's modern adaptation of Arthur Conan Doyle's classic?\n\n\n\n\nBenedict Cumberbatch, about Sherlock\n\n\nThere's a great charge you get from playing him, because of the volume of\nwords in your head and the speed of thought \u2013 you really have to make your\nconnections incredibly fast. He is one step ahead of the audience, and of\nanyone around him with normal intellect. They can't quite fathom where his\nleaps are taking him.\n\n\n\n\nYes, exactly. That's Sherlock.\n\n\nSherlock, of HBO fame\n#\n\n\nAnd finally, we couldn't resist to the pleasure of citing the most\nprestigious accomplishment of Sherlock to date: a mention in \nHBO's Silicon\nValley\n \nSeason 4 finale\n!\n\n\n\n\n\n\nYep, you got that right, \nRichard Hendricks\n wanted to use our\nvery own Sherlock. Kudos to the show's crew and whomever did the research and\ngot it right, you made our day.", 
            "title": "About"
        }, 
        {
            "location": "/docs/overview/about/#about-us", 
            "text": "img[alt=\"logo\"] {\n    width: 400px;\n}", 
            "title": "About us"
        }, 
        {
            "location": "/docs/overview/about/#srcc", 
            "text": "The Stanford Research Computing Center ( SRCC ) is a joint effort of\nthe  Dean of Research  and  IT Services  to build and support a\ncomprehensive program to advance computational research at Stanford.  That\nincludes offering and supporting traditional high performance computing (HPC)\nsystems, as well as systems for high throughput and data-intensive computing.  The SRCC also helps researchers transition their analyses and models from the\ndesktop to more capable and plentiful resources, providing the opportunity to\nexplore their data and answer research questions at a scale typically not\npossible on desktops or departmental servers. Partnering with national\ninitiatives like NSF  XSEDE  program as well as vendors, the SRCC\noffers training and learning opportunities around HPC tools and technologies.  For more information, please see the  SRCC website", 
            "title": "SRCC"
        }, 
        {
            "location": "/docs/overview/about/#credits", 
            "text": "We would like to thank the following companies for their generous sponsorship,\nand for providing services and resources that help us manage Sherlock every\nday:   \nimg[alt=\"favicon\"] {\n    bottom: -3px;\n    height: 18px;\n    position:relative;\n}\nimg[alt=\"screencap\"] {\n    width: 320px;\n    margin: 5px;\n}\n.md-typeset__table{\n    width: 100%;\n}                 ZenHub    GitHub    Travis CI    Hund    Headway    updown.io     The Sherlock website and documentation also rely on the following projects:   MkDocs  Material for MkDocs  Agency Bootstrap", 
            "title": "Credits"
        }, 
        {
            "location": "/docs/overview/about/#why-the-sherlock-name", 
            "text": "If you're curious about where the  Sherlock  name came from, we always\nconsidered that computing resources in general and HPC clusters in particular\nshould be the catalyst of innovation, be ahead of their time, and spur new\ndiscoveries.  And what better account of what's happening on a high-performance computing\ncluster than Benedict Cumberbatch  describing  his role as Sherlock\nHolmes in the BBC's modern adaptation of Arthur Conan Doyle's classic?   Benedict Cumberbatch, about Sherlock  There's a great charge you get from playing him, because of the volume of\nwords in your head and the speed of thought \u2013 you really have to make your\nconnections incredibly fast. He is one step ahead of the audience, and of\nanyone around him with normal intellect. They can't quite fathom where his\nleaps are taking him.   Yes, exactly. That's Sherlock.", 
            "title": "Why the Sherlock name?"
        }, 
        {
            "location": "/docs/overview/about/#sherlock-of-hbo-fame", 
            "text": "And finally, we couldn't resist to the pleasure of citing the most\nprestigious accomplishment of Sherlock to date: a mention in  HBO's Silicon\nValley   Season 4 finale !    Yep, you got that right,  Richard Hendricks  wanted to use our\nvery own Sherlock. Kudos to the show's crew and whomever did the research and\ngot it right, you made our day.", 
            "title": "Sherlock, of HBO fame"
        }, 
        {
            "location": "/docs/getting-started/prerequisites/", 
            "text": "To start using Sherlock, you will need:\n\n\n\n\n\n\nan active \nSUNet ID\n,\n\n\n\n\nSUNet ID \nservice levels\n and external collaborators\n\n\nBase-level service is sufficient for Sherlock accounts. External\ncollaborators, or users without a SUNet ID, can be sponsored by a PI a\nget a sponsored SUNet ID at no cost. Please see the \nsponsorship\npage\n for more information.\n\n\n\n\n\n\n\n\na \nSherlock account\n,\n\n\n\n\na \nSSH client\n,\n\n\ngood understanding of the \nconcepts\n and \nterms\n\n  used throughout that documentation,\n\n\nsome familiarity with \nUnix/Linux command-line environments\n, and\n  notions of \nshell scripting\n.\n\n\n\n\nHow to request an account\n#\n\n\nTo request an account, the sponsoring Stanford faculty member should email\n\n, specifying the names and SUNet IDs of\nhis/her research team members needing an account.\n\n\nSherlock is open to the Stanford community as a computing resource to support\ndepartmental or sponsored research, thus a faculty member's explicit consent is\nrequired for account requests.\n\n\n\n\nSherlock is a resource for research\n\n\nSherlock is a resource to help and support research, and is not suitable\nfor course work, class assignments or general-use training sessions.\n\n\n\n\nThere is no fee associated with using Sherlock, and no limit in the amount of\naccounts each faculty member can request. We will periodically ensure that all\naccounts associated with each PI are still active, and reserve the right to\nclose any Sherlock account whose SUNet ID is expired.\n\n\nSSH clients\n#\n\n\nLinux \n#\n\n\nLinux distributions usually come with a version of the \nOpenSSH\n\nclient already installed. So no additional software installation is required.\nIf not, please refer to your distribution's documentation to install it.\n\n\nMacOS \n#\n\n\nMacOS systems usually come with a version of the \nOpenSSH\n client\nalready installed. So no additional software installation is required\n\n\nWindows \n#\n\n\nMicrosoft Windows doesn't provide any SSH client by default. To install one,\nyou have several options, depending on the version of Windows.\n\n\n\n\n\n\nWSL \nrecommended\n\n\nWindows 10 provides a feature called the \n\"Windows\nSubsystem for Linux\"\n (WSL). Please refer to the \nofficial\ndocumentation\n or \nthis howto\n for installation\ninstructions. Once installed, you'll be able to use the \nssh\n command from a\nWindows terminal to connect to Sherlock.\n\n\n\n\n\n\nCygwin\n\n\nThe \nCygwin project\n predates WSL and provides similar features,\nwhich among other things, allow users to install a command-line SSH client on\ntheir Windows machines.\n\n\n\n\n\n\nThe two options above will ensure the best compatibility with the Sherlock\nenvironment. If you'd like to explore other avenues, many \nother SSH client\nimplementations\n are available, but have not necessarily been tested\nwith Sherlock, so your mileage may vary.\n\n\nUnix/Linux resources\n#\n\n\nA full tutorial on using Unix/Linux is beyond the scope of this documentation.\nHowever, there are many tutorials for beginning to use Unix/Linux on the web.\n\n\nA few tutorials we recommend are:\n\n\n\n\nUnix Tutorial for Beginners\n (University of Surrey, UK)\n\n\nIntroduction to Unix\n (Imperial College, London)\n\n\nThe Unix Shell\n (Software Carpentry)\n\n\n\n\nMore specifically about HPC:\n\n\n\n\nIntro to HPC\n (HPC Carpentry)\n\n\nHPC in a day\n (Software Carpentry}\n\n\n\n\nText editors\n#\n\n\nMultiple text editors are available on Sherlock. For beginners, we recommend\nthe use of \nnano\n. And for more advanced uses, you'll also find below some\nresources about using \nvim\n\n\n\n\nNano guide\n (Gentoo wiki)\n\n\nVim guide\n (Gentoo wiki)\n\n\n\n\nShell scripting\n#\n\n\nCompute jobs launched on Sherlock are most often initialized by user-written\nshell scripts. Beyond that, many common operations can be simplified and\nautomated using shell scripts.\n\n\nFor an introduction to shell scripting, you can refer to:\n\n\n\n\nBash Programming - Introduction HOWTO", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/docs/getting-started/prerequisites/#how-to-request-an-account", 
            "text": "To request an account, the sponsoring Stanford faculty member should email , specifying the names and SUNet IDs of\nhis/her research team members needing an account.  Sherlock is open to the Stanford community as a computing resource to support\ndepartmental or sponsored research, thus a faculty member's explicit consent is\nrequired for account requests.   Sherlock is a resource for research  Sherlock is a resource to help and support research, and is not suitable\nfor course work, class assignments or general-use training sessions.   There is no fee associated with using Sherlock, and no limit in the amount of\naccounts each faculty member can request. We will periodically ensure that all\naccounts associated with each PI are still active, and reserve the right to\nclose any Sherlock account whose SUNet ID is expired.", 
            "title": "How to request an account"
        }, 
        {
            "location": "/docs/getting-started/prerequisites/#ssh-clients", 
            "text": "", 
            "title": "SSH clients"
        }, 
        {
            "location": "/docs/getting-started/prerequisites/#linux", 
            "text": "Linux distributions usually come with a version of the  OpenSSH \nclient already installed. So no additional software installation is required.\nIf not, please refer to your distribution's documentation to install it.", 
            "title": "Linux"
        }, 
        {
            "location": "/docs/getting-started/prerequisites/#macos", 
            "text": "MacOS systems usually come with a version of the  OpenSSH  client\nalready installed. So no additional software installation is required", 
            "title": "MacOS"
        }, 
        {
            "location": "/docs/getting-started/prerequisites/#windows", 
            "text": "Microsoft Windows doesn't provide any SSH client by default. To install one,\nyou have several options, depending on the version of Windows.    WSL  recommended  Windows 10 provides a feature called the  \"Windows\nSubsystem for Linux\"  (WSL). Please refer to the  official\ndocumentation  or  this howto  for installation\ninstructions. Once installed, you'll be able to use the  ssh  command from a\nWindows terminal to connect to Sherlock.    Cygwin  The  Cygwin project  predates WSL and provides similar features,\nwhich among other things, allow users to install a command-line SSH client on\ntheir Windows machines.    The two options above will ensure the best compatibility with the Sherlock\nenvironment. If you'd like to explore other avenues, many  other SSH client\nimplementations  are available, but have not necessarily been tested\nwith Sherlock, so your mileage may vary.", 
            "title": "Windows"
        }, 
        {
            "location": "/docs/getting-started/prerequisites/#unixlinux-resources", 
            "text": "A full tutorial on using Unix/Linux is beyond the scope of this documentation.\nHowever, there are many tutorials for beginning to use Unix/Linux on the web.  A few tutorials we recommend are:   Unix Tutorial for Beginners  (University of Surrey, UK)  Introduction to Unix  (Imperial College, London)  The Unix Shell  (Software Carpentry)   More specifically about HPC:   Intro to HPC  (HPC Carpentry)  HPC in a day  (Software Carpentry}", 
            "title": "Unix/Linux resources"
        }, 
        {
            "location": "/docs/getting-started/prerequisites/#text-editors", 
            "text": "Multiple text editors are available on Sherlock. For beginners, we recommend\nthe use of  nano . And for more advanced uses, you'll also find below some\nresources about using  vim   Nano guide  (Gentoo wiki)  Vim guide  (Gentoo wiki)", 
            "title": "Text editors"
        }, 
        {
            "location": "/docs/getting-started/prerequisites/#shell-scripting", 
            "text": "Compute jobs launched on Sherlock are most often initialized by user-written\nshell scripts. Beyond that, many common operations can be simplified and\nautomated using shell scripts.  For an introduction to shell scripting, you can refer to:   Bash Programming - Introduction HOWTO", 
            "title": "Shell scripting"
        }, 
        {
            "location": "/docs/getting-started/connecting/", 
            "text": "Connecting \nto Sherlock \n#\n\n\n\n\nSherlock account required\n\n\nTo be able to connect to Sherlock, you must first obtain a \nSherlock\naccount\n.\n\n\n\n\nCredentials\n#\n\n\nAll users must have a \nStanford SUNet ID\n and a \nSherlock\naccount\n to log in to Sherlock. Your Sherlock account uses the\nsame username/password as your SUnet ID:\n\n\nUsername: SUNet ID\nPassword: SUNet ID password\n\n\n\n\nTo request a Sherlock account, please see the \nPrerequisites\n\npage.\n\n\n\n\nResetting passwords\n\n\nSherlock does \nnot\n store your SUNet ID password. As a consequence, we\nare unable to reset your password. If you require password assistance,\nplease see the \nSUNet Account page\n.\n\n\n\n\nConnection\n#\n\n\nAccess to Sherlock is provided via Secure Shell (SSH) login. Most Unix-like\noperating systems provide an SSH client by default that can be accessed by\ntyping the \nssh\n command in a terminal window.\n\n\nTo login to Sherlock, open a terminal and type the following command, where\n\nsunetid\n should be replaced by your \nactual\n SUNet ID:\n\n\n$ ssh \nsunetid\n@login.sherlock.stanford.edu\n\n\n\nUpon logging in, you will be connected to one of Sherlock's load-balanced login\nnode. You should be automatically directed to the least-loaded login node at\nthe moment of your connection, which should give you the best possible\nenvironment to work.\n\n\nHost keys\n#\n\n\nUpon your very first connection to Sherlock, you will be greeted by a warning\nsuch as :\n\n\nThe authenticity of host 'login.sherlock.stanford.edu' can't be established.\nECDSA key fingerprint is SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg.\nAre you sure you want to continue connecting (yes/no)?\n\n\n\n\nThis warning is normal: your SSH client warns you that it is the first time it\nsees that new computer. To make sure you are actually connecting to the right\nmachine, you should compare the ECDSA key fingerprint shown in the message with\none of the fingerprints below:\n\n\n\n\n\n\n\n\nKey type\n\n\nKey Fingerprint\n\n\n\n\n\n\n\n\n\n\nRSA\n\n\nSHA256:T1q1Tbq8k5XBD5PIxvlCfTxNMi1ORWwKNRPeZPXUfJA\nlegacy format: \nf5:8f:01:46:d1:f9:66:5d:33:58:b4:82:d8:4a:34:41\n\n\n\n\n\n\nECDSA\n\n\nSHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg\nlegacy format: \n70:4c:76:ea:ae:b2:0f:81:4b:9c:c6:5a:52:4c:7f:64\n\n\n\n\n\n\n\n\nIf they match, you can proceed and type \u2018yes\u2019. Your SSH program will then store\nthat key and will verify it for every subsequent SSH connection, to make sure\nthat the server you're connecting to is indeed Sherlock.\n\n\nAuthentication\n#\n\n\nPassword\n#\n\n\nTo ease access and increase compatibility\n1\n with different\nplatforms, Sherlock allows a simple password-based authentication mechanism for\nSSH.\n2\n.\n\n\nUpon connection, you will be asked for your SUNet ID password with the\nfollowing prompt:\n\n\nsunetid\n@login.sherlock.stanford.edu's password:\n\n\n\n\nEnter your password, and if it's correct, you should see the following line:\n\n\nAuthenticated with partial success.\n\n\n\n\nSecond factor (2FA)\n#\n\n\nSherlock implements Stanford's \nMinimum Security Standards\n\npolicies which mandate two-step authentication to access the cluster.\n\n\nTwo-step authentication protects your personal information and credentials by\ncombining something only you \nknow\n (your password) with something only you\n\nhave\n (your phone, tablet or token). This prevents an attacker who would steal\nyour password to actually use it to impersonate you. For more details about\ntwo-step authentication at Stanford, please refer to the \nUniversity IT\ntwo-step\n page.\n\n\nAfter successfully entering your password, you'll be prompted for your second\nauthentication factor with a message like this:\n\n\nDuo two-factor login for \nsunetid\n\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999 (next code starts with: 9)\n\nPasscode or option (1-3):\n\n\n\n\n\n\nAvoiding two-factor prompt on each connection\n\n\nIf you routinely open multiple sessions to Sherlock, having to confirm each\none of them with a second authentication factor could rapidely become\ncumbersome. To work around this, the OpenSSH client allows multiplexing\nchannels and re-using existing authenticated for opening new sessions.\nPlease see the \nAdvanced Connection Options\n page for more\ndetails.\n\n\n\n\nIf your second factor is accepted, you'll see the following message:\n\n\nSuccess. Logging you in...\n\n\n\n\nAuthentication failures\n#\n\n\n\n\nExcessive authentication failures\n\n\nEntering an invalid password multiple times will result in a (temporary)\nban of your IP address.\n\n\n\n\nTo prevent brute-force password guessing attacks on Sherlock login nodes, we\nautomatically block IP addresses that generate too many authentication failures\nin a given time span. This results in a temporary ban of the infringing IP\naddress, and the impossibility for the user to connect to Sherlock from that\nIP address.\n\n\nWhen this happens, your SSH connection attempts will result in the following\nerror:\n\n\nssh: connect to host login.sherlock.stanford.edu port 22: Connection refused\n\n\n\n\nTo lift the ban before its automatic expiration, feel free to \ncontact\nus\n and provide your IP address.\n\n\nLogin\n#\n\n\nCongratulations! You've successfully connected to Sherlock. You'll be greeted\nby the following \nmessage of the day\n:\n\n\n\n             --*-*- Stanford Research Computing Center -*-*--\n                ____  _               _            _\n               / ___|| |__   ___ _ __| | ___   ___| | __\n               \\___ \\| '_ \\ / _ \\ '__| |/ _ \\ / __| |/ /\n                ___) | | | |  __/ |  | | (_) | (__|   \n\n               |____/|_| |_|\\___|_|  |_|\\___/ \\___|_|\\_\\ 2.0\n\n-----------------------------------------------------------------------------\n  This system is for authorized users only and you are expected to comply\n  with all Stanford computing, network and research policies.\n  For more info, see https://acomp.stanford.edu/about/policy and\n  https://doresearch.stanford.edu/policies/research-policy-handbook\n-----------------------------------------------------------------------------\n  This system is *NOT* HIPAA compliant and shouldn't be used to process PHI\n  See https://privacy.stanford.edu/faqs/hipaa-faqs for more information.\n-----------------------------------------------------------------------------\n\n  Support           email: srcc-support@stanford.edu\n  ========   office hours: Tuesdays 10-11am, Thursdays 3-4pm,\n                           room 255 @ Polya Hall\n\n  Web                 www: https://www.sherlock.stanford.edu\n  ========           news: https://news.sherlock.stanford.edu\n                   status: https://status.sherlock.stanford.edu\n\n-----------------------------------------------------------------------------\n\n\n\nOnce authenticated to Sherlock, you'll see the following prompt:\n\n\n\n\n\n\n[\nsunetid\n@sh-ln01 \nlogin!\n ~]$\n\n\n\n\nIt indicates the name of the login node you've been connected to, and a\nreminder that you're actually connected to a \nlogin node\n, not a\ncompute node.\n\n\n\n\nLogin nodes are not for computing\n\n\nLogin nodes are shared among many users and therefore must not be used to\nrun computationally intensive tasks. Those should be submitted to the\nscheduler which will dispatch them on compute nodes.\n\n\n\n\nBy contrast, the shell prompt on a login node looks like this:\n\n\n\n\n\n\n[\nsunetid\n@sh-101-01 ~]$\n\n\n\n\nStart computing\n#\n\n\nTo start computing, there's still a extra step required, which is requesting\nresources to run your application. It's all described in the \nnext\nsection\n.\n\n\n\n\n\n\n\n\n\n\nOn Sherlock 1.0, GSSAPI tokens (based on Kerberos tickets)\nwere the only allowed authentication method, which could cause some\ninteroperability with third-party SSH clients.\n\n\n\n\n\n\nFor other methods of authentication, see the \nAdvanced\n Connection Options\n page.", 
            "title": "Connecting"
        }, 
        {
            "location": "/docs/getting-started/connecting/#connecting-to-sherlock", 
            "text": "Sherlock account required  To be able to connect to Sherlock, you must first obtain a  Sherlock\naccount .", 
            "title": "Connecting to Sherlock "
        }, 
        {
            "location": "/docs/getting-started/connecting/#credentials", 
            "text": "All users must have a  Stanford SUNet ID  and a  Sherlock\naccount  to log in to Sherlock. Your Sherlock account uses the\nsame username/password as your SUnet ID:  Username: SUNet ID\nPassword: SUNet ID password  To request a Sherlock account, please see the  Prerequisites \npage.   Resetting passwords  Sherlock does  not  store your SUNet ID password. As a consequence, we\nare unable to reset your password. If you require password assistance,\nplease see the  SUNet Account page .", 
            "title": "Credentials"
        }, 
        {
            "location": "/docs/getting-started/connecting/#connection", 
            "text": "Access to Sherlock is provided via Secure Shell (SSH) login. Most Unix-like\noperating systems provide an SSH client by default that can be accessed by\ntyping the  ssh  command in a terminal window.  To login to Sherlock, open a terminal and type the following command, where sunetid  should be replaced by your  actual  SUNet ID:  $ ssh  sunetid @login.sherlock.stanford.edu  Upon logging in, you will be connected to one of Sherlock's load-balanced login\nnode. You should be automatically directed to the least-loaded login node at\nthe moment of your connection, which should give you the best possible\nenvironment to work.", 
            "title": "Connection"
        }, 
        {
            "location": "/docs/getting-started/connecting/#host-keys", 
            "text": "Upon your very first connection to Sherlock, you will be greeted by a warning\nsuch as :  The authenticity of host 'login.sherlock.stanford.edu' can't be established.\nECDSA key fingerprint is SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg.\nAre you sure you want to continue connecting (yes/no)?  This warning is normal: your SSH client warns you that it is the first time it\nsees that new computer. To make sure you are actually connecting to the right\nmachine, you should compare the ECDSA key fingerprint shown in the message with\none of the fingerprints below:     Key type  Key Fingerprint      RSA  SHA256:T1q1Tbq8k5XBD5PIxvlCfTxNMi1ORWwKNRPeZPXUfJA legacy format:  f5:8f:01:46:d1:f9:66:5d:33:58:b4:82:d8:4a:34:41    ECDSA  SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg legacy format:  70:4c:76:ea:ae:b2:0f:81:4b:9c:c6:5a:52:4c:7f:64     If they match, you can proceed and type \u2018yes\u2019. Your SSH program will then store\nthat key and will verify it for every subsequent SSH connection, to make sure\nthat the server you're connecting to is indeed Sherlock.", 
            "title": "Host keys"
        }, 
        {
            "location": "/docs/getting-started/connecting/#authentication", 
            "text": "", 
            "title": "Authentication"
        }, 
        {
            "location": "/docs/getting-started/connecting/#password", 
            "text": "To ease access and increase compatibility 1  with different\nplatforms, Sherlock allows a simple password-based authentication mechanism for\nSSH. 2 .  Upon connection, you will be asked for your SUNet ID password with the\nfollowing prompt:  sunetid @login.sherlock.stanford.edu's password:  Enter your password, and if it's correct, you should see the following line:  Authenticated with partial success.", 
            "title": "Password"
        }, 
        {
            "location": "/docs/getting-started/connecting/#second-factor-2fa", 
            "text": "Sherlock implements Stanford's  Minimum Security Standards \npolicies which mandate two-step authentication to access the cluster.  Two-step authentication protects your personal information and credentials by\ncombining something only you  know  (your password) with something only you have  (your phone, tablet or token). This prevents an attacker who would steal\nyour password to actually use it to impersonate you. For more details about\ntwo-step authentication at Stanford, please refer to the  University IT\ntwo-step  page.  After successfully entering your password, you'll be prompted for your second\nauthentication factor with a message like this:  Duo two-factor login for  sunetid \n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999 (next code starts with: 9)\n\nPasscode or option (1-3):   Avoiding two-factor prompt on each connection  If you routinely open multiple sessions to Sherlock, having to confirm each\none of them with a second authentication factor could rapidely become\ncumbersome. To work around this, the OpenSSH client allows multiplexing\nchannels and re-using existing authenticated for opening new sessions.\nPlease see the  Advanced Connection Options  page for more\ndetails.   If your second factor is accepted, you'll see the following message:  Success. Logging you in...", 
            "title": "Second factor (2FA)"
        }, 
        {
            "location": "/docs/getting-started/connecting/#authentication-failures", 
            "text": "Excessive authentication failures  Entering an invalid password multiple times will result in a (temporary)\nban of your IP address.   To prevent brute-force password guessing attacks on Sherlock login nodes, we\nautomatically block IP addresses that generate too many authentication failures\nin a given time span. This results in a temporary ban of the infringing IP\naddress, and the impossibility for the user to connect to Sherlock from that\nIP address.  When this happens, your SSH connection attempts will result in the following\nerror:  ssh: connect to host login.sherlock.stanford.edu port 22: Connection refused  To lift the ban before its automatic expiration, feel free to  contact\nus  and provide your IP address.", 
            "title": "Authentication failures"
        }, 
        {
            "location": "/docs/getting-started/connecting/#login", 
            "text": "Congratulations! You've successfully connected to Sherlock. You'll be greeted\nby the following  message of the day :  \n             --*-*- Stanford Research Computing Center -*-*--\n                ____  _               _            _\n               / ___|| |__   ___ _ __| | ___   ___| | __\n               \\___ \\| '_ \\ / _ \\ '__| |/ _ \\ / __| |/ /\n                ___) | | | |  __/ |  | | (_) | (__|    \n               |____/|_| |_|\\___|_|  |_|\\___/ \\___|_|\\_\\ 2.0\n\n-----------------------------------------------------------------------------\n  This system is for authorized users only and you are expected to comply\n  with all Stanford computing, network and research policies.\n  For more info, see https://acomp.stanford.edu/about/policy and\n  https://doresearch.stanford.edu/policies/research-policy-handbook\n-----------------------------------------------------------------------------\n  This system is *NOT* HIPAA compliant and shouldn't be used to process PHI\n  See https://privacy.stanford.edu/faqs/hipaa-faqs for more information.\n-----------------------------------------------------------------------------\n\n  Support           email: srcc-support@stanford.edu\n  ========   office hours: Tuesdays 10-11am, Thursdays 3-4pm,\n                           room 255 @ Polya Hall\n\n  Web                 www: https://www.sherlock.stanford.edu\n  ========           news: https://news.sherlock.stanford.edu\n                   status: https://status.sherlock.stanford.edu\n\n-----------------------------------------------------------------------------  Once authenticated to Sherlock, you'll see the following prompt:   \n[ sunetid @sh-ln01  login!  ~]$  It indicates the name of the login node you've been connected to, and a\nreminder that you're actually connected to a  login node , not a\ncompute node.   Login nodes are not for computing  Login nodes are shared among many users and therefore must not be used to\nrun computationally intensive tasks. Those should be submitted to the\nscheduler which will dispatch them on compute nodes.   By contrast, the shell prompt on a login node looks like this:   \n[ sunetid @sh-101-01 ~]$", 
            "title": "Login"
        }, 
        {
            "location": "/docs/getting-started/connecting/#start-computing", 
            "text": "To start computing, there's still a extra step required, which is requesting\nresources to run your application. It's all described in the  next\nsection .      On Sherlock 1.0, GSSAPI tokens (based on Kerberos tickets)\nwere the only allowed authentication method, which could cause some\ninteroperability with third-party SSH clients.    For other methods of authentication, see the  Advanced\n Connection Options  page.", 
            "title": "Start computing"
        }, 
        {
            "location": "/docs/getting-started/submitting/", 
            "text": "Principle\n#\n\n\n\n\nLogin nodes are not for computing\n\n\nLogin nodes are shared among many users and therefore must not be used to\nrun computationally intensive tasks. Those should be submitted to the\nscheduler which will dispatch them on compute nodes.\n\n\n\n\nRequesting resources\n#\n\n\nA mandatory prerequisite for running computational tasks on Sherlock is to\nrequest computing resources. This is done via a resource scheduler, whose very\npurpose is to match compute resources in the cluster (CPUs, GPUs, memory, ...)\nwith user resource requests.\n\n\nThe scheduler provides three key functions:\n\n\n\n\nit allocates access to resources (compute nodes) to users for some duration\n   of time so they can perform work.\n\n\nit provides a framework for starting, executing, and monitoring work\n   (typically a parallel job such as MPI) on a set of allocated nodes.\n\n\nit arbitrates contention for resources by managing a queue of pending jobs\n\n\n\n\nSlurm\n#\n\n\n\n\n\n\n@media only screen and (max-width: 720px) {\n    #logo_head {\n        display: none;\n    }\n}\n\n\n\n\n\n\nSherlock uses \nSlurm\n, an open-source resource manager and job\nscheduler, used by many of the world's \nsupercomputers and computer\nclusters\n.\n\n\nSlurm supports a variety of job submission techniques. By accurately requesting\nthe resources you need, you\u2019ll be able to get your work done.\n\n\n\n\nWait times in queue\n\n\nAs a quick rule of thumb, it's important to keep in mind that the more\nresources your job requests (CPUs, GPUs, memory, nodes, and time), the\nlonger it may have to wait in queue before it could start.\n\n\nIn other words:\naccurately requesting resources to match your job's needs will minimize\nyour wait times.\n\n\n\n\nHow to submit a job\n#\n\n\n\n\nA job consists in two parts: resource requests and job steps.\n\n\n\n\nResource requests\n describe the amount of computing resource (CPUs, GPUs,\nmemory, expected run time, etc.) that the job will need to successfully run.\n\n\n\n\n\n\nJob steps\n describe tasks that must be executed.\n\n\n\n\n\n\nBatch scripts\n#\n\n\nThe typical way of creating a job is to write a job submission script. A\nsubmission script is a shell script (e.g. a Bash script) whose first comments,\nif they are prefixed with \n#SBATCH\n, are interpreted by Slurm as parameters\ndescribing resource requests and submissions options\n1\n.\n\n\nThe submission script itself is a job step. Other job steps are created with\nthe \nsrun\n command.\n\n\nFor instance, the following script would request one task with one CPU for 10\nminutes, along with 2 GB of memory, in the default partition:\n\n\n#!/bin/bash\n#\n#SBATCH --job-name=test\n#\n#SBATCH --time=10:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=2G\n\nsrun hostname\nsrun sleep 60\n\n\n\nWhen started, the job would run a first job step \nsrun hostname\n, which will\nlaunch the command \nhostname\n on the node on which the requested CPU was\nallocated. Then, a second job step will start the \nsleep\n command.\n\n\nYou can create this job submission script on Sherlock using a \ntext\neditor\n such as \nnano\n or \nvim\n, and save it as \nsubmit.sh\n.\n\n\n\n\nSlurm directives must be at the top of the script\n\n\nSlurm will ignore all \n#SBATCH\n directives after the first non-comment\nline.  Always put your \n#SBATCH\n parameters at the top of your batch\nscript.\n\n\n\n\nJob submission\n#\n\n\nOnce the submission script is written properly, you can submit it to the\nscheduler with the \nsbatch\n command. Upon success, \nsbatch\n will return the ID\nit has assigned to the job (the jobid).\n\n\n$ sbatch submit.sh\nSubmitted batch job 1377\n\n\n\nCheck the job\n#\n\n\nOnce submitted, the job enters the queue in the \nPENDING\n state. When resources\nbecome available and the job has sufficient priority, an allocation is created\nfor it and it moves to the \nRUNNING\n state. If the job completes correctly, it\ngoes to the \nCOMPLETED\n state, otherwise, its state is set to \nFAILED\n.\n\n\nYou'll be able to check the status of your job and follow its evolution with\nthe \nsqueue -u $USER\n command:\n\n\n$ squeue -u $USER\n     JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n      1377    normal     test   kilian  R       0:12      1 sh-101-01\n\n\n\nThe scheduler will automatically create an output file that will contain the\nresult of the commands run in the script file. That output file is names\n\nslurm-\njobid\n.out\n by default, but can be customized via submission options.\nIn the above example, you can list the contents of that output file with the\nfollowing commands:\n\n\n$ cat slurm-1377.out\nsh-101-01\n\n\n\nCongratulations, you've submitted your first batch job on Sherlock!\n\n\nWhat's next?\n#\n\n\nActually, quite a lot. Although you now know how to submit a simple batch job,\nthere are many other options and areas to explore in the next sections:\n\n\n\n\nData transfer\n\n\nStorage\n\n\nRunning jobs\n\n\n\n\n\n\n\n\n\n\n\n\nYou can get the complete list of parameters by referring to the\n  \nsbatch\n manual page (\nman sbatch\n).", 
            "title": "Submitting jobs"
        }, 
        {
            "location": "/docs/getting-started/submitting/#principle", 
            "text": "Login nodes are not for computing  Login nodes are shared among many users and therefore must not be used to\nrun computationally intensive tasks. Those should be submitted to the\nscheduler which will dispatch them on compute nodes.", 
            "title": "Principle"
        }, 
        {
            "location": "/docs/getting-started/submitting/#requesting-resources", 
            "text": "A mandatory prerequisite for running computational tasks on Sherlock is to\nrequest computing resources. This is done via a resource scheduler, whose very\npurpose is to match compute resources in the cluster (CPUs, GPUs, memory, ...)\nwith user resource requests.  The scheduler provides three key functions:   it allocates access to resources (compute nodes) to users for some duration\n   of time so they can perform work.  it provides a framework for starting, executing, and monitoring work\n   (typically a parallel job such as MPI) on a set of allocated nodes.  it arbitrates contention for resources by managing a queue of pending jobs", 
            "title": "Requesting resources"
        }, 
        {
            "location": "/docs/getting-started/submitting/#slurm", 
            "text": "@media only screen and (max-width: 720px) {\n    #logo_head {\n        display: none;\n    }\n}   Sherlock uses  Slurm , an open-source resource manager and job\nscheduler, used by many of the world's  supercomputers and computer\nclusters .  Slurm supports a variety of job submission techniques. By accurately requesting\nthe resources you need, you\u2019ll be able to get your work done.   Wait times in queue  As a quick rule of thumb, it's important to keep in mind that the more\nresources your job requests (CPUs, GPUs, memory, nodes, and time), the\nlonger it may have to wait in queue before it could start.  In other words:\naccurately requesting resources to match your job's needs will minimize\nyour wait times.", 
            "title": "Slurm"
        }, 
        {
            "location": "/docs/getting-started/submitting/#how-to-submit-a-job", 
            "text": "A job consists in two parts: resource requests and job steps.   Resource requests  describe the amount of computing resource (CPUs, GPUs,\nmemory, expected run time, etc.) that the job will need to successfully run.    Job steps  describe tasks that must be executed.", 
            "title": "How to submit a job"
        }, 
        {
            "location": "/docs/getting-started/submitting/#batch-scripts", 
            "text": "The typical way of creating a job is to write a job submission script. A\nsubmission script is a shell script (e.g. a Bash script) whose first comments,\nif they are prefixed with  #SBATCH , are interpreted by Slurm as parameters\ndescribing resource requests and submissions options 1 .  The submission script itself is a job step. Other job steps are created with\nthe  srun  command.  For instance, the following script would request one task with one CPU for 10\nminutes, along with 2 GB of memory, in the default partition:  #!/bin/bash\n#\n#SBATCH --job-name=test\n#\n#SBATCH --time=10:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=2G\n\nsrun hostname\nsrun sleep 60  When started, the job would run a first job step  srun hostname , which will\nlaunch the command  hostname  on the node on which the requested CPU was\nallocated. Then, a second job step will start the  sleep  command.  You can create this job submission script on Sherlock using a  text\neditor  such as  nano  or  vim , and save it as  submit.sh .   Slurm directives must be at the top of the script  Slurm will ignore all  #SBATCH  directives after the first non-comment\nline.  Always put your  #SBATCH  parameters at the top of your batch\nscript.", 
            "title": "Batch scripts"
        }, 
        {
            "location": "/docs/getting-started/submitting/#job-submission", 
            "text": "Once the submission script is written properly, you can submit it to the\nscheduler with the  sbatch  command. Upon success,  sbatch  will return the ID\nit has assigned to the job (the jobid).  $ sbatch submit.sh\nSubmitted batch job 1377", 
            "title": "Job submission"
        }, 
        {
            "location": "/docs/getting-started/submitting/#check-the-job", 
            "text": "Once submitted, the job enters the queue in the  PENDING  state. When resources\nbecome available and the job has sufficient priority, an allocation is created\nfor it and it moves to the  RUNNING  state. If the job completes correctly, it\ngoes to the  COMPLETED  state, otherwise, its state is set to  FAILED .  You'll be able to check the status of your job and follow its evolution with\nthe  squeue -u $USER  command:  $ squeue -u $USER\n     JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n      1377    normal     test   kilian  R       0:12      1 sh-101-01  The scheduler will automatically create an output file that will contain the\nresult of the commands run in the script file. That output file is names slurm- jobid .out  by default, but can be customized via submission options.\nIn the above example, you can list the contents of that output file with the\nfollowing commands:  $ cat slurm-1377.out\nsh-101-01  Congratulations, you've submitted your first batch job on Sherlock!", 
            "title": "Check the job"
        }, 
        {
            "location": "/docs/getting-started/submitting/#whats-next", 
            "text": "Actually, quite a lot. Although you now know how to submit a simple batch job,\nthere are many other options and areas to explore in the next sections:   Data transfer  Storage  Running jobs       You can get the complete list of parameters by referring to the\n   sbatch  manual page ( man sbatch ).", 
            "title": "What's next?"
        }, 
        {
            "location": "/docs/user-guide/storage/", 
            "text": "Sherlock provides access to several file systems, each with distinct storage\ncharacteristics. Each user and PI group get access to a set of pre-defined\ndirectories in these file systems to store their data.\n\n\n\n\nSherlock is a compute cluster, not a storage system\n\n\nSherlock's storage resources are limited and are shared among many users.\nThey are meant to store data and code associated with projects for which\nyou are using Sherlock's computational resources. This space is for work\nactively being computed on with Sherlock, and should not be used as a\ntarget for backups from other systems.\n\n\n\n\nThose file systems are shared with other users, and are subject to quota limits\nand for some of them, purge policies (time-residency limits).\n\n\nFilesystem overview\n#\n\n\nFeatures and purpose\n#\n\n\n\n\n\n\n.yes { color: darkgreen; }\n.no  { color: darkred;   }\n\n\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nBackups/Snapshots\n\n\nPerformance\n\n\nPurpose\n\n\nCost\n\n\n\n\n\n\n\n\n\n\n$HOME\n, \n$PI_HOME\n\n\nNFS\n\n\n / \n\n\nLow\n\n\nSmall, important files (source code, executables, configuration files...)\n\n\nFree\n\n\n\n\n\n\n$SCRATCH\n, \n$PI_SCRATCH\n\n\nLustre\n\n\n / \n\n\nHigh bandwidth\n\n\nLarge, temporary files (checkpoints, raw application output...)\n\n\nFree\n\n\n\n\n\n\n$L_SCRATCH\n\n\nLocal SSD\n\n\n / \n\n\nLow latency, high IOPS\n\n\nJob specific output requiring high IOPS\n\n\nFree\n\n\n\n\n\n\n$OAK\n\n\nLustre\n\n\n / option\n\n\nModerate\n\n\nLong term storage of research data\n\n\nVolume\n1\n\n\n\n\n\n\n\n\nQuotas and limits\n#\n\n\n\n\n\n\n\n\nName\n\n\nQuota type\n\n\nQuota value\n\n\nRetention\n\n\n\n\n\n\n\n\n\n\n$HOME\n\n\nuser\n\n\n15 GB\n\n\n\\infty\n\\infty\n\n\n\n\n\n\n$PI_HOME\n\n\ngroup\n\n\n1 TB\n\n\n\\infty\n\\infty\n\n\n\n\n\n\n$SCRATCH\n\n\nuser\n\n\n20 TB\n\n\ntime limited\n\n\n\n\n\n\n$PI_SCRATCH\n\n\ngroup\n\n\n30 TB\n\n\ntime limited\n\n\n\n\n\n\n$L_SCRATCH\n\n\nn/a\n\n\nn/a\n\n\njob\n\n\n\n\n\n\n$OAK\n\n\ngroup\n\n\namount purchased\n\n\n\\infty\n\\infty\n\n\n\n\n\n\n\n\nQuota types:\n\n\n\n\nuser\n: based on files ownership and account for all the files that\n  belong to a given user.\n\n\ngroup\n: based on files ownership and account for all the files that\n  belong to a given group.\n\n\ndirectory\n: based on files location and account for all the files\n  that are in a given directory.\n\n\n\n\nRetention types:\n\n\n\n\n\\infty\n\\infty\n: files are kept as long as the user account exists on Sherlock.\n\n\ntime limited\n: files are kept for a fixed length of time after they've\n  been last modified. Once the limit is reached, files expire and are\n  automatically deleted.\n\n\njob\n: files are only kept for the duration of the job and are\n  automatically purged when the job ends.\n\n\n\n\nAccess scope\n#\n\n\n\n\n\n\n\n\nName\n\n\nScope\n\n\nAccess sharing level\n\n\n\n\n\n\n\n\n\n\n$HOME\n\n\ncluster\n\n\nuser\n\n\n\n\n\n\n$PI_HOME\n\n\ncluster\n\n\ngroup\n\n\n\n\n\n\n$SCRATCH\n\n\ncluster\n\n\nuser\n\n\n\n\n\n\n$PI_SCRATCH\n\n\ncluster\n\n\ngroup\n\n\n\n\n\n\n$L_SCRATCH\n\n\ncompute node\n\n\nuser\n\n\n\n\n\n\n$OAK\n\n\ncluster (optional, purchase required)\n\n\ngroup\n\n\n\n\n\n\n\n\nGroup storage locations are typically shared between all the members of the\nsame PI group. User locations are only accessible by the user.\n\n\nWhere should I store my files?\n#\n\n\n\n\nNot all filesystems are equivalent\n\n\nChoosing the appropriate storage location for your files is an essential\nstep towards making your utilization of the cluster the most efficient\npossible. It will make your own experience much smoother, yield better\nperformance for your jobs and simulations, and contribute to make Sherlock\na useful and well-functioning resource for everyone.\n\n\n\n\nHere is where we recommend storing different types of files and data on\nSherlock:\n\n\n\n\npersonal scripts, configuration files and software installations \n \n$HOME\n\n\ngroup-shared scripts, software installations and medium-sized datasets \n\n  \n$PI_HOME\n\n\ntemporary output of jobs, large checkpoint files \n \n$SCRATCH\n\n\ncurated output of job campaigns, large group-shared datasets, archives \n \n$OAK\n\n\n\n\nAccessing filesystems\n#\n\n\nOn Sherlock\n#\n\n\n\n\nFilesystem environment variables\n\n\nTo facilitate access and data management, user and group storage location\non Sherlock are identified by a set of environment variables, such as\n\n$HOME\n or \n$SCRATCH\n.\n\n\n\n\nWe strongly recommend using those variables in your scripts rather than\nexplicit paths, to facilitate transition to new systems for instance. By using\nthose environment variables, you'll be sure that your scripts will continue to\nwork even if the underlying filesystem paths change.\n\n\nTo see the contents of these variables, you can use the \necho\n command. For\ninstance, to see the absolute path of your $SCRATCH directory:\n\n$ echo $SCRATCH\n/scratch/users/kilian\n\n\nOr for instance, to move to your group-shared home directory:\n\n$ cd $PI_HOME\n\n\nFrom other systems\n#\n\n\n\n\nExternal filesystems cannot be mounted on Sherlock\n\n\nFor a variety of security, manageability and technical considerations, we\ncan't mount external filesystems nor data storage systems on Sherlock. The\nrecommended approach is to make Sherlock's data available on external\nsystems.\n\n\n\n\nYou can mount any of your Sherlock directories on any external system you have\naccess to by using SSHFS. For more details, please refer to the \nData\nTransfer\n page.\n\n\n\n\n\n\n\n\n\n\nFor more information about Oak, its characteristics and cost model,\n       please see the \nOak Service Description page\n.", 
            "title": "Overview"
        }, 
        {
            "location": "/docs/user-guide/storage/#filesystem-overview", 
            "text": "", 
            "title": "Filesystem overview"
        }, 
        {
            "location": "/docs/user-guide/storage/#features-and-purpose", 
            "text": ".yes { color: darkgreen; }\n.no  { color: darkred;   }     Name  Type  Backups/Snapshots  Performance  Purpose  Cost      $HOME ,  $PI_HOME  NFS   /   Low  Small, important files (source code, executables, configuration files...)  Free    $SCRATCH ,  $PI_SCRATCH  Lustre   /   High bandwidth  Large, temporary files (checkpoints, raw application output...)  Free    $L_SCRATCH  Local SSD   /   Low latency, high IOPS  Job specific output requiring high IOPS  Free    $OAK  Lustre   / option  Moderate  Long term storage of research data  Volume 1", 
            "title": "Features and purpose"
        }, 
        {
            "location": "/docs/user-guide/storage/#quotas-and-limits", 
            "text": "Name  Quota type  Quota value  Retention      $HOME  user  15 GB  \\infty \\infty    $PI_HOME  group  1 TB  \\infty \\infty    $SCRATCH  user  20 TB  time limited    $PI_SCRATCH  group  30 TB  time limited    $L_SCRATCH  n/a  n/a  job    $OAK  group  amount purchased  \\infty \\infty     Quota types:   user : based on files ownership and account for all the files that\n  belong to a given user.  group : based on files ownership and account for all the files that\n  belong to a given group.  directory : based on files location and account for all the files\n  that are in a given directory.   Retention types:   \\infty \\infty : files are kept as long as the user account exists on Sherlock.  time limited : files are kept for a fixed length of time after they've\n  been last modified. Once the limit is reached, files expire and are\n  automatically deleted.  job : files are only kept for the duration of the job and are\n  automatically purged when the job ends.", 
            "title": "Quotas and limits"
        }, 
        {
            "location": "/docs/user-guide/storage/#access-scope", 
            "text": "Name  Scope  Access sharing level      $HOME  cluster  user    $PI_HOME  cluster  group    $SCRATCH  cluster  user    $PI_SCRATCH  cluster  group    $L_SCRATCH  compute node  user    $OAK  cluster (optional, purchase required)  group     Group storage locations are typically shared between all the members of the\nsame PI group. User locations are only accessible by the user.", 
            "title": "Access scope"
        }, 
        {
            "location": "/docs/user-guide/storage/#where-should-i-store-my-files", 
            "text": "Not all filesystems are equivalent  Choosing the appropriate storage location for your files is an essential\nstep towards making your utilization of the cluster the most efficient\npossible. It will make your own experience much smoother, yield better\nperformance for your jobs and simulations, and contribute to make Sherlock\na useful and well-functioning resource for everyone.   Here is where we recommend storing different types of files and data on\nSherlock:   personal scripts, configuration files and software installations    $HOME  group-shared scripts, software installations and medium-sized datasets  \n   $PI_HOME  temporary output of jobs, large checkpoint files    $SCRATCH  curated output of job campaigns, large group-shared datasets, archives    $OAK", 
            "title": "Where should I store my files?"
        }, 
        {
            "location": "/docs/user-guide/storage/#accessing-filesystems", 
            "text": "", 
            "title": "Accessing filesystems"
        }, 
        {
            "location": "/docs/user-guide/storage/#on-sherlock", 
            "text": "Filesystem environment variables  To facilitate access and data management, user and group storage location\non Sherlock are identified by a set of environment variables, such as $HOME  or  $SCRATCH .   We strongly recommend using those variables in your scripts rather than\nexplicit paths, to facilitate transition to new systems for instance. By using\nthose environment variables, you'll be sure that your scripts will continue to\nwork even if the underlying filesystem paths change.  To see the contents of these variables, you can use the  echo  command. For\ninstance, to see the absolute path of your $SCRATCH directory: $ echo $SCRATCH\n/scratch/users/kilian  Or for instance, to move to your group-shared home directory: $ cd $PI_HOME", 
            "title": "On Sherlock"
        }, 
        {
            "location": "/docs/user-guide/storage/#from-other-systems", 
            "text": "External filesystems cannot be mounted on Sherlock  For a variety of security, manageability and technical considerations, we\ncan't mount external filesystems nor data storage systems on Sherlock. The\nrecommended approach is to make Sherlock's data available on external\nsystems.   You can mount any of your Sherlock directories on any external system you have\naccess to by using SSHFS. For more details, please refer to the  Data\nTransfer  page.      For more information about Oak, its characteristics and cost model,\n       please see the  Oak Service Description page .", 
            "title": "From other systems"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/", 
            "text": "The following sections describe the characteristics and best uses of each of\nthe Sherlock's filesystems.\n\n\n$HOME\n#\n\n\n\n\nSummary\n\n\n$HOME\n is your home directory. It's the best place to keep your code\nand important data as it provides snapshots and off-site replication. It is\nnot meant to host data that will be actively read and written to by compute\njobs.\n\n\n\n\n\n\n\n\n\n\nCharacteristics\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nIsilon high speed NFS file system\n\n\n\n\n\n\nQuota\n\n\n15 GB for the whole \n$HOME\n directory\n\n\n\n\n\n\nSnapshots\n\n\nyes \n(cf. \nSnapshots\n for more info)\n\n\n\n\n\n\nBackups\n\n\noff-site replication\n\n\n\n\n\n\nPurge policy\n\n\nnot purged\n\n\n\n\n\n\nScope\n\n\nall login and compute nodes\n\n\n\n\n\n\n\n\nRecommended usage\n#\n\n\n$HOME\n is best suited for personal configuration files, scripts, small\nreference files or datasets, source code and individual software installation\n\n\nWhen you log in, the system automatically sets the current working directory to\n\n$HOME\n: it's the location you'll end up when connecting to Sherlock. You can\nstore your source code and build your executables there.\n\n\nWe strongly recommend using \n$HOME\n to reference your home directory in\nscripts, rather than its explicit path.\n\n\nCheck quota usage\n#\n\n\nThe \ndf -h $HOME\n command could be used to check your quota usage in \n$HOME\n:\n\n\n$ df -h $HOME\nFilesystem             Size  Used Avail Use% Mounted on\nsrcf.isilon:/ifs/home   15G  3.5G   12G  24% /home/users\n\n\n\n\n\n$PI_HOME\n#\n\n\n\n\nSummary\n\n\n$PI_HOME\n is your group home directory. It's the best place to keep your\ngroup's shared code, software installations and important data as it\nprovides snapshots and off-site replication. It is not meant to host data\nthat will be actively read and written to by compute jobs.\n\n\n\n\n$HOME\n and \n$PI_HOME\n are based on the same physical file system.\n\n\n\n\n\n\n\n\nCharacteristics\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nIsilon high speed NFS file system\n\n\n\n\n\n\nQuota\n\n\n1 TB for the whole \n$PI_HOME\n directory\n\n\n\n\n\n\nSnapshots\n\n\nyes \n(cf. \nSnapshots\n for more info)\n\n\n\n\n\n\nBackups\n\n\noff-site replication\n\n\n\n\n\n\nPurge policy\n\n\nnot purged\n\n\n\n\n\n\nScope\n\n\nall login and compute nodes\n\n\n\n\n\n\n\n\nRecommended usage\n#\n\n\n$PI_HOME\n is best suited for group shared source code, common software\ninstallations, shared data sets and scripts.\n\n\nWe strongly recommend using \n$PI_HOME\n to reference your group home directory in\nscripts, rather than its explicit path.\n\n\nCheck quota usage\n#\n\n\nThe \ndf -h $PI_HOME\n command could be used to check your group quota usage in\n\n$PI_HOME\n:\n\n\n$ df -h $PI_HOME\nFilesystem           Size  Used Avail Use% Mounted on\nsrcf.isilon:/ifs/PI  1.0T  646G  379G  64% /home/groups\n\n\n\n\n\n$SCRATCH\n#\n\n\n\n\nSummary\n\n\n$SCRATCH\n is your personal scratch space. It's the best place to store\ntemporary files, such as raw job output, intermediate files, unprocessed\nresults, and so on.\n\n\n\n\n\n\n$SCRATCH\n is not a backup target\n\n\n$SCRATCH\n is not meant to store permanent data, and should only be used\nfor data associated with currently running jobs. It's not a target for\nbackups, archived data, etc.\n\n\n\n\n\n\n\n\n\n\nCharacteristics\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nParallel, high-performance Lustre file system\n\n\n\n\n\n\nQuota\n\n\n20 TB per user (based on file ownership)\n\n\n\n\n\n\nSnapshots\n\n\nNO\n\n\n\n\n\n\nBackups\n\n\nNO\n\n\n\n\n\n\nPurge policy\n\n\ndata not modified in the last 6 months are automatically purged\n\n\n\n\n\n\nScope\n\n\nall login and compute nodes\n\n\n\n\n\n\n\n\nRecommended usage\n#\n\n\n$SCRATCH\n is best suited for large files, such as raw job output, intermediate\njob files, unprocessed simlulation results, and so on.  This is the recommended\nlocation to run jobs from, and to store files that will be read or written to\nduring job execution.\n\n\nOld files are automatically purged on \n$SCRATCH\n so users should avoid storing\nlong-term data there.\n\n\nEach compute node has a 56Gb/s low latency Infiniband link to \n$SCRATCH\n. The\naggregate bandwidth of the filesystem is about 50GB/s. So any job with high\ndata performance requirements will take advantage from using \n$SCRATCH\n for\nI/O.\n\n\nWe strongly recommend using \n$SCRATCH\n to reference your scratch directory in\nscripts, rather than its explicit path.\n\n\nCheck quota usage\n#\n\n\nThe \nlfs quota -u $USER -h $SCRATCH\n command could be used to check your quota\nusage in \n$SCRATCH\n:\n\n\n$ lfs quota -u $USER -h $SCRATCH\nDisk quotas for user kilian (uid 215845):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n/scratch/users/kilian\n                 749.4G     18T     20T       - 2028766  18000000 20000000       -\n\n\n\nNB\n: user quotas are based on file ownership, meaning that all files\nbelonging to a given user will count towards her user quota, no matter\nwhere they're located on the file system. That means that if you have files\nin \n$PI_SCRATCH\n, those will also count toward your user quota.\n\n\nThe different values displayed in \nlfs quota\n are as follows:\n\n\n\n\nused\n: actual usage in volume (bytes) or number of files,\n\n\nquota\n, or \n\"soft quota\"\n: this limit can be exceeded for the duration\n  of the grace period (7 days). When the grace period expired, the soft\n  quota is considered a hard limit and can't be exceeded anymore.\n\n\nlimit\n, or \n\"hard quota\"\n: this value represents the maximum amount of\n  resources (volume or number of files) that a user is allowed to use. That\n  limit can never be exceeded.\n\n\n\n\nExpiration policy\n#\n\n\n\n\nWarning\n\n\nFiles not modified in over 6 months will be automatically deleted\n\n\n\n\nTo manage available space and maintain optimal performance for all jobs, all\nfiles on \n$SCRATCH\n are subject to automatic purges. Files whose contents have\nnot been modified in the previous 6 months will be automatically deleted.\n\n\nFor instance, if you create a file on February 1\nst\n and don't ever modify it\nafterwards, it will be automatically deleted on August 1\nst\n.\n\n\nPlease note that reading a file does not qualify as a modification.\n\n\n\n\n$PI_SCRATCH\n#\n\n\n$SCRATCH\n and \n$PI_SCRATCH\n are based on the same physical file system.\n\n\n\n\nSummary\n\n\n$PI_SCRATCH\n is your group shared scratch space. It's the best place to\nstore temporary files, such as raw job output, intermediate files, or\nunprocessed results that need to be shared among users within a group.\n\n\n\n\n\n\n$PI_SCRATCH\n is \nNOT\n a backup target\n\n\n$PI_SCRATCH\n is not meant to store permanent data, and should only be used\nfor data associated with currently running jobs. It's not a target for\nbackups, archived data, etc.\n\n\n\n\n\n\n\n\n\n\nCharacteristics\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nParallel, high-performance Lustre file system\n\n\n\n\n\n\nQuota\n\n\n30 TB per group (based on file ownership)\n\n\n\n\n\n\nSnapshots\n\n\nNO\n\n\n\n\n\n\nBackups\n\n\nNO\n\n\n\n\n\n\nPurge policy\n\n\ndata not accessed in the last 6 months are automatically purged\n\n\n\n\n\n\nScope\n\n\nall login and compute nodes\n\n\n\n\n\n\n\n\nRecommended usage\n#\n\n\n$PI_SCRATCH\n is best suited for large files, such as raw job output,\nintermediate job files, unprocessed simlulation results, and so on.  This is\nthe recommended location to run jobs from, and to store files that will be read\nor written to during job execution.\n\n\nOld files are automatically purged on \n$PI_SCRATCH\n so users should avoid\nstoring long-term data there.\n\n\nWe strongly recommend using \n$PI_SCRATCH\n to reference your group scratch\ndirectory in scripts, rather than its explicit path.\n\n\nCheck quota usage\n#\n\n\nThe \nlfs quota -g $(id -gn $USER) -h $SCRATCH\n command could be used to check\nyour group  quota usage in \n$SCRATCH\n:\n\n\n$ lfs quota -g $(id -gn $USER) -h $SCRATCH\nDisk quotas for group ruthm (gid 32264):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n/scratch/PI/ruthm\n                 13.87T     28T     30T       - 4171746  28000000 30000000       -\n\n\n\nNB\n: group quotas are based on file ownership, meaning that all files\nbelonging to a given group will count towards the group quota, no matter where\nthey're located on the file system. That means that all the files that belong\nto each of the group members will count toward the group quota.\n\n\nSee the \n$SCRATCH\n section for more details about the meaning of\nthe different fields in \nlfs quota\n.\n\n\nExpiration policy\n#\n\n\nAs \n$SCRATCH\n and \n$PI_SCRATCH\n are on the same fielsystem, the same expiration\npolicy applies to both. Please see the \n$SCRATCH\n section above for\nmore details.\n\n\n\n\n$L_SCRATCH\n#\n\n\n\n\nSummary\n\n\n$L_SCRATCH\n is \nlocal\n to each compute node, and could be used to store\ntemporary files for jobs with high IOPS requirements. Files stored in\n\n$L_SCRATCH\n are purged at the end of the job.\n\n\n\n\n\n\n\n\n\n\nCharacteristics\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nLocal filesystem, specific to each node, based on SSD\n\n\n\n\n\n\nQuota\n\n\nn/a (usable space limited by the size of the physical storage devices, typically around 150 GB)\n\n\n\n\n\n\nSnapshots\n\n\nNO\n\n\n\n\n\n\nBackups\n\n\nNO\n\n\n\n\n\n\nPurge policy\n\n\ndata immediately purged at the end of the job\n\n\n\n\n\n\nScope\n\n\nlocally on each node, not shared across nodes\n\n\n\n\n\n\n\n\nRecommended usage\n#\n\n\n$L_SCRATCH\n is best suited for small temporary files and applications which\nrequire low latency and high IOPS levels, typically intermediate job files,\ncheckpoints, dumps of temporary states, etc.\n\n\nFiles stored in \n$L_SCRATCH\n are local to each node and can't be accessed from\nother nodes, nor from login nodes.\n\n\nPlease note that an additional, job-specific environment variable,\n\n$L_SCRATCH_JOB\n, will be set to a subdirectory of \n$L_SCRATCH\n for each job.\nSo, if you have two jobs running on the same compute node, \n$L_SCRATCH\n will be\nthe same and accessible from both jobs, while \n$L_SCRATCH_JOB\n will be\ndifferent for each job.\n\n\nFor instance, if you have jobs \n98423\n and \n98672\n running on ths same\nnodes, the variables will be set as follows:\n\n\n\n\n\n\n\n\nJob id\n\n\n$L_SCRATCH\n\n\nL_SCRATCH_JOB\n\n\n\n\n\n\n\n\n\n\n98423\n\n\n/lscratch/kilian\n\n\n/lscratch/kilian/98423\n\n\n\n\n\n\n98672\n\n\n/lscratch/kilian\n\n\n/lscratch/kilian/98672\n\n\n\n\n\n\n\n\nWe strongly recommend using \n$L_SCRATCH\n to reference your local scratch\ndirectory in scripts, rather than its full path.\n\n\nExpiration policy\n#\n\n\nAll files stored in \n$L_SCRATCH\n are automatically purged ath the end of the\njob, whether the job was successful or not. If you need to conserve files that\nwere generated in \n$L_SCRATCH\n after the job ends, don't forget to add a\ncommand to copy them to one of the more peristent storage locations, such as\n\n$HOME\n or \n$SCRATCH\n.\n\n\n\n\n$OAK\n#\n\n\n\n\nSummary\n\n\n$OAK\n is the latest \ncheap'n'deep\n storage offering of the SRCC. It\nprovides a long-term, affordable storage option for research data, and is\nideally suited to host large datasets, or curated, post-processed results\nfrom job campaigns, as well as final results used for publication.\n\n\n\n\n$OAK\n is available as an option on Sherlock. For complete details and\ncharacteristics, including pricing, please refer to the \nOak Storage Service\npage\n.\n\n\n\n\n\n\n\n\nCharacteristics\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nParallel, capacitive Lustre filesystem\n\n\n\n\n\n\nQuota\n\n\namount purchased \n(in 10 TB increments)\n\n\n\n\n\n\nSnapshots\n\n\noptional\n1\n\n\n\n\n\n\nBackups\n\n\nNO\n\n\n\n\n\n\nPurge policy\n\n\nnot purged\n\n\n\n\n\n\nScope\n\n\nall login and compute nodes \n(also available through gateways outside of Sherlock)\n\n\n\n\n\n\n\n\nRecommended usage\n#\n\n\n$OAK\n is ideally suited for large shared datasets, archival data and curated,\npost-processed results   from job campaigns, as well as final results used for\npublication.\n\n\nAlthough jobs can directly read and write to \n$OAK\n during execution, it is\nrecommended to first stage files from \n$OAK\n to \n$SCRATCH\n at the beginning of\na series of jobs, and save the desired results back from \n$SCRATCH\n to \n$OAK\n\nat the end of the job campaign.\n\n\nWe strongly recommend using \n$OAK\n to reference your group home directory in\nscripts, rather than its explicit path.\n\n\nCheck quota usage\n#\n\n\nThe \nlfs quota -g $(id -gn $USER) -h $OAK\n command could be used to check\nyour group  quota usage in \n$OAK\n:\n\n\n$ lfs quota -h -g ruthm $OAK\nDisk quotas for group ruthm (gid 32264):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n/oak/stanford/groups/ruthm\n                  9.16T  18.63T  18.63T       -  384864  1500000 1500000       -\n\n\n\nSee the \n$SCRATCH\n section for more details about the meaning\nof the different fields in \nlfs quota\n.\n\n\n\n\n\n\n\n\n\n\nSnapshots on \n$OAK\n require additional storage space. Please see\n       the \nOak Snapshots Feature\n page for details.", 
            "title": "Filesystems"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#home", 
            "text": "Summary  $HOME  is your home directory. It's the best place to keep your code\nand important data as it provides snapshots and off-site replication. It is\nnot meant to host data that will be actively read and written to by compute\njobs.      Characteristics       Type  Isilon high speed NFS file system    Quota  15 GB for the whole  $HOME  directory    Snapshots  yes  (cf.  Snapshots  for more info)    Backups  off-site replication    Purge policy  not purged    Scope  all login and compute nodes", 
            "title": "$HOME"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#recommended-usage", 
            "text": "$HOME  is best suited for personal configuration files, scripts, small\nreference files or datasets, source code and individual software installation  When you log in, the system automatically sets the current working directory to $HOME : it's the location you'll end up when connecting to Sherlock. You can\nstore your source code and build your executables there.  We strongly recommend using  $HOME  to reference your home directory in\nscripts, rather than its explicit path.", 
            "title": "Recommended usage"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#check-quota-usage", 
            "text": "The  df -h $HOME  command could be used to check your quota usage in  $HOME :  $ df -h $HOME\nFilesystem             Size  Used Avail Use% Mounted on\nsrcf.isilon:/ifs/home   15G  3.5G   12G  24% /home/users", 
            "title": "Check quota usage"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#pi_home", 
            "text": "Summary  $PI_HOME  is your group home directory. It's the best place to keep your\ngroup's shared code, software installations and important data as it\nprovides snapshots and off-site replication. It is not meant to host data\nthat will be actively read and written to by compute jobs.   $HOME  and  $PI_HOME  are based on the same physical file system.     Characteristics       Type  Isilon high speed NFS file system    Quota  1 TB for the whole  $PI_HOME  directory    Snapshots  yes  (cf.  Snapshots  for more info)    Backups  off-site replication    Purge policy  not purged    Scope  all login and compute nodes", 
            "title": "$PI_HOME"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#recommended-usage_1", 
            "text": "$PI_HOME  is best suited for group shared source code, common software\ninstallations, shared data sets and scripts.  We strongly recommend using  $PI_HOME  to reference your group home directory in\nscripts, rather than its explicit path.", 
            "title": "Recommended usage"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#check-quota-usage_1", 
            "text": "The  df -h $PI_HOME  command could be used to check your group quota usage in $PI_HOME :  $ df -h $PI_HOME\nFilesystem           Size  Used Avail Use% Mounted on\nsrcf.isilon:/ifs/PI  1.0T  646G  379G  64% /home/groups", 
            "title": "Check quota usage"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#scratch", 
            "text": "Summary  $SCRATCH  is your personal scratch space. It's the best place to store\ntemporary files, such as raw job output, intermediate files, unprocessed\nresults, and so on.    $SCRATCH  is not a backup target  $SCRATCH  is not meant to store permanent data, and should only be used\nfor data associated with currently running jobs. It's not a target for\nbackups, archived data, etc.      Characteristics       Type  Parallel, high-performance Lustre file system    Quota  20 TB per user (based on file ownership)    Snapshots  NO    Backups  NO    Purge policy  data not modified in the last 6 months are automatically purged    Scope  all login and compute nodes", 
            "title": "$SCRATCH"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#recommended-usage_2", 
            "text": "$SCRATCH  is best suited for large files, such as raw job output, intermediate\njob files, unprocessed simlulation results, and so on.  This is the recommended\nlocation to run jobs from, and to store files that will be read or written to\nduring job execution.  Old files are automatically purged on  $SCRATCH  so users should avoid storing\nlong-term data there.  Each compute node has a 56Gb/s low latency Infiniband link to  $SCRATCH . The\naggregate bandwidth of the filesystem is about 50GB/s. So any job with high\ndata performance requirements will take advantage from using  $SCRATCH  for\nI/O.  We strongly recommend using  $SCRATCH  to reference your scratch directory in\nscripts, rather than its explicit path.", 
            "title": "Recommended usage"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#check-quota-usage_2", 
            "text": "The  lfs quota -u $USER -h $SCRATCH  command could be used to check your quota\nusage in  $SCRATCH :  $ lfs quota -u $USER -h $SCRATCH\nDisk quotas for user kilian (uid 215845):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n/scratch/users/kilian\n                 749.4G     18T     20T       - 2028766  18000000 20000000       -  NB : user quotas are based on file ownership, meaning that all files\nbelonging to a given user will count towards her user quota, no matter\nwhere they're located on the file system. That means that if you have files\nin  $PI_SCRATCH , those will also count toward your user quota.  The different values displayed in  lfs quota  are as follows:   used : actual usage in volume (bytes) or number of files,  quota , or  \"soft quota\" : this limit can be exceeded for the duration\n  of the grace period (7 days). When the grace period expired, the soft\n  quota is considered a hard limit and can't be exceeded anymore.  limit , or  \"hard quota\" : this value represents the maximum amount of\n  resources (volume or number of files) that a user is allowed to use. That\n  limit can never be exceeded.", 
            "title": "Check quota usage"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#expiration-policy", 
            "text": "Warning  Files not modified in over 6 months will be automatically deleted   To manage available space and maintain optimal performance for all jobs, all\nfiles on  $SCRATCH  are subject to automatic purges. Files whose contents have\nnot been modified in the previous 6 months will be automatically deleted.  For instance, if you create a file on February 1 st  and don't ever modify it\nafterwards, it will be automatically deleted on August 1 st .  Please note that reading a file does not qualify as a modification.", 
            "title": "Expiration policy"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#pi_scratch", 
            "text": "$SCRATCH  and  $PI_SCRATCH  are based on the same physical file system.   Summary  $PI_SCRATCH  is your group shared scratch space. It's the best place to\nstore temporary files, such as raw job output, intermediate files, or\nunprocessed results that need to be shared among users within a group.    $PI_SCRATCH  is  NOT  a backup target  $PI_SCRATCH  is not meant to store permanent data, and should only be used\nfor data associated with currently running jobs. It's not a target for\nbackups, archived data, etc.      Characteristics       Type  Parallel, high-performance Lustre file system    Quota  30 TB per group (based on file ownership)    Snapshots  NO    Backups  NO    Purge policy  data not accessed in the last 6 months are automatically purged    Scope  all login and compute nodes", 
            "title": "$PI_SCRATCH"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#recommended-usage_3", 
            "text": "$PI_SCRATCH  is best suited for large files, such as raw job output,\nintermediate job files, unprocessed simlulation results, and so on.  This is\nthe recommended location to run jobs from, and to store files that will be read\nor written to during job execution.  Old files are automatically purged on  $PI_SCRATCH  so users should avoid\nstoring long-term data there.  We strongly recommend using  $PI_SCRATCH  to reference your group scratch\ndirectory in scripts, rather than its explicit path.", 
            "title": "Recommended usage"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#check-quota-usage_3", 
            "text": "The  lfs quota -g $(id -gn $USER) -h $SCRATCH  command could be used to check\nyour group  quota usage in  $SCRATCH :  $ lfs quota -g $(id -gn $USER) -h $SCRATCH\nDisk quotas for group ruthm (gid 32264):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n/scratch/PI/ruthm\n                 13.87T     28T     30T       - 4171746  28000000 30000000       -  NB : group quotas are based on file ownership, meaning that all files\nbelonging to a given group will count towards the group quota, no matter where\nthey're located on the file system. That means that all the files that belong\nto each of the group members will count toward the group quota.  See the  $SCRATCH  section for more details about the meaning of\nthe different fields in  lfs quota .", 
            "title": "Check quota usage"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#expiration-policy_1", 
            "text": "As  $SCRATCH  and  $PI_SCRATCH  are on the same fielsystem, the same expiration\npolicy applies to both. Please see the  $SCRATCH  section above for\nmore details.", 
            "title": "Expiration policy"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#l_scratch", 
            "text": "Summary  $L_SCRATCH  is  local  to each compute node, and could be used to store\ntemporary files for jobs with high IOPS requirements. Files stored in $L_SCRATCH  are purged at the end of the job.      Characteristics       Type  Local filesystem, specific to each node, based on SSD    Quota  n/a (usable space limited by the size of the physical storage devices, typically around 150 GB)    Snapshots  NO    Backups  NO    Purge policy  data immediately purged at the end of the job    Scope  locally on each node, not shared across nodes", 
            "title": "$L_SCRATCH"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#recommended-usage_4", 
            "text": "$L_SCRATCH  is best suited for small temporary files and applications which\nrequire low latency and high IOPS levels, typically intermediate job files,\ncheckpoints, dumps of temporary states, etc.  Files stored in  $L_SCRATCH  are local to each node and can't be accessed from\nother nodes, nor from login nodes.  Please note that an additional, job-specific environment variable, $L_SCRATCH_JOB , will be set to a subdirectory of  $L_SCRATCH  for each job.\nSo, if you have two jobs running on the same compute node,  $L_SCRATCH  will be\nthe same and accessible from both jobs, while  $L_SCRATCH_JOB  will be\ndifferent for each job.  For instance, if you have jobs  98423  and  98672  running on ths same\nnodes, the variables will be set as follows:     Job id  $L_SCRATCH  L_SCRATCH_JOB      98423  /lscratch/kilian  /lscratch/kilian/98423    98672  /lscratch/kilian  /lscratch/kilian/98672     We strongly recommend using  $L_SCRATCH  to reference your local scratch\ndirectory in scripts, rather than its full path.", 
            "title": "Recommended usage"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#expiration-policy_2", 
            "text": "All files stored in  $L_SCRATCH  are automatically purged ath the end of the\njob, whether the job was successful or not. If you need to conserve files that\nwere generated in  $L_SCRATCH  after the job ends, don't forget to add a\ncommand to copy them to one of the more peristent storage locations, such as $HOME  or  $SCRATCH .", 
            "title": "Expiration policy"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#oak", 
            "text": "Summary  $OAK  is the latest  cheap'n'deep  storage offering of the SRCC. It\nprovides a long-term, affordable storage option for research data, and is\nideally suited to host large datasets, or curated, post-processed results\nfrom job campaigns, as well as final results used for publication.   $OAK  is available as an option on Sherlock. For complete details and\ncharacteristics, including pricing, please refer to the  Oak Storage Service\npage .     Characteristics       Type  Parallel, capacitive Lustre filesystem    Quota  amount purchased  (in 10 TB increments)    Snapshots  optional 1    Backups  NO    Purge policy  not purged    Scope  all login and compute nodes  (also available through gateways outside of Sherlock)", 
            "title": "$OAK"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#recommended-usage_5", 
            "text": "$OAK  is ideally suited for large shared datasets, archival data and curated,\npost-processed results   from job campaigns, as well as final results used for\npublication.  Although jobs can directly read and write to  $OAK  during execution, it is\nrecommended to first stage files from  $OAK  to  $SCRATCH  at the beginning of\na series of jobs, and save the desired results back from  $SCRATCH  to  $OAK \nat the end of the job campaign.  We strongly recommend using  $OAK  to reference your group home directory in\nscripts, rather than its explicit path.", 
            "title": "Recommended usage"
        }, 
        {
            "location": "/docs/user-guide/storage/filesystems/#check-quota-usage_4", 
            "text": "The  lfs quota -g $(id -gn $USER) -h $OAK  command could be used to check\nyour group  quota usage in  $OAK :  $ lfs quota -h -g ruthm $OAK\nDisk quotas for group ruthm (gid 32264):\n     Filesystem    used   quota   limit   grace   files   quota   limit   grace\n/oak/stanford/groups/ruthm\n                  9.16T  18.63T  18.63T       -  384864  1500000 1500000       -  See the  $SCRATCH  section for more details about the meaning\nof the different fields in  lfs quota .      Snapshots on  $OAK  require additional storage space. Please see\n       the  Oak Snapshots Feature  page for details.", 
            "title": "Check quota usage"
        }, 
        {
            "location": "/docs/user-guide/storage/data-sharing/", 
            "text": "The following sections present and detail options to share data across users\nand groups on Sherlock.\n\n\nSharing data locally on Sherlock\n#\n\n\nTraditional Unix permissions\n#\n\n\nStandard \nUnix file permissions\n are supported on Sherlock and\nprovide \nread\n, \nwrite\n and \nexecute\n permissions for the three distinct\naccess classes.\n\n\nThe access classes are defined as follows:\n\n\n\n\nFiles and directories are owned by a user. The owner determines the file's\n  \nuser class\n. Distinct permissions apply to the owner.\n\n\nFiles and directories are assigned a group, which define the file's \ngroup\n  class\n. Distinct permissions apply to members of the file's group. The owner\n  may be a member of the file's group.\n\n\nUsers who are not the owner, nor a member of the group, comprise a file's\n  \nothers class\n. Distinct permissions apply to others.\n\n\n\n\nThe following permissions apply to each class:\n\n\n\n\nThe \nread\n permission grants the ability to read a file. When set for a\n  directory, this permission grants the ability to read the names of files in\n  the directory, but not to find out any further information about them such as\n  contents, file type, size, ownership, permissions.\n\n\nThe \nwrite\n permission grants the ability to modify a file. When set for a\n  directory, this permission grants the ability to modify entries in the\n  directory. This includes creating files, deleting files, and renaming files.\n\n\nThe \nexecute\n permission grants the ability to execute a file. This\n  permission must be set for executable programs, including shell scripts, in\n  order to allow the operating system to run them. When set for a directory,\n  this permission grants the ability to access file contents and\n  meta-information if its name is known, but not list files inside the\n  directory, unless read is set also.\n\n\n\n\n\n\nShared directories traversal\n\n\nIf you need to give access to one of your files to another user, they will\nat least need \nexecute\n permission on each directory within the path to\nthat file.\n\n\n\n\nThe effective permissions are determined based on the first class the user\nfalls within in the order of \nuser\n, \ngroup\n then \nothers\n. For example, the\nuser who is the owner of the file will have the permissions given to the user\nclass regardless of the permissions assigned to the group class or others\nclass.\n\n\nWhile traditional Unix permissions are sufficient in most cases to share files\nwith all the users within the same group, they are not enough to share files\nwith a specific subset of users, or with users from other groups. Access\nControl Lists (ACLs) can be used for that purpose.\n\n\nThere are two type of ACLs supported on Sherlock depending on the underlying\nfilesystem:\n\n\n\n\n\n\n\n\nType\n\n\nFilesystems\n\n\n\n\n\n\n\n\n\n\nNFSv4 ACLs\n\n\n$HOME\n and \n$PI_HOME\n\n\n\n\n\n\nPOSIX ACLs\n\n\n$SCRATCH\n, \n$PI_SCRATCH\n, \n$L_SCRATCH\n and \n$OAK\n\n\n\n\n\n\n\n\nPOSIX ACLs\n#\n\n\nPOSIX ACLs allows you to grant or deny access to files and directories for\ndifferent users (or groups), independently of the file owner or group.\n\n\nTwo types of POSIX ACLs can be defined:\n\n\n\n\nAccess ACLs\n: grant permission for a specific file or directory.\n\n\nDefault ACLs\n: allow to set a default set of ACLs that will be applied to\n  any file or directory without any already defined ACL. Can only be set on\n  directories.\n\n\n\n\nACLs are set with the \nsetfacl\n command, and displayed with \ngetfacl\n. For more\ndetails and examples, please refer to \nthis documentation\n.\n\n\nIn the example below, we allow two users to access a restricted directory\nlocated at \n$PI_SCRATCH/restricted-dir/\n:\n\n\n$ cd $PI_SCRATCH\n\n### Create new directory\n$ mkdir restricted-dir\n\n### Remove 'group' and 'other' access\n$ chmod g-rwx,o-rwx restricted-dir\n\n### Give user bob read and traversal permissions to the directory\n$ setfacl -m u:bob:rX restricted-dir\n\n### Use default ACLs (-d) to give user bob read access to all new\n### files and sub-directories that will be created in \nrestricted-dir\n\n$ setfacl -d -m u:bob:rX restricted-dir\n\n### Give user alice read, write and traversal permissions for the directory\n$ setfacl -m u:alice:rwX restricted-dir\n\n### Use default ACLs (-d) to give user alice read and write access to all\n### new files and sub-directories\n$ setfacl -d -m u:alice:rwX restricted-dir\n\n### Show ACLs\n$ getfacl restricted-dir\n# file: restricted-dir/\n# owner: joe\n# group: grp\n# flags: -s-\nuser::rwx\nuser:bob:r-x\ngroup::---\nmask::r-x\nother::---\ndefault:user::rwx\ndefault:user:alice:rwx\ndefault:user:bob:r-x\ndefault:group::---\ndefault:mask::rwx\ndefault:other::---\n\n\n\n\n\nDefault permissions on \n$PI_SCRATCH\n don't allow sharing outside of your group\n\n\nBy default, the Unix permissions on the root directory \n$PI_SCRATCH\n don't\nallow read nor traversal access for \nothers\n (\nie.\n any user no part of\nyour PI group). If you need to share files with users outside of your own\ngroup, please \ncontact us\n so we can set the appropriate\npermissions on your folder.\n\n\n\n\nFor \n$SCRATCH\n, you're the owner of the directory and so you can change the\npermissions yourself.\n\n\nNFSv4 ACLs\n#\n\n\n$HOME\n and \n$PI_HOME\n also allow setting ACLs, albeit with different\nsyntax and semantics than POSIX ACLs. The principle is very similar,\nthough.\n\n\nAn ACL in NFSv4 is a list of rules setting permissions on files or directories.\nA permission rule, or Access Control Entry (ACE), is of the form\n\ntype:flags:principle:permissions\n.\n\n\nCommonly used entries for these fields are:\n\n\n\n\ntype\n: \nA\n (allow) or \nD\n (deny)\n\n\nflags\n: \ng\n (group), \nd\n (directory-inherit), \nf\n (file-inherit), \nn\n\n  (no-propagate-inherit), or \ni\n (inherit-only)\n\n\nprinciple\n:  a named user (\nuser@sherlock\n), a group, or one of three\n  special principles: \nOWNER@\n, \nGROUP@\n, and \nEVERYONE@\n.\n\n\npermissions\n: there are 14 permission characters, as well as the shortcuts\n  \nR\n, \nW\n, and \nX\n. Here is a list of possible permissions that can be\n  included in the permissions field (options are Case Sensitive)\n\n\nr\n read-data (files) / list-directory (directories)\n\n\nw\n write-data (files) / create-file (directories)\n\n\nx\n execute (files) / change-directory (directories)\n\n\na\n append-data (files) / create-subdirectory (directories)\n\n\nt\n read-attributes: read the attributes of the file/directory.\n\n\nT\n write-attributes: write the attributes of the file/directory.\n\n\nn\n read-named-attributes: read the named attributes of the\n  file/directory.\n\n\nN\n write-named-attributes: write the named attributes of the\n  file/directory.\n\n\nc\n read-ACL: read the file/directory NFSv4 ACL.\n\n\nC\n write-ACL: write the file/directory NFSv4 ACL.\n\n\no\n write-owner: change ownership of the file/directory.\n\n\ny\n synchronize: allow clients to use synchronous I/O with the server.\n\n\nd\n delete: delete the file/directory. Some servers will allow a delete\n  to occur if either this permission is set in the file/directory or if the\n  delete-child permission is set in its parent direcory.\n\n\nD\n delete-child: remove a file or subdirectory from within the given\n  directory (directories only)\n\n\n\n\n\n\n\n\n\n\nA comprehensive listing of allowable field strings is given in the manual page\n\nnfs4_acl(5)\n\n\nTo see what permissions are set on a particular file, use the \nnfs4_getfacl\n\ncommand. For example, newly created \nfile1\n may have default permissions listed\nby \nls -l\n as \n-rw-r\u2014r\u2014\n. Listing the permissions with \nnfs4_getfacl\n would\ndisplay the following:\n\n\n$ nfs4_getfacl file1\nA::OWNER@:rwatTnNcCoy\nA:g:GROUP@:rtncy\nA::EVERYONE@:rtncy\n\n\n\nTo set permissions on a file, use the \nnfs4_setfacl\n command. For convenience,\nNFSv4 provides the shortcuts \nR\n, \nW\n and \nX\n for setting read, write, and\nexecute permissions. For example, to add write permissions for the current\ngroup on \nfile1\n, use \nnfs4_setfacl\n with the \n-a\n switch:\n\n\n$ nfs4_setfacl -a A::GROUP@:W file1\n\n\n\nThis command switched the \nGROUP@\n permission field from \nrtncy\n to\n\nrwatTnNcCoy\n.  However, be aware that NFSv4 file permission shortcuts have a\ndifferent meanings than the traditional Unix \nr\n, \nw\n, and \nx\n. For example\nissuing \nchmod g+w file1\n will set \nGROUP@\n to \nrwatncy\n.\n\n\nAlthough the shortcut permissions can be handy, often rules need to be more\ncustomized. Use \nnfs4_setfacl -e file1\n  to open the ACL for \nfile1\n in a text\neditor.\n\n\nAccess Control Entries allow more fine grained control over file and directory\npermissions than does the \nchmod\n command. For example, if user \njoe\n wants to\ngive read and write permissions to \njack\n for her directory \nprivate\n, she\nwould issue:\n\n\n$ nfs4_setfacl -R -a A::jack@sherlock:RW private/\n\n\n\nThe \n-R\n switch recursively applies the rule to the files and directories\nwithin \nprivate/\n as well.\n\n\nTo allow \njack\n to create files and subdirectories within \nprivate/\n with the\npermissions as granted above, inheritance rules need to be applied.\n\n\n$ nfs4_setfacl -R -a A:fdi:jack@sherlock:RW private/\n\n\n\nBy default, each permission is in the Deny state and an ACE is required to\nexplicitly allow a permission. However, be aware that a server may silently\noverride a users ACE, usually to a less permissive setting.\n\n\nFor complete documentation and examples on using NFSv4 ACLs, please see the\nmanual page at \nnfs4_acl(5)\n.\n\n\nSharing data outside of Sherlock\n#\n\n\n\n\n Work in progress \n\n\nThis page is a work in progress and is not complete yet. We are actively\nworking on adding more content and information.", 
            "title": "Data sharing"
        }, 
        {
            "location": "/docs/user-guide/storage/data-sharing/#sharing-data-locally-on-sherlock", 
            "text": "", 
            "title": "Sharing data locally on Sherlock"
        }, 
        {
            "location": "/docs/user-guide/storage/data-sharing/#traditional-unix-permissions", 
            "text": "Standard  Unix file permissions  are supported on Sherlock and\nprovide  read ,  write  and  execute  permissions for the three distinct\naccess classes.  The access classes are defined as follows:   Files and directories are owned by a user. The owner determines the file's\n   user class . Distinct permissions apply to the owner.  Files and directories are assigned a group, which define the file's  group\n  class . Distinct permissions apply to members of the file's group. The owner\n  may be a member of the file's group.  Users who are not the owner, nor a member of the group, comprise a file's\n   others class . Distinct permissions apply to others.   The following permissions apply to each class:   The  read  permission grants the ability to read a file. When set for a\n  directory, this permission grants the ability to read the names of files in\n  the directory, but not to find out any further information about them such as\n  contents, file type, size, ownership, permissions.  The  write  permission grants the ability to modify a file. When set for a\n  directory, this permission grants the ability to modify entries in the\n  directory. This includes creating files, deleting files, and renaming files.  The  execute  permission grants the ability to execute a file. This\n  permission must be set for executable programs, including shell scripts, in\n  order to allow the operating system to run them. When set for a directory,\n  this permission grants the ability to access file contents and\n  meta-information if its name is known, but not list files inside the\n  directory, unless read is set also.    Shared directories traversal  If you need to give access to one of your files to another user, they will\nat least need  execute  permission on each directory within the path to\nthat file.   The effective permissions are determined based on the first class the user\nfalls within in the order of  user ,  group  then  others . For example, the\nuser who is the owner of the file will have the permissions given to the user\nclass regardless of the permissions assigned to the group class or others\nclass.  While traditional Unix permissions are sufficient in most cases to share files\nwith all the users within the same group, they are not enough to share files\nwith a specific subset of users, or with users from other groups. Access\nControl Lists (ACLs) can be used for that purpose.  There are two type of ACLs supported on Sherlock depending on the underlying\nfilesystem:     Type  Filesystems      NFSv4 ACLs  $HOME  and  $PI_HOME    POSIX ACLs  $SCRATCH ,  $PI_SCRATCH ,  $L_SCRATCH  and  $OAK", 
            "title": "Traditional Unix permissions"
        }, 
        {
            "location": "/docs/user-guide/storage/data-sharing/#posix-acls", 
            "text": "POSIX ACLs allows you to grant or deny access to files and directories for\ndifferent users (or groups), independently of the file owner or group.  Two types of POSIX ACLs can be defined:   Access ACLs : grant permission for a specific file or directory.  Default ACLs : allow to set a default set of ACLs that will be applied to\n  any file or directory without any already defined ACL. Can only be set on\n  directories.   ACLs are set with the  setfacl  command, and displayed with  getfacl . For more\ndetails and examples, please refer to  this documentation .  In the example below, we allow two users to access a restricted directory\nlocated at  $PI_SCRATCH/restricted-dir/ :  $ cd $PI_SCRATCH\n\n### Create new directory\n$ mkdir restricted-dir\n\n### Remove 'group' and 'other' access\n$ chmod g-rwx,o-rwx restricted-dir\n\n### Give user bob read and traversal permissions to the directory\n$ setfacl -m u:bob:rX restricted-dir\n\n### Use default ACLs (-d) to give user bob read access to all new\n### files and sub-directories that will be created in  restricted-dir \n$ setfacl -d -m u:bob:rX restricted-dir\n\n### Give user alice read, write and traversal permissions for the directory\n$ setfacl -m u:alice:rwX restricted-dir\n\n### Use default ACLs (-d) to give user alice read and write access to all\n### new files and sub-directories\n$ setfacl -d -m u:alice:rwX restricted-dir\n\n### Show ACLs\n$ getfacl restricted-dir\n# file: restricted-dir/\n# owner: joe\n# group: grp\n# flags: -s-\nuser::rwx\nuser:bob:r-x\ngroup::---\nmask::r-x\nother::---\ndefault:user::rwx\ndefault:user:alice:rwx\ndefault:user:bob:r-x\ndefault:group::---\ndefault:mask::rwx\ndefault:other::---   Default permissions on  $PI_SCRATCH  don't allow sharing outside of your group  By default, the Unix permissions on the root directory  $PI_SCRATCH  don't\nallow read nor traversal access for  others  ( ie.  any user no part of\nyour PI group). If you need to share files with users outside of your own\ngroup, please  contact us  so we can set the appropriate\npermissions on your folder.   For  $SCRATCH , you're the owner of the directory and so you can change the\npermissions yourself.", 
            "title": "POSIX ACLs"
        }, 
        {
            "location": "/docs/user-guide/storage/data-sharing/#nfsv4-acls", 
            "text": "$HOME  and  $PI_HOME  also allow setting ACLs, albeit with different\nsyntax and semantics than POSIX ACLs. The principle is very similar,\nthough.  An ACL in NFSv4 is a list of rules setting permissions on files or directories.\nA permission rule, or Access Control Entry (ACE), is of the form type:flags:principle:permissions .  Commonly used entries for these fields are:   type :  A  (allow) or  D  (deny)  flags :  g  (group),  d  (directory-inherit),  f  (file-inherit),  n \n  (no-propagate-inherit), or  i  (inherit-only)  principle :  a named user ( user@sherlock ), a group, or one of three\n  special principles:  OWNER@ ,  GROUP@ , and  EVERYONE@ .  permissions : there are 14 permission characters, as well as the shortcuts\n   R ,  W , and  X . Here is a list of possible permissions that can be\n  included in the permissions field (options are Case Sensitive)  r  read-data (files) / list-directory (directories)  w  write-data (files) / create-file (directories)  x  execute (files) / change-directory (directories)  a  append-data (files) / create-subdirectory (directories)  t  read-attributes: read the attributes of the file/directory.  T  write-attributes: write the attributes of the file/directory.  n  read-named-attributes: read the named attributes of the\n  file/directory.  N  write-named-attributes: write the named attributes of the\n  file/directory.  c  read-ACL: read the file/directory NFSv4 ACL.  C  write-ACL: write the file/directory NFSv4 ACL.  o  write-owner: change ownership of the file/directory.  y  synchronize: allow clients to use synchronous I/O with the server.  d  delete: delete the file/directory. Some servers will allow a delete\n  to occur if either this permission is set in the file/directory or if the\n  delete-child permission is set in its parent direcory.  D  delete-child: remove a file or subdirectory from within the given\n  directory (directories only)      A comprehensive listing of allowable field strings is given in the manual page nfs4_acl(5)  To see what permissions are set on a particular file, use the  nfs4_getfacl \ncommand. For example, newly created  file1  may have default permissions listed\nby  ls -l  as  -rw-r\u2014r\u2014 . Listing the permissions with  nfs4_getfacl  would\ndisplay the following:  $ nfs4_getfacl file1\nA::OWNER@:rwatTnNcCoy\nA:g:GROUP@:rtncy\nA::EVERYONE@:rtncy  To set permissions on a file, use the  nfs4_setfacl  command. For convenience,\nNFSv4 provides the shortcuts  R ,  W  and  X  for setting read, write, and\nexecute permissions. For example, to add write permissions for the current\ngroup on  file1 , use  nfs4_setfacl  with the  -a  switch:  $ nfs4_setfacl -a A::GROUP@:W file1  This command switched the  GROUP@  permission field from  rtncy  to rwatTnNcCoy .  However, be aware that NFSv4 file permission shortcuts have a\ndifferent meanings than the traditional Unix  r ,  w , and  x . For example\nissuing  chmod g+w file1  will set  GROUP@  to  rwatncy .  Although the shortcut permissions can be handy, often rules need to be more\ncustomized. Use  nfs4_setfacl -e file1   to open the ACL for  file1  in a text\neditor.  Access Control Entries allow more fine grained control over file and directory\npermissions than does the  chmod  command. For example, if user  joe  wants to\ngive read and write permissions to  jack  for her directory  private , she\nwould issue:  $ nfs4_setfacl -R -a A::jack@sherlock:RW private/  The  -R  switch recursively applies the rule to the files and directories\nwithin  private/  as well.  To allow  jack  to create files and subdirectories within  private/  with the\npermissions as granted above, inheritance rules need to be applied.  $ nfs4_setfacl -R -a A:fdi:jack@sherlock:RW private/  By default, each permission is in the Deny state and an ACE is required to\nexplicitly allow a permission. However, be aware that a server may silently\noverride a users ACE, usually to a less permissive setting.  For complete documentation and examples on using NFSv4 ACLs, please see the\nmanual page at  nfs4_acl(5) .", 
            "title": "NFSv4 ACLs"
        }, 
        {
            "location": "/docs/user-guide/storage/data-sharing/#sharing-data-outside-of-sherlock", 
            "text": "Work in progress   This page is a work in progress and is not complete yet. We are actively\nworking on adding more content and information.", 
            "title": "Sharing data outside of Sherlock"
        }, 
        {
            "location": "/docs/user-guide/storage/data-protection/", 
            "text": "Data protection is mostly a task for the user\n\n\nExcept for \n$HOME\n and \n$PI_HOME\n, data on Sherlock is not backed up,\nnor archived. It's up to each user and group to make sure they maintain\nmultiple copies of their data if needed.\n\n\n\n\nSnapshots\n#\n\n\nFile system snapshots represent the state of the file system at a particular\npoint in time. They allow accessing the file system contents as it was a\ndifferent times in the past, and get back data that may have been deleted or\nmodified since the snapshot was taken.\n\n\n\n\nImportant\n\n\nSnapshots are only available on \n$HOME\n and \n$PI_HOME\n.\n\n\n\n\nAccessing snapshots\n#\n\n\nSnapshots taken in \n$HOME\n and \n$PI_HOME\n are accessible in a \n.snapshot\n\ndirectory at any level of the hierarchy. Those \n.snapshot\n directories don't\nappear when listing directory contents with \nls\n, but they can be listed\nexplicitly or accessed with \ncd\n:\n\n\n$ cd $HOME\n$ ls -ald .snapshot/home*\n[...]\ndrwx------ 118 sunetid group  6680 Jul 21 11:16 .snapshot/home.daily.20170721\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.daily.20170722\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.daily.20170723\ndrwx------ 118 sunetid group  6702 Jul 24 10:57 .snapshot/home.daily.20170724\ndrwx------ 118 sunetid group  6702 Jul 24 10:57 .snapshot/home.daily.latest\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.hourly.20170722-16:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.hourly.20170722-17:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.hourly.20170722-18:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.hourly.20170722-19:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.hourly.20170722-20:00\n[...]\n$ cd .snapshot/home.daily.latest\n\n\n\nFor instance:\n\n\n\n\nthe \n$HOME/.snapshot/home.daily.latest\n directory is the latest daily\n  snapshot available, and store the contents of the $HOME directory as they\n  were when the last daily snapshot was taken,\n\n\nthe \n$HOME/foo/.snapshot/home.hourly.20170722-18:00\n can be used to retrieve\n  the contents of the \n$HOME/foo\n directory as it was at 6pm on July 22th,\n  2017.\n\n\n\n\nRestoring from a snapshot\n#\n\n\nIf you deleted a file or modified it and want to restore an earlier version,\nyou can simply copy the file from its saved version in the appropriate\nsnapshot.\n\n\nExamples:\n\n\n\n\n\n\nto restore the last known version of \n$HOME/foo/bar\n:\n\n\n$ cp $HOME/foo/.snapshot/home.hourly.latest/bar $HOME/foo/bar\n\nor\n\n$ cp $HOME/.snapshot/foo/home.hourly.latest/bar $HOME/foo/bar\n\n\n(both commands are equivalent)\n\n\n\n\n\n\nto restore your \n~/.bashrc\n file from 2 days ago:\n\n\n$ SNAP_DATE=$(date +%Y%m%d -d \n2 days ago\n)\n$ cp $HOME/.snapshot/home.daily.${SNAP_DATE}/.bashrc $HOME/.bashrc\n\n\n\n\n\n\n\nSnapshot policy\n#\n\n\nThe current\n1\n policy is to take snapshots on an hourly, daily and\nweekly basis.  Older snapshots automatically expire after their retention\nperiod. The snapshot policy applies to both \n$HOME\n and \n$PI_HOME\n storage\nspaces.\n\n\n\n\n\n\n\n\nSnapshot frequency\n\n\nRetention period\n\n\nNumber of snapshots\n\n\n\n\n\n\n\n\n\n\nhourly\n\n\n2 days\n\n\n48\n\n\n\n\n\n\ndaily\n\n\n1 week\n\n\n7\n\n\n\n\n\n\nweekly\n\n\n1 month\n\n\n4\n\n\n\n\n\n\n\n\nThe shortest interval between snapshots is an hour. That means that if you\ncreate a file and then delete it within the hour, it won't appear in snapshots,\nand you won't be able to restore it.\n\n\nIf a file exists for more than an hour, and is then deleted, it will be present\nin the hourly snapshots for the next 48 hours, and you'll be able to retrieve\nit during that period. Similarly, if a file exists for more than a day, it\ncould be restored for up to 7 days.\n\n\nSnapshots don't count towards your quota.\n\n\nSnapshots, as well as the entire filesystem, are replicated to an off-site\nsystem, to ensure that data could be retrieved even in case of a catastrophic\nfailure of the whole system or datacenter-level disaster.\n\n\nBackups\n#\n\n\nAlthough the SRCC doesn't offer any backup service \nper se\n, we do provide all\nthe tools required to \ntransfer data\n in and out of Sherlock.\n\n\nSuggested options to backup your data include:\n\n\n\n\nOak\n, SRCC's long-term research data storage service\n  \n(\nRecommended\n)\n\n\nUniversity IT Storage options\n and \nbackup\n  services\n\n\nCloud storage providers (see the \nData transfer\n page for\n  information about the tools we provide to transfer files to/from the cloud)\n\n\n\n\n\n\n\n\n\n\n\n\nThe snapshot policy is subject to change and may be adjusted as\n       the storage system usage conditions evolve.", 
            "title": "Data protection"
        }, 
        {
            "location": "/docs/user-guide/storage/data-protection/#snapshots", 
            "text": "File system snapshots represent the state of the file system at a particular\npoint in time. They allow accessing the file system contents as it was a\ndifferent times in the past, and get back data that may have been deleted or\nmodified since the snapshot was taken.   Important  Snapshots are only available on  $HOME  and  $PI_HOME .", 
            "title": "Snapshots"
        }, 
        {
            "location": "/docs/user-guide/storage/data-protection/#accessing-snapshots", 
            "text": "Snapshots taken in  $HOME  and  $PI_HOME  are accessible in a  .snapshot \ndirectory at any level of the hierarchy. Those  .snapshot  directories don't\nappear when listing directory contents with  ls , but they can be listed\nexplicitly or accessed with  cd :  $ cd $HOME\n$ ls -ald .snapshot/home*\n[...]\ndrwx------ 118 sunetid group  6680 Jul 21 11:16 .snapshot/home.daily.20170721\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.daily.20170722\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.daily.20170723\ndrwx------ 118 sunetid group  6702 Jul 24 10:57 .snapshot/home.daily.20170724\ndrwx------ 118 sunetid group  6702 Jul 24 10:57 .snapshot/home.daily.latest\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.hourly.20170722-16:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.hourly.20170722-17:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.hourly.20170722-18:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.hourly.20170722-19:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/home.hourly.20170722-20:00\n[...]\n$ cd .snapshot/home.daily.latest  For instance:   the  $HOME/.snapshot/home.daily.latest  directory is the latest daily\n  snapshot available, and store the contents of the $HOME directory as they\n  were when the last daily snapshot was taken,  the  $HOME/foo/.snapshot/home.hourly.20170722-18:00  can be used to retrieve\n  the contents of the  $HOME/foo  directory as it was at 6pm on July 22th,\n  2017.", 
            "title": "Accessing snapshots"
        }, 
        {
            "location": "/docs/user-guide/storage/data-protection/#restoring-from-a-snapshot", 
            "text": "If you deleted a file or modified it and want to restore an earlier version,\nyou can simply copy the file from its saved version in the appropriate\nsnapshot.  Examples:    to restore the last known version of  $HOME/foo/bar :  $ cp $HOME/foo/.snapshot/home.hourly.latest/bar $HOME/foo/bar \nor $ cp $HOME/.snapshot/foo/home.hourly.latest/bar $HOME/foo/bar \n(both commands are equivalent)    to restore your  ~/.bashrc  file from 2 days ago:  $ SNAP_DATE=$(date +%Y%m%d -d  2 days ago )\n$ cp $HOME/.snapshot/home.daily.${SNAP_DATE}/.bashrc $HOME/.bashrc", 
            "title": "Restoring from a snapshot"
        }, 
        {
            "location": "/docs/user-guide/storage/data-protection/#snapshot-policy", 
            "text": "The current 1  policy is to take snapshots on an hourly, daily and\nweekly basis.  Older snapshots automatically expire after their retention\nperiod. The snapshot policy applies to both  $HOME  and  $PI_HOME  storage\nspaces.     Snapshot frequency  Retention period  Number of snapshots      hourly  2 days  48    daily  1 week  7    weekly  1 month  4     The shortest interval between snapshots is an hour. That means that if you\ncreate a file and then delete it within the hour, it won't appear in snapshots,\nand you won't be able to restore it.  If a file exists for more than an hour, and is then deleted, it will be present\nin the hourly snapshots for the next 48 hours, and you'll be able to retrieve\nit during that period. Similarly, if a file exists for more than a day, it\ncould be restored for up to 7 days.  Snapshots don't count towards your quota.  Snapshots, as well as the entire filesystem, are replicated to an off-site\nsystem, to ensure that data could be retrieved even in case of a catastrophic\nfailure of the whole system or datacenter-level disaster.", 
            "title": "Snapshot policy"
        }, 
        {
            "location": "/docs/user-guide/storage/data-protection/#backups", 
            "text": "Although the SRCC doesn't offer any backup service  per se , we do provide all\nthe tools required to  transfer data  in and out of Sherlock.  Suggested options to backup your data include:   Oak , SRCC's long-term research data storage service\n   ( Recommended )  University IT Storage options  and  backup\n  services  Cloud storage providers (see the  Data transfer  page for\n  information about the tools we provide to transfer files to/from the cloud)       The snapshot policy is subject to change and may be adjusted as\n       the storage system usage conditions evolve.", 
            "title": "Backups"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/", 
            "text": "Transfer protocols\n#\n\n\nA number of methods allow transferring data in/out of Sherlock. For most cases,\nwe recommend using \nSSH-based file transfer commands\n,\nsuch as \nscp\n, \nsftp\n, or \nrsync\n.  They will provide the best\nperformance for data transfers from and to campus.\n\n\nMost casual data transfers could be done through the login nodes, by pointing\nyour transfer tool to \nlogin.sherlock.stanford.edu\n. But because of resource\nlimits on the login nodes, larger transfer may not work as expected.\n\n\nFor transferring large amounts of data, Sherlock features a specific \nData\nTransfer Node\n, with dedicated bandwidth, as well as a managed \nGlobus\nOnline endpoint\n, that can be used for scheduled, unattended data\ntransfers.\n\n\nWe also provide tools on Sherlock to transfer data to various \nCloud\nproviders\n, such as AWS, Google Drive, Dropbox, Box, etc.\n\n\nPrerequisites\n#\n\n\nMost of the commands detailed below require a terminal and an SSH client\n1\n\non your local machine to launch commands.\n\n\nYou'll need to start a terminal and type the given example commands at the\nprompt, omitting the initial \n$\n character (it just indicates a command prompt,\nand then should not be typed in).\n\n\nSSH-based protocols\n#\n\n\n\n\nLogin name\n\n\nIn all the examples below, you'll need to replace \nsunetid\n by your\nactual SUNet ID. If you happen to use the same login name on your local\nmachine, you can omit it..\n\n\n\n\nSCP (Secure Copy)\n#\n\n\n\n\n\n\nThe easiest command to use to transfer files to/from Sherlock is \nscp\n. It\nworks like the \ncp\n command, except it can work over the network to copy\nfiles from one computer to another, using the secure SSH protocol.\n\n\nThe general syntax to copy a file to a remote server is:\n\n$ scp \nsource_file_path\n \nusername\n@\nremote_host\n:\ndestination_path\n'\n\n\nFor instance, the following command will copy the file named \nfoo\n from\nyour local machine to your home directory on Sherlock:\n\n$ scp foo \nsunetid\n@login.sherlock.stanford.edu:\n\nNote the \n:\n character, that separates the hostname from the destination\npath. Here, the destination path is empty, which will instruct scp to copy\nthe file in your home directory.\n\n\nYou can copy \nfoo\n under a different name, or to another directory, with\nthe following commands:\n\n$ scp foo \nsunetid\n@login.sherlock.stanford.edu:bar\n$ scp foo \nsunetid\n@login.sherlock.stanford.edu:~/subdir/baz\n\n\nTo copy back files from Sherlock to your local machine, you just need to\nreverse the order of the arguments:\n\n$ scp \nsunetid\n@login.sherlock.stanford.edu:foo local_foo\n\n\nAnd finally, \nscp\n also support recursive copying of directories, with the\n\n-r\n option:\n\n$ scp -r dir/ \nsunetid\n@login.sherlock.stanford.edu:dir/\n\nThis will copy the \ndir/\n directory and all of its contents in your home\ndirectory on Sherlock.\n\n\n\n\n\n\nSFTP (Secure File Transfer Protocol)\n#\n\n\n\n\n\n\nSFTP clients are interactive file transfer programs, similar to FTP, which\nperform all operations over an encrypted transport.\n\n\nA variety of graphical\nSFTP clients are available for different OSes:\n\n\n\n\nWinSCP\n \n\n\nFileZilla\n \n, \n, \n\n\nFetch\n2\n \n\n\nCyberDuck\n \n\n\n\n\nWhen setting up your connection to Sherlock in the above programs, use the\nfollowing information:\n\n\nHostname: login.sherlock.stanford.edu\nPort:     22\nUsername: SUNet ID\nPassword: SUNet ID password\n\n\n\n\nOpenSSH also provides a command-line SFTP client, originally named \nsftp\n.\n\n\nTo log in to Sherlock:\n\n$ sftp \nsunetid\n@login.sherlock.stanford.edu\nConnected to login.sherlock.stanford.edu.\nsftp\n\n\n\nFor more information about using the command-line SFTP client, you can\nrefer to this \ntutorial\n for more details and examples.\n\n\n\n\n\n\nrsync\n#\n\n\n\n\n\n\nIf you have complex hierarchies of files to transfer, or if you need to\nsynchronize a set of files and directories between your local machine and\nSherlock, \nrsync\n will be the best tool for the job. It will efficiently\ntransfer and synchronize files across systems, by checking the timestamp\nand size of files. Which means that it won't re-transfer files that have\nnot changed since the last transfer, and will complete faster.\n\n\nFor instance, to transfer the whole \n~/data/\n folder tree from your local\nmachine to your home directory on Sherlock, you can use the following\ncommand:\n\n$ rsync -a ~/data/ \nsunetid\n@login.sherlock.stanford.edu:data/\n\nNote the slash (\n/\n) at the end of the directories name,  which is\nimportant to instruct \nrsync\n to synchronize the whole directories.\n\n\nTo get more information about the transfer rate and follow its progress,\nyou can use additional options:\n\n$ rsync -avP ~/data/ \nsunetid\n@login.sherlock.stanford.edu:data/\nsending incremental file list\n./\nfile1\n      1,755,049 100%    2.01MB/s    0:00:00 (xfr#2, to-chk=226/240)\nfile2\n      2,543,699 100%    2.48MB/s    0:00:00 (xfr#3, to-chk=225/240)\nfile3\n     34,930,688  19%   72.62MB/s    0:00:08\n\n[...]\n\nFor more information about using the \nrsync\n, you can\nrefer to this \ntutorial\n for more details and examples.\n\n\n\n\n\n\nSSHFS\n#\n\n\n\n\n\n\nSometimes, moving files in and out of the cluster, and maintaining two\ncopies of each of the files you work on, both on your local machine and on\nSherlock, may be painful. Fortunately, Sherlock offers the ability to mount\nany of its filesystems to your local machine, using a secure and encrypted\nconnection.\n\n\nWith SSHFS, a FUSE-based filesystem implementation used to mount remote\nSSH-accessible filesystems, you can access your files on Sherlock as if\nthey were locally stored on your own computer.\n\n\nThis comes particularly handy when you need to access those files from an\napplication that is not available on Sherlock, but that you already use or\ncan install on your local machine. Like a data processing program that you\nhave licensed for your own computer but can't use on Sherlock, a specific\ntext editor that only runs on MacOS, or any data-intensive 3D rendering\nsoftware that wouldn't work comfortably enough over a forwarded X11\nconnection.\n\n\nSSHFS is available for \nLinux\n \n,\n\nMacOS\n \n, and \nWindows\n\n\n.\n\n\nFor instance, on a Linux machine with SSHFS installed, you could mount your\nSherlock home directory with the following commands:\n\n\n$ mkdir ~/sherlock_home\n$ sshfs \nsunetid\n@login.sherlock.stanford.edu:./ ~/sherlock_home\n\n\n\nAnd to unmount it:\n\n$ fusermount -u ~/sherlock_home\n\n\nFor more information about using SSHFS on your local machine, you can\nrefer to this \ntutorial\n for more details and examples.\n\n\n\n\n\n\nGlobus\n#\n\n\nGlobus improves SSH-based file transfer protocols by providing the following\nfeatures:\n\n\n\n\nautomates large data transfers,\n\n\ncan resume failed transfers,\n\n\nsimplifies the implementation of high-performance transfers between computing\n  centers.\n\n\n\n\nGlobus Online is a Software as a Service (SaaS) deployment of the Globus\nToolkit which provides end-users with a browser interface to initiate data\ntransfers between endpoints registered with the Globus Alliance. Globus Online\nallows registered users to \"drag and drop\" files from one endpoint to another.\nEndpoints are terminals for data; they can be laptops or supercomputers, and\nanything in between. The servers at Globus.org act as\nintermediaries-negotiating, monitoring and optimizing transfers through\nfirewalls and across network address translation (NAT). Under certain\ncircumstances with high performance hardware transfer rates exceeding 1 GB/s\nare possible. For more information about Globus, please see the \nGlobus.org's\ndocumentation\n.\n\n\nAuthentication\n#\n\n\nTo use Globus Online, you will first need to authenticate at\n\nGlobus.org\n. You can either sign up for a Globus.org account, or\nuse your SUNet ID account for authentication to Globus Online (which will be\nrequired to authenticate to the Sherlock endpoint).\n\n\nTo use your SUNet ID, choose \"Stanford University\" from the drop down menu at\nthe \nLogin page\n and follow the instructions from there.\n\n\nTransfer\n#\n\n\n\n\nEndpoint name\n\n\nThe Globus endpoint name for Sherlock is \nSRCC Sherlock\n (aka\n\nsrcc#sherlock\n)\n\n\n\n\nYou can use Globus Online to transfer data between your local workstation\n(e.g., your laptop or desktop) and Sherlock. In this workflow, you\nconfigure your local workstation as a Globus endpoint using Globus Connect.\n\n\n\n\nLog in to \nGlobus.org\n\n\nUse the \nManage Endpoints\n interface to \"add Globus\n   Connect Personal\" as an endpoint (you'll need to install Globus Connect\n   Personal on your local machine)\n\n\nTransfer Files\n, using your new workstation endpoint\n   for one side of the transfer, and the Sherlock endpoint (\nSRCC Sherlock\n) on\n   the other side.\n\n\n\n\nYou can also transfer data between two remote endpoints, by choosing another\nendpoint you have access to instead of your local machine.\n\n\nCLI\n#\n\n\nGlobus also provides a command-line interface as an alternative to its web\ninterface. This command-line interface is provided over an SSH connection to a\nGlobus.org server. Please see the \nGlobus CLI documentation\n for\nmore details.\n\n\nData Transfer Node (DTN)\n#\n\n\nA dedicated Data Transfer Node is available on Sherlock, to provide exclusive\nresources for large-scale data transfers.\n\n\nThe main benefit of using it is that\ntransfer tasks can't be disrupted by other users interactive tasks or\nfilesystem access and I/O-related workloads on the login nodes.\n\n\nBy using the Sherlock DTN, you'll make sure that your data flows will go\nthrough a computer whose sole purpose is to move data around.\n\n\nIt supports:\n\n\n\n\nSSH-based protocols (such as the ones \ndescribed above\n)\n\n\nBBCP\n\n\nGlobus\n\n\n\n\nTo transfer files via the DTN, simply use \ndtn.sherlock.stanford.edu\n as a\nremote server hostname. For instance:\n\n\n$ scp foo \nsunetid\n@dtn.sherlock.stanford.edu:~/foo\n\n\n\n\n\nNo shell\n\n\nThe DTN doesn't provide any interactive shell, so connecting via SSH\ndirectly won't work. It will only accept \nscp\n, \nsftp\n, \nrsync\n of \nbbcp\n\nconnections.\n\n\n\n\nOne important difference to keep in mind when transferring files through the\nSherlock DTN is that the default destination path for files, unless specified,\nis the user \n$SCRATCH\n directory, not \n$HOME\n.\n\n\nThat means that the following command:\n\n$ scp foo \nsunetid\n@dtn.sherlock.stanford.edu:\n\nwill create the \nfoo\n file in  \n$SCRATCH/foo\n, and not in \n$HOME/foo\n.\n\n\nYou can transfer file to your \n$HOME\n directory via the DTN by specifying the\nfull path as the destination:\n\n$ scp foo \nsunetid\n@dtn.sherlock.stanford.edu:$HOME/foo\n\n\nCloud storage\n#\n\n\nIf you need to backup some of your Sherlock files to cloud-based storage\nservices, we also provide a set of utilities that can help.\n\n\nGoogle Drive\n#\n\n\n\n\nGoogle Drive storage for Stanford users\n\n\nGoogle Drive is free for educational institutions. Meaning you can get\n\nfree\n and \nunlimited\n storage on Google Drive using your @stanford.edu\naccount. See the \nUniversity IT Google Drive page\n for more\ndetails.\n\n\n\n\nWe provide the \ngdrive\n tool on Sherlock to interact with Google Drive. You'll\njust need to load the \ngdrive\n module to be able to use it to move your\nfiles from/to Google Drive:\n\n\n$ module load system gdrive\n$ gdrive help\n\n\n\nFor more details about how to use \ngdrive\n, please see the \nofficial\ndocumentation\n.\n\n\nOther services\n#\n\n\nIf you need to access other cloud storage services, you can use\n\nrclone\n: it can be used to sync files and directories to and from\nGoogle Drive, Amazon S3, Dropbox, Google Cloud Storage, Amazon Drive, Microsoft\nOneDrive and many more.\n\n\n$ ml load system rclone\n$ rclone -h\n\n\n\nFor more details about how to use \nrclone\n, please see the \nofficial\ndocumentation\n.\n\n\n\n\n\n\n\n\n\n\nFor more details, see the \nSSH clients page\n.\n\n\n\n\n\n\nFetch is a commercial program, and is available as part of\n  the \nEssential Stanford Software\n bundle.", 
            "title": "Data transfer"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#transfer-protocols", 
            "text": "A number of methods allow transferring data in/out of Sherlock. For most cases,\nwe recommend using  SSH-based file transfer commands ,\nsuch as  scp ,  sftp , or  rsync .  They will provide the best\nperformance for data transfers from and to campus.  Most casual data transfers could be done through the login nodes, by pointing\nyour transfer tool to  login.sherlock.stanford.edu . But because of resource\nlimits on the login nodes, larger transfer may not work as expected.  For transferring large amounts of data, Sherlock features a specific  Data\nTransfer Node , with dedicated bandwidth, as well as a managed  Globus\nOnline endpoint , that can be used for scheduled, unattended data\ntransfers.  We also provide tools on Sherlock to transfer data to various  Cloud\nproviders , such as AWS, Google Drive, Dropbox, Box, etc.", 
            "title": "Transfer protocols"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#prerequisites", 
            "text": "Most of the commands detailed below require a terminal and an SSH client 1 \non your local machine to launch commands.  You'll need to start a terminal and type the given example commands at the\nprompt, omitting the initial  $  character (it just indicates a command prompt,\nand then should not be typed in).", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#ssh-based-protocols", 
            "text": "Login name  In all the examples below, you'll need to replace  sunetid  by your\nactual SUNet ID. If you happen to use the same login name on your local\nmachine, you can omit it..", 
            "title": "SSH-based protocols"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#scp-secure-copy", 
            "text": "The easiest command to use to transfer files to/from Sherlock is  scp . It\nworks like the  cp  command, except it can work over the network to copy\nfiles from one computer to another, using the secure SSH protocol.  The general syntax to copy a file to a remote server is: $ scp  source_file_path   username @ remote_host : destination_path '  For instance, the following command will copy the file named  foo  from\nyour local machine to your home directory on Sherlock: $ scp foo  sunetid @login.sherlock.stanford.edu: \nNote the  :  character, that separates the hostname from the destination\npath. Here, the destination path is empty, which will instruct scp to copy\nthe file in your home directory.  You can copy  foo  under a different name, or to another directory, with\nthe following commands: $ scp foo  sunetid @login.sherlock.stanford.edu:bar\n$ scp foo  sunetid @login.sherlock.stanford.edu:~/subdir/baz  To copy back files from Sherlock to your local machine, you just need to\nreverse the order of the arguments: $ scp  sunetid @login.sherlock.stanford.edu:foo local_foo  And finally,  scp  also support recursive copying of directories, with the -r  option: $ scp -r dir/  sunetid @login.sherlock.stanford.edu:dir/ \nThis will copy the  dir/  directory and all of its contents in your home\ndirectory on Sherlock.", 
            "title": "SCP (Secure Copy)"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#sftp-secure-file-transfer-protocol", 
            "text": "SFTP clients are interactive file transfer programs, similar to FTP, which\nperform all operations over an encrypted transport.  A variety of graphical\nSFTP clients are available for different OSes:   WinSCP    FileZilla   ,  ,   Fetch 2    CyberDuck     When setting up your connection to Sherlock in the above programs, use the\nfollowing information:  Hostname: login.sherlock.stanford.edu\nPort:     22\nUsername: SUNet ID\nPassword: SUNet ID password  OpenSSH also provides a command-line SFTP client, originally named  sftp .  To log in to Sherlock: $ sftp  sunetid @login.sherlock.stanford.edu\nConnected to login.sherlock.stanford.edu.\nsftp  \nFor more information about using the command-line SFTP client, you can\nrefer to this  tutorial  for more details and examples.", 
            "title": "SFTP (Secure File Transfer Protocol)"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#rsync", 
            "text": "If you have complex hierarchies of files to transfer, or if you need to\nsynchronize a set of files and directories between your local machine and\nSherlock,  rsync  will be the best tool for the job. It will efficiently\ntransfer and synchronize files across systems, by checking the timestamp\nand size of files. Which means that it won't re-transfer files that have\nnot changed since the last transfer, and will complete faster.  For instance, to transfer the whole  ~/data/  folder tree from your local\nmachine to your home directory on Sherlock, you can use the following\ncommand: $ rsync -a ~/data/  sunetid @login.sherlock.stanford.edu:data/ \nNote the slash ( / ) at the end of the directories name,  which is\nimportant to instruct  rsync  to synchronize the whole directories.  To get more information about the transfer rate and follow its progress,\nyou can use additional options: $ rsync -avP ~/data/  sunetid @login.sherlock.stanford.edu:data/\nsending incremental file list\n./\nfile1\n      1,755,049 100%    2.01MB/s    0:00:00 (xfr#2, to-chk=226/240)\nfile2\n      2,543,699 100%    2.48MB/s    0:00:00 (xfr#3, to-chk=225/240)\nfile3\n     34,930,688  19%   72.62MB/s    0:00:08\n\n[...] \nFor more information about using the  rsync , you can\nrefer to this  tutorial  for more details and examples.", 
            "title": "rsync"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#sshfs", 
            "text": "Sometimes, moving files in and out of the cluster, and maintaining two\ncopies of each of the files you work on, both on your local machine and on\nSherlock, may be painful. Fortunately, Sherlock offers the ability to mount\nany of its filesystems to your local machine, using a secure and encrypted\nconnection.  With SSHFS, a FUSE-based filesystem implementation used to mount remote\nSSH-accessible filesystems, you can access your files on Sherlock as if\nthey were locally stored on your own computer.  This comes particularly handy when you need to access those files from an\napplication that is not available on Sherlock, but that you already use or\ncan install on your local machine. Like a data processing program that you\nhave licensed for your own computer but can't use on Sherlock, a specific\ntext editor that only runs on MacOS, or any data-intensive 3D rendering\nsoftware that wouldn't work comfortably enough over a forwarded X11\nconnection.  SSHFS is available for  Linux   , MacOS   , and  Windows  .  For instance, on a Linux machine with SSHFS installed, you could mount your\nSherlock home directory with the following commands:  $ mkdir ~/sherlock_home\n$ sshfs  sunetid @login.sherlock.stanford.edu:./ ~/sherlock_home  And to unmount it: $ fusermount -u ~/sherlock_home  For more information about using SSHFS on your local machine, you can\nrefer to this  tutorial  for more details and examples.", 
            "title": "SSHFS"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#globus", 
            "text": "Globus improves SSH-based file transfer protocols by providing the following\nfeatures:   automates large data transfers,  can resume failed transfers,  simplifies the implementation of high-performance transfers between computing\n  centers.   Globus Online is a Software as a Service (SaaS) deployment of the Globus\nToolkit which provides end-users with a browser interface to initiate data\ntransfers between endpoints registered with the Globus Alliance. Globus Online\nallows registered users to \"drag and drop\" files from one endpoint to another.\nEndpoints are terminals for data; they can be laptops or supercomputers, and\nanything in between. The servers at Globus.org act as\nintermediaries-negotiating, monitoring and optimizing transfers through\nfirewalls and across network address translation (NAT). Under certain\ncircumstances with high performance hardware transfer rates exceeding 1 GB/s\nare possible. For more information about Globus, please see the  Globus.org's\ndocumentation .", 
            "title": "Globus"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#authentication", 
            "text": "To use Globus Online, you will first need to authenticate at Globus.org . You can either sign up for a Globus.org account, or\nuse your SUNet ID account for authentication to Globus Online (which will be\nrequired to authenticate to the Sherlock endpoint).  To use your SUNet ID, choose \"Stanford University\" from the drop down menu at\nthe  Login page  and follow the instructions from there.", 
            "title": "Authentication"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#transfer", 
            "text": "Endpoint name  The Globus endpoint name for Sherlock is  SRCC Sherlock  (aka srcc#sherlock )   You can use Globus Online to transfer data between your local workstation\n(e.g., your laptop or desktop) and Sherlock. In this workflow, you\nconfigure your local workstation as a Globus endpoint using Globus Connect.   Log in to  Globus.org  Use the  Manage Endpoints  interface to \"add Globus\n   Connect Personal\" as an endpoint (you'll need to install Globus Connect\n   Personal on your local machine)  Transfer Files , using your new workstation endpoint\n   for one side of the transfer, and the Sherlock endpoint ( SRCC Sherlock ) on\n   the other side.   You can also transfer data between two remote endpoints, by choosing another\nendpoint you have access to instead of your local machine.", 
            "title": "Transfer"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#cli", 
            "text": "Globus also provides a command-line interface as an alternative to its web\ninterface. This command-line interface is provided over an SSH connection to a\nGlobus.org server. Please see the  Globus CLI documentation  for\nmore details.", 
            "title": "CLI"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#data-transfer-node-dtn", 
            "text": "A dedicated Data Transfer Node is available on Sherlock, to provide exclusive\nresources for large-scale data transfers.  The main benefit of using it is that\ntransfer tasks can't be disrupted by other users interactive tasks or\nfilesystem access and I/O-related workloads on the login nodes.  By using the Sherlock DTN, you'll make sure that your data flows will go\nthrough a computer whose sole purpose is to move data around.  It supports:   SSH-based protocols (such as the ones  described above )  BBCP  Globus   To transfer files via the DTN, simply use  dtn.sherlock.stanford.edu  as a\nremote server hostname. For instance:  $ scp foo  sunetid @dtn.sherlock.stanford.edu:~/foo   No shell  The DTN doesn't provide any interactive shell, so connecting via SSH\ndirectly won't work. It will only accept  scp ,  sftp ,  rsync  of  bbcp \nconnections.   One important difference to keep in mind when transferring files through the\nSherlock DTN is that the default destination path for files, unless specified,\nis the user  $SCRATCH  directory, not  $HOME .  That means that the following command: $ scp foo  sunetid @dtn.sherlock.stanford.edu: \nwill create the  foo  file in   $SCRATCH/foo , and not in  $HOME/foo .  You can transfer file to your  $HOME  directory via the DTN by specifying the\nfull path as the destination: $ scp foo  sunetid @dtn.sherlock.stanford.edu:$HOME/foo", 
            "title": "Data Transfer Node (DTN)"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#cloud-storage", 
            "text": "If you need to backup some of your Sherlock files to cloud-based storage\nservices, we also provide a set of utilities that can help.", 
            "title": "Cloud storage"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#google-drive", 
            "text": "Google Drive storage for Stanford users  Google Drive is free for educational institutions. Meaning you can get free  and  unlimited  storage on Google Drive using your @stanford.edu\naccount. See the  University IT Google Drive page  for more\ndetails.   We provide the  gdrive  tool on Sherlock to interact with Google Drive. You'll\njust need to load the  gdrive  module to be able to use it to move your\nfiles from/to Google Drive:  $ module load system gdrive\n$ gdrive help  For more details about how to use  gdrive , please see the  official\ndocumentation .", 
            "title": "Google Drive"
        }, 
        {
            "location": "/docs/user-guide/storage/data-transfer/#other-services", 
            "text": "If you need to access other cloud storage services, you can use rclone : it can be used to sync files and directories to and from\nGoogle Drive, Amazon S3, Dropbox, Google Cloud Storage, Amazon Drive, Microsoft\nOneDrive and many more.  $ ml load system rclone\n$ rclone -h  For more details about how to use  rclone , please see the  official\ndocumentation .      For more details, see the  SSH clients page .    Fetch is a commercial program, and is available as part of\n  the  Essential Stanford Software  bundle.", 
            "title": "Other services"
        }, 
        {
            "location": "/docs/user-guide/running-jobs/", 
            "text": "Login nodes\n#\n\n\n\n\nLogin nodes are not for computing\n\n\nLogin nodes are shared among many users and therefore must not be\nused to run computationally intensive tasks. Those should be submitted to\nthe scheduler which will dispatch them on compute nodes.\n\n\n\n\nThe key principle of a shared computing environment is that resources are\nshared among users and must be scheduled. It is \nmandatory\n to schedule work\nby submitting jobs to the scheduler on Sherlock. And since login nodes are a\nshared resource, they must not be used to execute computing tasks.\n\n\nAcceptable use of login nodes include:\n\n\n\n\nlightweight file transfers,\n\n\nscript and configuration file editing,\n\n\njob submission and monitoring.\n\n\n\n\n\n\nResource limits are enforced\n\n\nTo minimize disruption and ensure a confortable working environment for\nusers, resource limits are enforced on login nodes, and processes started\nthere will automatically be terminated if their resource usage (including\nCPU time, memory and run time) exceed those limits.\n\n\n\n\nSlurm commands\n#\n\n\nSlurm allows requesting resources and submitting jobs in a variety of ways. The\nmain Slurm commands to submit jobs are listed in the table below:\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\nBehavior\n\n\n\n\n\n\n\n\n\n\nsalloc\n\n\nRequest resources and allocates them to a job\n\n\nStarts a new shell, but does not execute anything\n\n\n\n\n\n\nsrun\n\n\nRequest resources and runs a command on the allocated compute node(s)\n\n\nBlocking command: will not return until the job ends\n\n\n\n\n\n\nsbatch\n\n\nRequest resources and runs a script on the allocated compute node(s)\n\n\nAsynchronous command: will return as soon as the job is submitted\n\n\n\n\n\n\n\n\nInteractive jobs\n#\n\n\nDedicated nodes\n#\n\n\nInteractive jobs allow users to log in to a compute node to run commands\ninteractively on the command line. They could be an integral\npart of an interactive programming and debugging workflow. The simplest way to\nestablish an interactive session on Sherlock is to use the \nsdev\n command:\n\n\n$ sdev\n\n\n\nThis will open a login shell using one core and 4 GB of memory on one node for\none hour. The \nsdev\n sessions run on dedicated compute nodes. This ensures\nminimal wait times when you need to access a node for testing script, debug\ncode or any kind of interactive work.\n\n\nsdev\n also provides X11 forwarding via the submission host (typically the\nlogin node you're connected to) and can thus be used to run GUI applications.\n\n\nCompute nodes\n#\n\n\nIf you need more resources\n1\n, you can pass options to \nsdev\n, to\nrequest more CPU cores, more nodes, or even run in a different partition.\n\nsdev -h\n will provide more information:\n\n\n$ sdev -h\nsdev: start an interactive shell on a compute node\n\nUsage: sdev [OPTIONS]\n    Optional arguments:\n        -n      number of CPU cores to request (default: 1)\n        -N      number of nodes to request (default: 1)\n        -m      memory amount to request (default: 4GB)\n        -p      partition to run the job in (default: dev)\n        -t      time limit (default: 01:00:00, ie. 1 hour)\n        -r      allocate resources from the named reservation (default: none)\n        -J      job name (default: sdev)\n        -q      quality of service to request for the job (default: normal)\n\n\n\n\nAnother way to get an interactive session on a compute node is to use \nsrun\n to\nexecute a shell through the scheduler. For instance, to start a \nbash\n session\non a compute node, with the default resource requirements (one core for 2\nhours), you can run:\n\n\n$ srun --pty bash\n\n\n\nThe main advantage of this approach is that it will allow you to specify the\nwhole range of submission options that \nsdev\n may not support.\n\n\nFinally, if you prefer to submit an existing job script or other executable as\nan interactive job, you can use the \nsalloc\n command:\n\n\n$ salloc script.sh\n\n\n\nIf you don't provide a command to execute, \nsalloc\n will start a Slurm job and\nallocate resources for it, but it will not automatically connect you to the\nallocated node(s). It will only start a new shell on the same node you launched\n\nsalloc\n from, and set up the appropriate \n$SLURM_*\n environment\nvariables. So you will typically need to look at them to see\nwhat nodes have been assigned to your job. For instance:\n\n\n$ salloc\nsalloc: Granted job allocation 655914\n$ echo $SLURM_NODELIST\nsh-101-55\n$ ssh sh-101-55\n[...]\nsh-101-55 ~ $\n\n\n\nConnecting to nodes\n#\n\n\n\n\nLogin to compute nodes\n\n\nUsers are not allowed to login to compute nodes unless they have a job\nrunning there.\n\n\n\n\nIf you SSH to a compute node without any active job allocation, you'll be\ngreeted by the following message:\n\n\n$ ssh sh-101-01\nAccess denied by pam_slurm_adopt: you have no active jobs on this node\nConnection closed\n$\n\n\n\nOnce you have a job running on a node, you can SSH directly to it and\nrun additional processes\n2\n, or observe how you application\nbehaves, debug issues, and so on.\n\n\nThe \nsalloc\n command supports the same parameters as \nsbatch\n, and can override\nany default configuration. Note that any \n#SBATCH\n directive in your job script\nwill not be interpreted by \nsalloc\n when it is executed in this way. You must\nspecify all arguments directly on the command line for them to be taken into\naccount.\n\n\nBatch jobs\n#\n\n\n\n\n Work in progress \n\n\nThis page is a work in progress and is not complete yet. We are actively\nworking on adding more content and information.\n\n\n\n\nRecurring jobs\n#\n\n\n\n\nWarning\n\n\nCron\n tasks are not supported on Sherlock.\n\n\n\n\nUsers are not allowed to create \ncron\n jobs on Sherlock, for a variety of\nreasons:\n\n\n\n\nresources limits cannot be easily enforced in \ncron\n jobs, meaning that a\n  single user can end up monopolizing all the resources of a login node,\n\n\nno amount of resources can be guaranteed when executing a \ncron\n job, leading\n  to unreliable runtime and performance,\n\n\nuser \ncron\n jobs have the potential of bringing down whole\n  nodes by creating fork bombs, if they're not carefully crafted and tested,\n\n\ncompute and login nodes could be redeployed at any time, meaning that\n  \ncron\n jobs scheduled there could go away without the user being notified,\n  and cause all sorts of unexpected results,\n\n\ncron\n jobs could be mistakenly scheduled on several nodes and run multiple\n  times, which could result in corrupted files.\n\n\n\n\nAs an alternative, if you need to run recurring tasks at regular intervals, we\nrecommend the following approach: by using the \n--begin\n job submission option,\nand creating a job that resubmits itself once it's done, you can virtually\nemulate the behavior and benefits of a \ncron\n job, without its disadvantages:\nyour task will be scheduled on a compute node, and use all of the resources it\nrequested, without being impacted by anything else.\n\n\nDepending on your recurring job's specificities, where you submit it and the\nstate of the cluster at the time of execution, the starting time of that task\nmay not be guaranteed and result in a delay in execution, as it will be\nscheduled by Slurm like any other jobs. Typical recurring jobs, such as file\nsynchronization, database updates or backup tasks don't require strict starting\ntimes, though, so most users find this an acceptable trade-off.\n\n\nThe table below summarizes the advantages and inconvenients of each approach:\n\n\n\n\n\n\n.yes { color: darkgreen; }\n.no  { color: darkred;   }\n\n\n\n\n\n\n\n\n\n\n\n\nCron tasks\n\n\nRecurring jobs\n\n\n\n\n\n\n\n\n\n\nAuthorized on Sherlock\n\n\n\n\n\n\n\n\n\n\nDedicated resources for the task\n\n\n\n\n\n\n\n\n\n\nPersistent across node redeployments\n\n\n\n\n\n\n\n\n\n\nUnique, controlled execution\n\n\n\n\n\n\n\n\n\n\nPotential execution delay\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n#\n\n\nThe script below presents an example of such a recurring, self-resubmitting\njob, that would emulate a \ncron\n task. It will append a timestamped line to a\n\ncron.log\n file in your \n$HOME\n directory and run every 7 days.\n\n\n#!/bin/bash\n#SBATCH --job-name=cron\n#SBATCH --begin=now+7days\n#SBATCH --dependency=singleton\n#SBATCH --time=00:02:00\n#SBATCH --mail-type=FAIL\n\n\n## Insert you the command to run below. Here, we're just storing the date in a\n## cron.log file\n\ndate -R \n $HOME/cron.log\n\n## Resubmit the job for the next execution\nsbatch $0\n\n\n\nIf the job payload (here the \ndate\n command) fails for some reason and\ngenerates and error, the job will not be resubmitted, and the user will be\nnotified by email.\n\n\nWe encourage users to get familiar with the submission options used in this\nscript by giving a look at the \nsbatch\n \nman page\n, but some\ndetails are given below:\n\n\n\n\n\n\n\n\nSubmission\noption\nor\ncommand\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\n--job-name=cron\n\n\nmakes it easy to identify the job, is used by the  \n--dependency=singleton\n option to identify identical jobs, and will allow  cancelling the job by name (because its jobid will change each time it's  submitted)\n\n\n\n\n\n\n--begin=now+7days\n\n\nwill instruct the scheduler to not even consider the job   for scheduling before 7 days after it's been submitted\n\n\n\n\n\n\n--dependency=singleton\n\n\nwill make sure that only one \ncron\n job runs at any given time\n\n\n\n\n\n\n--time=00:02:00\n\n\nruntime limit for the job (here 2 minutes). You'll need to adjust the value   depending on the task you need to run (shorter runtime requests usually   result in the job running closer to the clock mark)\n\n\n\n\n\n\n--mail-type=FAIL\n\n\nwill send an email notification to the user if the job ever fails\n\n\n\n\n\n\nsbatch $0\n\n\nwill resubmit the job script by calling its own name (\n$0\n)   after successful execution\n\n\n\n\n\n\n\n\nYou can save the script as \ncron.sbatch\n or any other name, and submit it with:\n\n\n$ sbatch cron.sbatch\n\n\n\nIt will start running for the first time 7 days after\nyou submit it, and it will continue to run until you cancel it with the\nfollowing command (using the job name, as defined by the \n--job-name\n option):\n\n\n$ scancel -n cron\n\n\n\n\n\n\n\n\n\n\n\nThe dedicated partition that \nsdev\n uses by default only allows\n  up to 2 cores and 8 GB or memory per user at any given time. So if you need\n  more resources for your interactive session, you may have to specify a\n  different partition. See the \nPartitions\n section for more\n  details.\n\n\n\n\n\n\nPlease note that your SSH session will be attached to your\n  running job, and that resources used by that interactive shell will count\n  towards your job's resource limits. So if you start a process using large\n  amounts of memory via SSH while your job is running, you may hit the job's\n  memory limits, which will trigger its termination.", 
            "title": "Running jobs"
        }, 
        {
            "location": "/docs/user-guide/running-jobs/#login-nodes", 
            "text": "Login nodes are not for computing  Login nodes are shared among many users and therefore must not be\nused to run computationally intensive tasks. Those should be submitted to\nthe scheduler which will dispatch them on compute nodes.   The key principle of a shared computing environment is that resources are\nshared among users and must be scheduled. It is  mandatory  to schedule work\nby submitting jobs to the scheduler on Sherlock. And since login nodes are a\nshared resource, they must not be used to execute computing tasks.  Acceptable use of login nodes include:   lightweight file transfers,  script and configuration file editing,  job submission and monitoring.    Resource limits are enforced  To minimize disruption and ensure a confortable working environment for\nusers, resource limits are enforced on login nodes, and processes started\nthere will automatically be terminated if their resource usage (including\nCPU time, memory and run time) exceed those limits.", 
            "title": "Login nodes"
        }, 
        {
            "location": "/docs/user-guide/running-jobs/#slurm-commands", 
            "text": "Slurm allows requesting resources and submitting jobs in a variety of ways. The\nmain Slurm commands to submit jobs are listed in the table below:     Command  Description  Behavior      salloc  Request resources and allocates them to a job  Starts a new shell, but does not execute anything    srun  Request resources and runs a command on the allocated compute node(s)  Blocking command: will not return until the job ends    sbatch  Request resources and runs a script on the allocated compute node(s)  Asynchronous command: will return as soon as the job is submitted", 
            "title": "Slurm commands"
        }, 
        {
            "location": "/docs/user-guide/running-jobs/#interactive-jobs", 
            "text": "", 
            "title": "Interactive jobs"
        }, 
        {
            "location": "/docs/user-guide/running-jobs/#dedicated-nodes", 
            "text": "Interactive jobs allow users to log in to a compute node to run commands\ninteractively on the command line. They could be an integral\npart of an interactive programming and debugging workflow. The simplest way to\nestablish an interactive session on Sherlock is to use the  sdev  command:  $ sdev  This will open a login shell using one core and 4 GB of memory on one node for\none hour. The  sdev  sessions run on dedicated compute nodes. This ensures\nminimal wait times when you need to access a node for testing script, debug\ncode or any kind of interactive work.  sdev  also provides X11 forwarding via the submission host (typically the\nlogin node you're connected to) and can thus be used to run GUI applications.", 
            "title": "Dedicated nodes"
        }, 
        {
            "location": "/docs/user-guide/running-jobs/#compute-nodes", 
            "text": "If you need more resources 1 , you can pass options to  sdev , to\nrequest more CPU cores, more nodes, or even run in a different partition. sdev -h  will provide more information:  $ sdev -h\nsdev: start an interactive shell on a compute node\n\nUsage: sdev [OPTIONS]\n    Optional arguments:\n        -n      number of CPU cores to request (default: 1)\n        -N      number of nodes to request (default: 1)\n        -m      memory amount to request (default: 4GB)\n        -p      partition to run the job in (default: dev)\n        -t      time limit (default: 01:00:00, ie. 1 hour)\n        -r      allocate resources from the named reservation (default: none)\n        -J      job name (default: sdev)\n        -q      quality of service to request for the job (default: normal)  Another way to get an interactive session on a compute node is to use  srun  to\nexecute a shell through the scheduler. For instance, to start a  bash  session\non a compute node, with the default resource requirements (one core for 2\nhours), you can run:  $ srun --pty bash  The main advantage of this approach is that it will allow you to specify the\nwhole range of submission options that  sdev  may not support.  Finally, if you prefer to submit an existing job script or other executable as\nan interactive job, you can use the  salloc  command:  $ salloc script.sh  If you don't provide a command to execute,  salloc  will start a Slurm job and\nallocate resources for it, but it will not automatically connect you to the\nallocated node(s). It will only start a new shell on the same node you launched salloc  from, and set up the appropriate  $SLURM_*  environment\nvariables. So you will typically need to look at them to see\nwhat nodes have been assigned to your job. For instance:  $ salloc\nsalloc: Granted job allocation 655914\n$ echo $SLURM_NODELIST\nsh-101-55\n$ ssh sh-101-55\n[...]\nsh-101-55 ~ $", 
            "title": "Compute nodes"
        }, 
        {
            "location": "/docs/user-guide/running-jobs/#connecting-to-nodes", 
            "text": "Login to compute nodes  Users are not allowed to login to compute nodes unless they have a job\nrunning there.   If you SSH to a compute node without any active job allocation, you'll be\ngreeted by the following message:  $ ssh sh-101-01\nAccess denied by pam_slurm_adopt: you have no active jobs on this node\nConnection closed\n$  Once you have a job running on a node, you can SSH directly to it and\nrun additional processes 2 , or observe how you application\nbehaves, debug issues, and so on.  The  salloc  command supports the same parameters as  sbatch , and can override\nany default configuration. Note that any  #SBATCH  directive in your job script\nwill not be interpreted by  salloc  when it is executed in this way. You must\nspecify all arguments directly on the command line for them to be taken into\naccount.", 
            "title": "Connecting to nodes"
        }, 
        {
            "location": "/docs/user-guide/running-jobs/#batch-jobs", 
            "text": "Work in progress   This page is a work in progress and is not complete yet. We are actively\nworking on adding more content and information.", 
            "title": "Batch jobs"
        }, 
        {
            "location": "/docs/user-guide/running-jobs/#recurring-jobs", 
            "text": "Warning  Cron  tasks are not supported on Sherlock.   Users are not allowed to create  cron  jobs on Sherlock, for a variety of\nreasons:   resources limits cannot be easily enforced in  cron  jobs, meaning that a\n  single user can end up monopolizing all the resources of a login node,  no amount of resources can be guaranteed when executing a  cron  job, leading\n  to unreliable runtime and performance,  user  cron  jobs have the potential of bringing down whole\n  nodes by creating fork bombs, if they're not carefully crafted and tested,  compute and login nodes could be redeployed at any time, meaning that\n   cron  jobs scheduled there could go away without the user being notified,\n  and cause all sorts of unexpected results,  cron  jobs could be mistakenly scheduled on several nodes and run multiple\n  times, which could result in corrupted files.   As an alternative, if you need to run recurring tasks at regular intervals, we\nrecommend the following approach: by using the  --begin  job submission option,\nand creating a job that resubmits itself once it's done, you can virtually\nemulate the behavior and benefits of a  cron  job, without its disadvantages:\nyour task will be scheduled on a compute node, and use all of the resources it\nrequested, without being impacted by anything else.  Depending on your recurring job's specificities, where you submit it and the\nstate of the cluster at the time of execution, the starting time of that task\nmay not be guaranteed and result in a delay in execution, as it will be\nscheduled by Slurm like any other jobs. Typical recurring jobs, such as file\nsynchronization, database updates or backup tasks don't require strict starting\ntimes, though, so most users find this an acceptable trade-off.  The table below summarizes the advantages and inconvenients of each approach:   \n.yes { color: darkgreen; }\n.no  { color: darkred;   }      Cron tasks  Recurring jobs      Authorized on Sherlock      Dedicated resources for the task      Persistent across node redeployments      Unique, controlled execution      Potential execution delay", 
            "title": "Recurring jobs"
        }, 
        {
            "location": "/docs/user-guide/running-jobs/#example", 
            "text": "The script below presents an example of such a recurring, self-resubmitting\njob, that would emulate a  cron  task. It will append a timestamped line to a cron.log  file in your  $HOME  directory and run every 7 days.  #!/bin/bash\n#SBATCH --job-name=cron\n#SBATCH --begin=now+7days\n#SBATCH --dependency=singleton\n#SBATCH --time=00:02:00\n#SBATCH --mail-type=FAIL\n\n\n## Insert you the command to run below. Here, we're just storing the date in a\n## cron.log file\n\ndate -R   $HOME/cron.log\n\n## Resubmit the job for the next execution\nsbatch $0  If the job payload (here the  date  command) fails for some reason and\ngenerates and error, the job will not be resubmitted, and the user will be\nnotified by email.  We encourage users to get familiar with the submission options used in this\nscript by giving a look at the  sbatch   man page , but some\ndetails are given below:     Submission option or command  Explanation      --job-name=cron  makes it easy to identify the job, is used by the   --dependency=singleton  option to identify identical jobs, and will allow  cancelling the job by name (because its jobid will change each time it's  submitted)    --begin=now+7days  will instruct the scheduler to not even consider the job   for scheduling before 7 days after it's been submitted    --dependency=singleton  will make sure that only one  cron  job runs at any given time    --time=00:02:00  runtime limit for the job (here 2 minutes). You'll need to adjust the value   depending on the task you need to run (shorter runtime requests usually   result in the job running closer to the clock mark)    --mail-type=FAIL  will send an email notification to the user if the job ever fails    sbatch $0  will resubmit the job script by calling its own name ( $0 )   after successful execution     You can save the script as  cron.sbatch  or any other name, and submit it with:  $ sbatch cron.sbatch  It will start running for the first time 7 days after\nyou submit it, and it will continue to run until you cancel it with the\nfollowing command (using the job name, as defined by the  --job-name  option):  $ scancel -n cron      The dedicated partition that  sdev  uses by default only allows\n  up to 2 cores and 8 GB or memory per user at any given time. So if you need\n  more resources for your interactive session, you may have to specify a\n  different partition. See the  Partitions  section for more\n  details.    Please note that your SSH session will be attached to your\n  running job, and that resources used by that interactive shell will count\n  towards your job's resource limits. So if you start a process using large\n  amounts of memory via SSH while your job is running, you may hit the job's\n  memory limits, which will trigger its termination.", 
            "title": "Example"
        }, 
        {
            "location": "/docs/user-guide/gpu/", 
            "text": "To support the latest computing evolutions in many fields of science, Sherlock\nprovides GPU nodes that can be used to run a variety of GPU-accelerated\napplications. Those nodes are available to everyone, but are a scarce,\nhighly-demanded resource, so getting access to them may require some wait time\nin queue.\n\n\n\n\nGetting your own GPU nodes\n\n\nIf you need frequent access to GPU nodes, we recommend considering\n\nbecoming an owner on Sherlock\n, so you can have immediate\naccess to your GPU nodes when you need them.\n\n\n\n\nGPU nodes\n#\n\n\nA limited number of GPU nodes are available in the \ngpu\n partition. Anybody\nrunning on Sherlock can submit a job there. As owners contribute to expand\nSherlock, more GPU nodes are added to the \nowners\n partition, for use by PI\ngroups which purchased their own compute nodes.\n\n\nGPU types\n#\n\n\n\n\nMore GPU types to come\n\n\nAs we merge cluster nodes during Phase 2, the existing Sherlock 1.0 GPU\nnodes will be added to the \ngpu\n partition on Sherlock 2.0\n\n\n\n\nThere currently are two types of GPUs available in the \ngpu\n partition:\n\n\n\n\nNVIDIA Tesla P100-PCIe\n1\n, for applications requiring double-precision\n  (64-bit) and deep-learning training workloads,\n\n\nNVIDIA Tesla P40\n2\n, for single- or half-precision (32, 16-bit) workloads\n  and deep-learning inference jobs.\n\n\n\n\nSubmitting a GPU job\n#\n\n\nTo submit a GPU job, you'll need to use the \n--gres\n option in your batch script\nor command line submission options.\n\n\nFor instance, the following script will request one GPU for two hours in the\n\ngpu\n partition, and run the GPU-enabled version of \ngromacs\n:\n\n\n#!/bin/bash\n# SBATCH -p gpu\n# SBATCH -c 10\n# SBATCH --gres gpu:1\n\nml load gromacs/2016.3\n\nsrun gmx_gpu ...\n\n\n\nYou can also directly run GPU processes on compute nodes with \nsrun\n. For\ninstance, the following command will display details about the GPUs allocated\nto your job:\n\n\n$ srun -p gpu --gres gpu:2 nvidia-smi\nFri Jul 28 12:41:49 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 0000:03:00.0     Off |                    0 |\n| N/A   26C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla P40           On   | 0000:04:00.0     Off |                    0 |\n| N/A   24C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n\n\n\nGPU resources MUST be requested explicitly\n\n\nJobs will be rejected at submission time if they don't explictly request\nGPU resources.\n\n\n\n\nThe \ngpu\n partition only accepts jobs explicitly requesting GPU resources. If\nthey don't, they will be rejected with the following message:\n\n\n$ srun -p gpu --pty bash\nsrun: error: Unable to allocate resources: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n\n\n\nGPU types\n#\n\n\nSince Sherlock features many different types of GPUs, each with its own\ntechnical characteristics, performance profiles and specificities, you may want\nto ensure that your job runs on a specific type of GPU.\n\n\nTo that end, Slurm allows users to specify \nconstraints\n when submitting jobs,\nwhich will indicate the scheduler that only nodes having features matching the\njob constraints could be used to satisfy the request. Multiple constraints may\nbe specified and combined with various operators (please refer to the official\n\nSlurm documentation\n for details).\n\n\nThe list of available features on GPU nodes can be obtained with the\n\nnode_feat\n3\n command:\n\n\n$ node_feat -p gpu | grep GPU_\nGPU_BRD:TESLA\nGPU_GEN:PSC\nGPU_MEM:16GB\nGPU_MEM:24GB\nGPU_SKU:TESLA_P100_PCIE\nGPU_SKU:TESLA_P40\n\n\n\nnode_feat\n will only list the features of nodes from partitions you have\naccess to, so output may vary depending on your group membership.\n\n\nThe different characteristics\n4\n of various GPU types are listed in the following\ntable\n\n\n\n\n\n\n\n\nSlurm\nfeature\n\n\nDescription\n\n\nPossible values\n\n\nExample job constraint\n\n\n\n\n\n\n\n\n\n\nGPU_BRD\n\n\nGPU brand\n\n\nGEFORCE\n: GeForce / TITAN\nTESLA\n: Tesla\n\n\n#SBATCH -C GPU_BRD:TESLA\n\n\n\n\n\n\nGPU_GEN\n\n\nGPU generation\n\n\nPSC\n: Pascal\nMXW\n: Maxwell\n\n\n#SBATCH -C GPU_GEN:PSC\n\n\n\n\n\n\nGPU_MEM\n\n\nAmount of GPU memory\n\n\n16GB\n, \n24GB\n\n\n#SBATCH -C GPU_MEM:16GB\n\n\n\n\n\n\nGPU_SKU\n\n\nGPU model\n\n\nTESLA_P100_PCIE\nTESLA_P40\n\n\n#SBATCH -C GPU_SKU:TESLA_P40\n\n\n\n\n\n\n\n\nDepending on the partitions you have access to, more features may be available\nto be requested in your jobs.\n\n\nFor instance, to request a Tesla GPU for you job, you can use the following\nsubmisison options:\n\n\n$ srun -p owners --gres gpu:1 -C GPU_BRD:TESLA nvidia-smi -L\nGPU 0: Tesla P100-SXM2-16GB (UUID: GPU-4f91f58f-f3ea-d414-d4ce-faf587c5c4d4)\n\n\n\n\n\nUnsatisfiable contraints\n\n\nIf you specify a constraint that can't be satisfied in the partition you're\nsubmitting your job to, the job will be rejected by the scheduler.     For\ninstance, requesting a GeForce GPU in the \ngpu\n partition, which only\nfeatures Tesla GPUs, will result in an error:\n\n\n$ srun -p gpu --gres gpu:1 -C GPU_BRD:GEFORCE nvidia-smi -L\nsrun: error: Unable to allocate resources: Requested node configuration is not available\n\n\n\n\n\nGPU compute modes\n#\n\n\nBy default, GPUs on Sherlock are set in the \nExclusive Process\n compute\nmode\n5\n, to provide the best performance and an isolated environment\nfor jobs, out of the box.\n\n\nSome software may require GPUs to be set to a different compute mode, for\ninstance to share a GPU across different processes within the same application.\n\n\nTo handle that case, we developed a specific option, \n--gpu_cmode\n, that users\ncan add to their \nsrun\n and \nsbatch\n submission options, to choose the compute\nmode for the GPUs allocated to their job.\n\n\nHere's the list of the different compute modes supported on Sherlock's GPUs:\n\n\n\n\n\n\n\n\nGPU\ncompute\nmode\n\n\n--gpu_cmode\n option\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\"Default\"\n\n\nshared\n\n\nMultiple contexts are allowed per device (NVIDIA default)\n\n\n\n\n\n\n\"Exclusive Process\"\n\n\nexclusive\n\n\nOnly one context is allowed per device, usable from multiple threads at a time (Sherlock default)\n\n\n\n\n\n\n\"Prohibited\"\n\n\nprohibited\n\n\nNo CUDA context can be created on the device\n\n\n\n\n\n\n\n\nBy default, or if the \n--gpu_cmode\n option is not specified, GPUs will be set\nin the \"Exclusive Process\" mode, as demonstrated by this example command:\n\n\n$ srun -p gpu --gres gpu:1 nvidia-smi\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 00000000:03:00.0 Off |                    0 |\n| N/A   22C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n\n\n\nWith the \n--gpu_cmode\n option, the scheduler will set the GPU compute mode to\nthe desired value before execution:\n\n\n$ srun -p gpu --gres gpu:1 --gpu_cmode=shared nvidia-smi\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 00000000:03:00.0 Off |                    0 |\n| N/A   22C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n\n\n\n\nTip\n\n\n\"Default\" is the name that the NVIDIA System Management Interface\n(\nnvidia-smi\n) uses to describe the mode where a GPU can be shared between\ndifferent processes. It does not represent the default GPU compute mode on\nSherlock, which is \"Exclusive Process\".\n\n\n\n\nEnvironment and diagnostic tools\n#\n\n\n\n\n Work in progress \n\n\nThis page is a work in progress and is not complete yet. We are actively\nworking on adding more content and information.\n\n\n\n\n\n\n\n\n\n\n\n\nSee the complete \nTesla P100 technical specifications\n for\n  details.\n\n\n\n\n\n\nSee the complete \nTesla P40 technical specifications\n for\n  details.\n\n\n\n\n\n\nSee \nnode_feat -h\n for more details.\n\n\n\n\n\n\nThe lists of values provided in the table are non exhaustive.\n\n\n\n\n\n\nThe list of available GPU compute modes and relevant details are\n  available in the \nCUDA Toolkit Documentation", 
            "title": "GPU nodes"
        }, 
        {
            "location": "/docs/user-guide/gpu/#gpu-nodes", 
            "text": "A limited number of GPU nodes are available in the  gpu  partition. Anybody\nrunning on Sherlock can submit a job there. As owners contribute to expand\nSherlock, more GPU nodes are added to the  owners  partition, for use by PI\ngroups which purchased their own compute nodes.", 
            "title": "GPU nodes"
        }, 
        {
            "location": "/docs/user-guide/gpu/#gpu-types", 
            "text": "More GPU types to come  As we merge cluster nodes during Phase 2, the existing Sherlock 1.0 GPU\nnodes will be added to the  gpu  partition on Sherlock 2.0   There currently are two types of GPUs available in the  gpu  partition:   NVIDIA Tesla P100-PCIe 1 , for applications requiring double-precision\n  (64-bit) and deep-learning training workloads,  NVIDIA Tesla P40 2 , for single- or half-precision (32, 16-bit) workloads\n  and deep-learning inference jobs.", 
            "title": "GPU types"
        }, 
        {
            "location": "/docs/user-guide/gpu/#submitting-a-gpu-job", 
            "text": "To submit a GPU job, you'll need to use the  --gres  option in your batch script\nor command line submission options.  For instance, the following script will request one GPU for two hours in the gpu  partition, and run the GPU-enabled version of  gromacs :  #!/bin/bash\n# SBATCH -p gpu\n# SBATCH -c 10\n# SBATCH --gres gpu:1\n\nml load gromacs/2016.3\n\nsrun gmx_gpu ...  You can also directly run GPU processes on compute nodes with  srun . For\ninstance, the following command will display details about the GPUs allocated\nto your job:  $ srun -p gpu --gres gpu:2 nvidia-smi\nFri Jul 28 12:41:49 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 0000:03:00.0     Off |                    0 |\n| N/A   26C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla P40           On   | 0000:04:00.0     Off |                    0 |\n| N/A   24C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+   GPU resources MUST be requested explicitly  Jobs will be rejected at submission time if they don't explictly request\nGPU resources.   The  gpu  partition only accepts jobs explicitly requesting GPU resources. If\nthey don't, they will be rejected with the following message:  $ srun -p gpu --pty bash\nsrun: error: Unable to allocate resources: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)", 
            "title": "Submitting a GPU job"
        }, 
        {
            "location": "/docs/user-guide/gpu/#gpu-types_1", 
            "text": "Since Sherlock features many different types of GPUs, each with its own\ntechnical characteristics, performance profiles and specificities, you may want\nto ensure that your job runs on a specific type of GPU.  To that end, Slurm allows users to specify  constraints  when submitting jobs,\nwhich will indicate the scheduler that only nodes having features matching the\njob constraints could be used to satisfy the request. Multiple constraints may\nbe specified and combined with various operators (please refer to the official Slurm documentation  for details).  The list of available features on GPU nodes can be obtained with the node_feat 3  command:  $ node_feat -p gpu | grep GPU_\nGPU_BRD:TESLA\nGPU_GEN:PSC\nGPU_MEM:16GB\nGPU_MEM:24GB\nGPU_SKU:TESLA_P100_PCIE\nGPU_SKU:TESLA_P40  node_feat  will only list the features of nodes from partitions you have\naccess to, so output may vary depending on your group membership.  The different characteristics 4  of various GPU types are listed in the following\ntable     Slurm feature  Description  Possible values  Example job constraint      GPU_BRD  GPU brand  GEFORCE : GeForce / TITAN TESLA : Tesla  #SBATCH -C GPU_BRD:TESLA    GPU_GEN  GPU generation  PSC : Pascal MXW : Maxwell  #SBATCH -C GPU_GEN:PSC    GPU_MEM  Amount of GPU memory  16GB ,  24GB  #SBATCH -C GPU_MEM:16GB    GPU_SKU  GPU model  TESLA_P100_PCIE TESLA_P40  #SBATCH -C GPU_SKU:TESLA_P40     Depending on the partitions you have access to, more features may be available\nto be requested in your jobs.  For instance, to request a Tesla GPU for you job, you can use the following\nsubmisison options:  $ srun -p owners --gres gpu:1 -C GPU_BRD:TESLA nvidia-smi -L\nGPU 0: Tesla P100-SXM2-16GB (UUID: GPU-4f91f58f-f3ea-d414-d4ce-faf587c5c4d4)   Unsatisfiable contraints  If you specify a constraint that can't be satisfied in the partition you're\nsubmitting your job to, the job will be rejected by the scheduler.     For\ninstance, requesting a GeForce GPU in the  gpu  partition, which only\nfeatures Tesla GPUs, will result in an error:  $ srun -p gpu --gres gpu:1 -C GPU_BRD:GEFORCE nvidia-smi -L\nsrun: error: Unable to allocate resources: Requested node configuration is not available", 
            "title": "GPU types"
        }, 
        {
            "location": "/docs/user-guide/gpu/#gpu-compute-modes", 
            "text": "By default, GPUs on Sherlock are set in the  Exclusive Process  compute\nmode 5 , to provide the best performance and an isolated environment\nfor jobs, out of the box.  Some software may require GPUs to be set to a different compute mode, for\ninstance to share a GPU across different processes within the same application.  To handle that case, we developed a specific option,  --gpu_cmode , that users\ncan add to their  srun  and  sbatch  submission options, to choose the compute\nmode for the GPUs allocated to their job.  Here's the list of the different compute modes supported on Sherlock's GPUs:     GPU compute mode  --gpu_cmode  option  Description      \"Default\"  shared  Multiple contexts are allowed per device (NVIDIA default)    \"Exclusive Process\"  exclusive  Only one context is allowed per device, usable from multiple threads at a time (Sherlock default)    \"Prohibited\"  prohibited  No CUDA context can be created on the device     By default, or if the  --gpu_cmode  option is not specified, GPUs will be set\nin the \"Exclusive Process\" mode, as demonstrated by this example command:  $ srun -p gpu --gres gpu:1 nvidia-smi\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 00000000:03:00.0 Off |                    0 |\n| N/A   22C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+  With the  --gpu_cmode  option, the scheduler will set the GPU compute mode to\nthe desired value before execution:  $ srun -p gpu --gres gpu:1 --gpu_cmode=shared nvidia-smi\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 00000000:03:00.0 Off |                    0 |\n| N/A   22C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+   Tip  \"Default\" is the name that the NVIDIA System Management Interface\n( nvidia-smi ) uses to describe the mode where a GPU can be shared between\ndifferent processes. It does not represent the default GPU compute mode on\nSherlock, which is \"Exclusive Process\".", 
            "title": "GPU compute modes"
        }, 
        {
            "location": "/docs/user-guide/gpu/#environment-and-diagnostic-tools", 
            "text": "Work in progress   This page is a work in progress and is not complete yet. We are actively\nworking on adding more content and information.       See the complete  Tesla P100 technical specifications  for\n  details.    See the complete  Tesla P40 technical specifications  for\n  details.    See  node_feat -h  for more details.    The lists of values provided in the table are non exhaustive.    The list of available GPU compute modes and relevant details are\n  available in the  CUDA Toolkit Documentation", 
            "title": "Environment and diagnostic tools"
        }, 
        {
            "location": "/docs/user-guide/troubleshoot/", 
            "text": "Sherlock is a resource for ressearch, and as such, it is in perpetual\nevolution, as hardware, applications, libraries, and modules are added,\nupdated, and/or modified on a regular basis.  Sometimes issues can appear where\nnone existed before. When you find something missing or a behavior that seems\nodd, please \nlet us know\n.\n\n\nHow to submit a support request\n#\n\n\n\n\nGoogle it first!\n\n\nWhen encountering issues with software, if the misbehavior involves an\nerror message, the first step should always be to look up the error message\nonline.  There's a good chance somebody stumbled upon the same hurdles\nbefore, and may even provide some fix or workaround.\n\n\n\n\nIf you're facing issues you can't figure out, we're here to help. Feel free to\nemail us at \n, but please keep the following points in\nmind to ensure a timely and relevant response to your support requests.\n\n\n\n\nPlease provide relevant information\n\n\nWe need to understand the issue you're facing, and in most cases, \nwe need\nto be able to reproduce it\n, so it could be diagnosed and addressed.\nPlease make sure to provide enough information so we could help you in the\nbest possible way.\n\n\n\n\nThis typically involves providing the following information:\n\n\n\n\nyour SUNet ID,\n\n\nsome context about your problem (were you submitting a job, copying a file,\n  compiling an application?),\n\n\nif relevant, the full path to the files involved in your question or problem,\n\n\nthe name of node where you received the error (usually displayed in your\n  command-line prompt),\n\n\nthe command(s) you ran, and/or the job submission script(s) you used,\n\n\nthe relevant job ID(s),\n\n\nthe \nexact\n, \nentire\n error message (or trace) you received.\n\n\n\n\n\n\nError messages are critical\n\n\nThis is very important. Without proper error messages, there is nothing\nwe can do to help. And \"\nit doesn't work\n\" is not a proper error\nmessage.\n\n\n\n\nYou can avoid email back and forth where we ask for all the relevant details,\nand thus delay the problem resolution, by providing all this information\nfrom the start. This will help us get to your problem immediately.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/docs/user-guide/troubleshoot/#how-to-submit-a-support-request", 
            "text": "Google it first!  When encountering issues with software, if the misbehavior involves an\nerror message, the first step should always be to look up the error message\nonline.  There's a good chance somebody stumbled upon the same hurdles\nbefore, and may even provide some fix or workaround.   If you're facing issues you can't figure out, we're here to help. Feel free to\nemail us at  , but please keep the following points in\nmind to ensure a timely and relevant response to your support requests.   Please provide relevant information  We need to understand the issue you're facing, and in most cases,  we need\nto be able to reproduce it , so it could be diagnosed and addressed.\nPlease make sure to provide enough information so we could help you in the\nbest possible way.   This typically involves providing the following information:   your SUNet ID,  some context about your problem (were you submitting a job, copying a file,\n  compiling an application?),  if relevant, the full path to the files involved in your question or problem,  the name of node where you received the error (usually displayed in your\n  command-line prompt),  the command(s) you ran, and/or the job submission script(s) you used,  the relevant job ID(s),  the  exact ,  entire  error message (or trace) you received.    Error messages are critical  This is very important. Without proper error messages, there is nothing\nwe can do to help. And \" it doesn't work \" is not a proper error\nmessage.   You can avoid email back and forth where we ask for all the relevant details,\nand thus delay the problem resolution, by providing all this information\nfrom the start. This will help us get to your problem immediately.", 
            "title": "How to submit a support request"
        }, 
        {
            "location": "/docs/software/", 
            "text": "Available software\n#\n\n\nA set of supported software installations is provided for use on Sherlock. This\nsoftware is made available through a \nSoftware Modules\n system. For\nthe complete list of available software, please refer to the \nSoftware List\npage\n.\n\n\nLicensed software can be used on Sherlock, under certain conditions. Feel free\nto \ncontact us\n for more details or if you have questions. For more\ninformation about purchasing software licenses, you can contact the \nStanford\nSoftware Licensing office\n.\n\n\nInstallation requests\n#\n\n\n\n\nInstallation requests\n\n\nThe SRCC team installs, for general use, a set of libraries, tools and\nsoftware applications that are commonly used across many research groups.\nHowever, our staff resources are quite limited and don't allow us to build\nnor maintain custom software applications that may be requested by or be of\nuse to a small number of users.\n\n\n\n\nWe strongly encourage users to build custom and field- or domain-specific\nsoftware themselves, and install it in their own \npersonal or group shared\ndirectories\n. That way, they can share the software installations\nwith the rest of the users in their group, if necessary.\n\n\nUsers may even maintain and publish their own \nlocal module\nfiles\n to dynamically configure a running environment\nto use the software. They could share those modules with other users to\nsimplify the use of their own custom software installations.\n\n\n\n\nInstalling your own software\n\n\nFor more information about building your own software on Sherlock, please\nsee the \nSoftware Installation page\n.\n\n\n\n\nIf the software you need is not in the \nlist of available software\n, and\nyou have trouble installing it on your own, please \ncontact us\n\nwith \ncomplete details about the package\n, and we will try to help you\ninstall it.\n\n\nIf it's a widely used software that could benefit multiple users across\ndifferent scientific communities, we will consider install it globally as\nresources permit\n1\n.\n\n\n\n\n\n\n\n\n\n\nSoftware requests, including version upgrades, are fulfilled in\n  the order they are received, and as time permits. We don't have any dedicated\n  team for software installations, and requests are handled along with other\n  duties, typically within two to three weeks of being received.", 
            "title": "Overview"
        }, 
        {
            "location": "/docs/software/#available-software", 
            "text": "A set of supported software installations is provided for use on Sherlock. This\nsoftware is made available through a  Software Modules  system. For\nthe complete list of available software, please refer to the  Software List\npage .  Licensed software can be used on Sherlock, under certain conditions. Feel free\nto  contact us  for more details or if you have questions. For more\ninformation about purchasing software licenses, you can contact the  Stanford\nSoftware Licensing office .", 
            "title": "Available software"
        }, 
        {
            "location": "/docs/software/#installation-requests", 
            "text": "Installation requests  The SRCC team installs, for general use, a set of libraries, tools and\nsoftware applications that are commonly used across many research groups.\nHowever, our staff resources are quite limited and don't allow us to build\nnor maintain custom software applications that may be requested by or be of\nuse to a small number of users.   We strongly encourage users to build custom and field- or domain-specific\nsoftware themselves, and install it in their own  personal or group shared\ndirectories . That way, they can share the software installations\nwith the rest of the users in their group, if necessary.  Users may even maintain and publish their own  local module\nfiles  to dynamically configure a running environment\nto use the software. They could share those modules with other users to\nsimplify the use of their own custom software installations.   Installing your own software  For more information about building your own software on Sherlock, please\nsee the  Software Installation page .   If the software you need is not in the  list of available software , and\nyou have trouble installing it on your own, please  contact us \nwith  complete details about the package , and we will try to help you\ninstall it.  If it's a widely used software that could benefit multiple users across\ndifferent scientific communities, we will consider install it globally as\nresources permit 1 .      Software requests, including version upgrades, are fulfilled in\n  the order they are received, and as time permits. We don't have any dedicated\n  team for software installations, and requests are handled along with other\n  duties, typically within two to three weeks of being received.", 
            "title": "Installation requests"
        }, 
        {
            "location": "/docs/software/list/", 
            "text": "Software list\n#\n\n\nThe full list of software centrally installed and managed on Sherlock is in the\ntables below.\n\n\n\n\nWork in progress\n\n\nSoftware installations on Sherlock are an ever ongoing process. We're\ncontinuously adding new software to the list. If you're looking for\nsomething that is not in the list, please take a look\n\nhere\n for options.\n\n\n\n\nSoftware modules on Sherlock are organized in \ncategories\n, by scientific\nfield. It means that you will have to first load a category module before\ngetting access to individual modules.  The \nmath\n and \ndevel\n categories are\nloaded by default. See the \nModules page\n for further details and\nexamples.\n\n\nCategories\n#\n\n\nAs of Wednesday, April 18 2018, we provide 331 software packages, in 7 categories, covering 45 fields of science:\n\n\n\n\nbiology\n \ncomputational biology, cryo-em, genomics, neurology, phylogenetics\n\n\nchemistry\n \ncomputational chemistry, molecular dynamics, quantum chemistry\n\n\ndevel\n \nbuild, compiler, data, language, libs, mpi, networking, parser\n\n\nmath\n \ncomputational geometry, deep learning, linear algebra, machine learning, numerical analysis, numerical library, optimization, scientific computing, statistics, symbolic\n\n\nphysics\n \nastronomy, geophysics, geoscience, photonics\n\n\nsystem\n \nbecnhmark, compression, containers, database, file management, file transfer, language, libs, resource monitoring, scm, tools\n\n\nviz\n \ngraphs, molecular visualization, plotting, remote display\n\n\n\n\n\n\nLicensed software\n\n\nAccess to software modules marked with \n in the tables\nbelow is restricted to properly licensed user groups.\n\n\nThe SRCC is not funded to provide commercial software on Sherlock and\nresearchers are responsible for the costs of purchasing and renewing\ncommercial software licenses. For more information, please feel free to\n\ncontact us\n and see the \nStanford\nSoftware Licensing page\n for\npurchasing information.\n\n\n\n\n\n\n\n\n.lic  { color: darkred; }\n.lic:after {\n    content: attr(class);\n}\n\n\n\n\nBiology\n#\n\n\n\n\n\n\n\n\nField\n\n\nModule\nname\n\n\nVersion(s)\n\n\nURL\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncomputational biology\n\n\nimp\n\n\n-\n2.8.0\n\n\nWebsite\n\n\nIMP's broad goal is to contribute to a comprehensive structural characterization of biomolecules ranging in size and complexity from small peptides to large macromolecular assemblies, by integrating data from diverse biochemical and biophysical experiments.\n\n\n\n\n\n\ncomputational biology\n\n\npy-biopython\n\n\n-\n1.70_py27\n\n\nWebsite\n\n\nBiopython is a set of freely available tools for biological computation written in Python.\n\n\n\n\n\n\ncomputational biology\n\n\nrosetta\n\n\n-\n3.8\n\n\nWebsite\n\n\nRosetta is the premier software suite for modeling macromolecular structures. As a flexible, multi-purpose application, it includes tools for structure prediction, design, and remodeling of proteins and nucleic acids.\n\n\n\n\n\n\ncryo-em\n\n\neman2\n\n\n-\n2.2\n\n\nWebsite\n\n\nEMAN2 is a broadly based greyscale scientific image processing suite with a primary focus on processing data from transmission electron microscopes.\n\n\n\n\n\n\ncryo-em\n\n\nrelion\n\n\n-\n2.0.3\n-\n2.1\n\n\nWebsite\n\n\nRELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a stand-alone computer program that employs an empirical Bayesian approach to refinement of (multiple) 3D reconstructions or 2D class averages in electron cryo-microscopy (cryo-EM).\n\n\n\n\n\n\ngenomics\n\n\nangsd\n\n\n-\n0.919\n\n\nWebsite\n\n\nANGSD is a software for analyzing next generation sequencing data.\n\n\n\n\n\n\ngenomics\n\n\nbcftools\n\n\n-\n1.6\n\n\nWebsite\n\n\nBCFtools is a program for variant calling and manipulating files in the Variant Call Format (VCF) and its binary counterpart BCF.\n\n\n\n\n\n\ngenomics\n\n\nbedtools\n\n\n-\n2.27.1\n\n\nWebsite\n\n\nThe bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks.\n\n\n\n\n\n\ngenomics\n\n\nbowtie\n\n\n-\n1.2.2\n\n\nWebsite\n\n\nBowtie is an ultrafast, memory-efficient short read aligner.\n\n\n\n\n\n\ngenomics\n\n\nbowtie2\n\n\n-\n2.3.4.1\n\n\nWebsite\n\n\nBowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences.\n\n\n\n\n\n\ngenomics\n\n\nbwa\n\n\n-\n0.7.17\n\n\nWebsite\n\n\nBWA (Burrows-Wheeler Aligner) is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome.\n\n\n\n\n\n\ngenomics\n\n\ncufflinks\n\n\n-\n2.2.1\n\n\nWebsite\n\n\nCufflinks assembles transcripts, estimates their abundances, and tests for differential expression and regulation in RNA-Seq samples.\n\n\n\n\n\n\ngenomics\n\n\nfastx_toolkit\n\n\n-\n0.0.14\n\n\nWebsite\n\n\nThe FASTX-Toolkit is a collection of command line tools for Short-Reads FASTA/FASTQ files preprocessing.\n\n\n\n\n\n\ngenomics\n\n\nhisat2\n\n\n-\n2.1.0\n\n\nWebsite\n\n\nHISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes (as well as to a single reference genome).\n\n\n\n\n\n\ngenomics\n\n\nhtslib\n\n\n-\n1.6\n\n\nWebsite\n\n\nC library for high-throughput sequencing data formats.\n\n\n\n\n\n\ngenomics\n\n\nncbi-blast+\n\n\n-\n2.6.0\n\n\nWebsite\n\n\nNCBI BLAST+ is a suite of command-line tools to run BLAST (Basic Local Alignment Search Tool), an algorithm for comparing primary biological sequence information.\n\n\n\n\n\n\ngenomics\n\n\nplink\n\n\n-\n1.07\n-\n1.90b5.3\n-\n2.0a1\n\n\nWebsite\n\n\nPLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner.\n\n\n\n\n\n\ngenomics\n\n\npy-bx-python\n\n\n-\n0.8.1_py27\n\n\nWebsite\n\n\nTools for manipulating biological data, particularly multiple sequence alignments.\n\n\n\n\n\n\ngenomics\n\n\npy-macs2\n\n\n-\n2.1.1_py27\n\n\nWebsite\n\n\nMACS (Model-based Analysis of ChIP-Seq) implements a novel ChIP-Seq analysis method.\n\n\n\n\n\n\ngenomics\n\n\nsamtools\n\n\n-\n1.6\n\n\nWebsite\n\n\nTools (written in C using htslib) for manipulating next-generation sequencing data.\n\n\n\n\n\n\ngenomics\n\n\nstar\n\n\n-\n2.5.4b\n\n\nWebsite\n\n\nSTAR: ultrafast universal RNA-seq aligner.\n\n\n\n\n\n\ngenomics\n\n\ntophat\n\n\n-\n2.1.1\n\n\nWebsite\n\n\nTopHat is a fast splice junction mapper for RNA-Seq reads.\n\n\n\n\n\n\ngenomics\n\n\nvcftools\n\n\n-\n0.1.15\n\n\nWebsite\n\n\nVCFtools is a program package designed for working with VCF files, such as those generated by the 1000 Genomes Project.\n\n\n\n\n\n\nneurology\n\n\nafni\n\n\n-\n17.2.07\n-\n18.0.09\n\n\nWebsite\n\n\nAFNI (Analysis of Functional NeuroImages) is a set of C programs for processing, analyzing, and displaying functional MRI (FMRI) data - a technique for mapping human brain activity.\n\n\n\n\n\n\nneurology\n\n\nants\n\n\n-\n2.1.0\n\n\nWebsite\n\n\nANTs computes high-dimensional mappings to capture the statistics of brain structure and function.\n\n\n\n\n\n\nneurology\n\n\ndcm2niix\n\n\n-\n1.0.20171215\n\n\nWebsite\n\n\ndcm2niix is a program esigned to convert neuroimaging data from the DICOM format to the NIfTI format.\n\n\n\n\n\n\nneurology\n\n\nfreesurfer\n\n\n-\n6.0.0\n\n\nWebsite\n\n\nAn open source software suite for processing and analyzing (human) brain MRI images.\n\n\n\n\n\n\nneurology\n\n\nfsl\n\n\n-\n5.0.10\n\n\nWebsite\n\n\nFSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data.\n\n\n\n\n\n\nneurology\n\n\nmricron\n\n\n-\n20160502\n\n\nWebsite\n\n\nMRIcron is a cross-platform NIfTI format image viewer.\n\n\n\n\n\n\nneurology\n\n\nmrtrix\n\n\n-\n0.3.16\n\n\nWebsite\n\n\nMRtrix3 provides a set of tools to perform various types of diffusion MRI analyses, from various forms of tractography through to next-generation group-level analyses.\n\n\n\n\n\n\nneurology\n\n\npy-mdt\n\n\n-\n0.10.9_py36\n\n\nWebsite\n\n\nThe Maastricht Diffusion Toolbox, MDT, is a framework and library for parallelized (GPU and multi-core CPU) diffusion Magnetic Resonance Imaging (MRI) modeling.\n\n\n\n\n\n\nneurology\n\n\nspm\n\n\n-\n12\n\n\nWebsite\n\n\nThe SPM software package has been designed for the analysis of brain imaging data sequences. The sequences can be a series of images from different cohorts, or time-series from the same subject.\n\n\n\n\n\n\nphylogenetics\n\n\npy-ete\n\n\n-\n3.0.0_py27\n\n\nWebsite\n\n\nA Python framework for the analysis and visualization of trees.\n\n\n\n\n\n\n\n\nChemistry\n#\n\n\n\n\n\n\n\n\nField\n\n\nModule\nname\n\n\nVersion(s)\n\n\nURL\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncomputational chemistry\n\n\ngaussian\n\n\n-\ng16\n\n\nWebsite\n\n\nGaussian is a general purpose computational chemistry software package.\n\n\n\n\n\n\ncomputational chemistry\n\n\nlibint\n\n\n-\n1.1.4\n-\n2.0.3\n\n\nWebsite\n\n\nLibint computes molecular integrals.\n\n\n\n\n\n\ncomputational chemistry\n\n\nlibxc\n\n\n-\n3.0.0\n\n\nWebsite\n\n\nLibxc is a library of exchange-correlation functionals for density-functional theory.\n\n\n\n\n\n\ncomputational chemistry\n\n\nnwchem\n\n\n-\n6.8\n\n\nWebsite\n\n\nNWChem is an ab initio computational chemistry software package which also includes quantum chemical and molecular dynamics functionality.\n\n\n\n\n\n\ncomputational chemistry\n\n\npy-ase\n\n\n-\n3.14.1_py27\n\n\nWebsite\n\n\nThe Atomic Simulation Environment (ASE) is a set of tools and Python modules for setting up, manipulating, running, visualizing and analyzing atomistic simulations.\n\n\n\n\n\n\ncomputational chemistry\n\n\nschrodinger\n\n\n-\n2017-3\n\n\nWebsite\n\n\nSchr\u00f6dinger Suites (Small-molecule Drug Discovery Suite, Material Science Suite, Biologics Suite) provide a set of molecular modelling software.\n\n\n\n\n\n\ncomputational chemistry\n\n\nvasp\n\n\n-\n5.4.1\n\n\nWebsite\n\n\nThe Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.\n\n\n\n\n\n\nmolecular dynamics\n\n\ngromacs\n\n\n-\n2016.3\n-\n2018\n\n\nWebsite\n\n\nGROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.\n\n\n\n\n\n\nmolecular dynamics\n\n\nlammps\n\n\n-\n20180316\n\n\nWebsite\n\n\nLAMMPS is a classical molecular dynamics code that models an ensemble of particles in a liquid, solid, or gaseous state.\n\n\n\n\n\n\nmolecular dynamics\n\n\nopenmm\n\n\n-\n7.1.1\n\n\nWebsite\n\n\nA high performance toolkit for molecular simulation.\n\n\n\n\n\n\nmolecular dynamics\n\n\nplumed\n\n\n-\n2.3.2\n\n\nWebsite\n\n\nPLUMED is an open source library for free energy calculations in molecular systems.\n\n\n\n\n\n\nmolecular dynamics\n\n\npy-raspa2\n\n\n-\n2.0.3_py27\n\n\nWebsite\n\n\nRASPA2 is a general purpose classical simulation package that can be used for the simulation of molecules in gases, fluids, zeolites, aluminosilicates, metal-organic frameworks, carbon nanotubes and external fields.\n\n\n\n\n\n\nmolecular dynamics\n\n\nquip\n\n\n-\n20170901\n\n\nWebsite\n\n\nThe QUIP package is a collection of software tools to carry out molecular dynamics simulations.\n\n\n\n\n\n\nquantum chemistry\n\n\ncp2k\n\n\n-\n4.1\n\n\nWebsite\n\n\nCP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems.\n\n\n\n\n\n\n\n\nDevel\n#\n\n\n\n\n\n\n\n\nField\n\n\nModule\nname\n\n\nVersion(s)\n\n\nURL\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbuild\n\n\ncmake\n\n\n-\n3.8.1\n\n\nWebsite\n\n\nCMake is an extensible, open-source system that manages the build process in an operating system and in a compiler-independent manner.\n\n\n\n\n\n\nbuild\n\n\nscons\n\n\n-\n2.5.1_py36\n-\n2.5.1_py27\n\n\nWebsite\n\n\nSCons is an Open Source software construction tool.\n\n\n\n\n\n\ncompiler\n\n\ngcc\n\n\n-\n7.1.0\n-\n7.3.0\n-\n6.3.0\n\n\nWebsite\n\n\nThe GNU Compiler Collection includes front ends for C, C++, Fortran, Java, and Go, as well as libraries for these languages (libstdc++, libgcj,...).\n\n\n\n\n\n\ncompiler\n\n\nicc\n\n\n-\n2017.u2\n-\n2018.u1\n-\n2018\n\n\nWebsite\n\n\nIntel C++ Compiler, also known as icc or icl, is a group of C and C++ compilers from Intel\n\n\n\n\n\n\ncompiler\n\n\nifort\n\n\n-\n2017.u2\n-\n2018.u1\n-\n2018\n\n\nWebsite\n\n\nIntel Fortran Compiler, also known as ifort, is a group of Fortran compilers from Intel\n\n\n\n\n\n\ncompiler\n\n\nllvm\n\n\n-\n3.8.1\n-\n4.0.0\n-\n5.0.0\n\n\nWebsite\n\n\nThe LLVM Project is a collection of modular and reusable compiler and toolchain technologies. Clang is an LLVM native C/C++/Objective-C compiler,\n\n\n\n\n\n\ncompiler\n\n\nnagfor\n\n\n-\nnpl6a61na\n\n\nWebsite\n\n\nThe NAG Fortran Compiler is a full standard implementation of the ISO Fortran 95 language with the addition of all of Fortran 2003, most of Fortran 2008 and OpenMP 3.0 and 3.1.\n\n\n\n\n\n\ncompiler\n\n\npgi\n\n\n-\n17.4\n\n\nWebsite\n\n\nPGI compilers and tools, including Open MPI (Community Edition).\n\n\n\n\n\n\ncompiler\n\n\nsmlnj\n\n\n-\n110.81\n\n\nWebsite\n\n\nStandard ML of New Jersey (abbreviated SML/NJ) is a compiler for the Standard ML '97 programming language.\n\n\n\n\n\n\ndata\n\n\nh5utils\n\n\n-\n1.12.1\n\n\nWebsite\n\n\nh5utils is a set of utilities for visualization and conversion of scientific data in the free, portable HDF5 format.\n\n\n\n\n\n\ndata\n\n\nhdf5\n\n\n-\n1.10.0p1\n\n\nWebsite\n\n\nHDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data.\n\n\n\n\n\n\ndata\n\n\nhiredis\n\n\n-\n0.13.3\n\n\nWebsite\n\n\nHiredis is a minimalistic C client library for the Redis database.\n\n\n\n\n\n\ndata\n\n\nncl\n\n\n-\n6.4.0\n\n\nWebsite\n\n\nNCL is a free interpreted language designed specifically for scientific data processing and visualization.\n\n\n\n\n\n\ndata\n\n\nnetcdf\n\n\n-\n4.4.1.1\n\n\nWebsite\n\n\nNetCDF is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.\n\n\n\n\n\n\ndata\n\n\npnetcdf\n\n\n-\n1.8.1\n\n\nWebsite\n\n\nParallel netCDF (PnetCDF) is a parallel I/O library for accessing NetCDF files in CDF-1, 2, and 5 formats.\n\n\n\n\n\n\ndata\n\n\nprotobuf\n\n\n-\n3.4.0\n\n\nWebsite\n\n\nProtocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data.\n\n\n\n\n\n\ndata\n\n\nredis\n\n\n-\n4.0.1\n\n\nWebsite\n\n\nRedis is an open source, in-memory data structure store, used as a database, cache and message broker.\n\n\n\n\n\n\nlanguage\n\n\ncuda\n\n\n-\n8.0.61\n-\n9.0.176\n-\n9.1.85\n\n\nWebsite\n\n\nCUDA is a parallel computing platform and application programming interface (API) model created by Nvidia. It allows software developers and software engineers to use a CUDA-enabled graphics processing unit (GPU) for general purpose processing.\n\n\n\n\n\n\nlanguage\n\n\ngo\n\n\n-\n1.9\n\n\nWebsite\n\n\nGo is an open source programming language that makes it easy to build simple, reliable, and efficient software.\n\n\n\n\n\n\nlanguage\n\n\nguile\n\n\n-\n2.2.2\n\n\nWebsite\n\n\nGNU Guile is the preferred extension system for the GNU Project, which features an implementation of the Scheme programming language.\n\n\n\n\n\n\nlanguage\n\n\njava\n\n\n-\n1.8.0_131\n\n\nWebsite\n\n\nJava is a general-purpose computer programming language that is concurrent, class-based, object-oriented,[14] and specifically designed to have as few implementation dependencies as possible.\n\n\n\n\n\n\nlanguage\n\n\njulia\n\n\n-\n0.5.1\n-\n0.6\n\n\nWebsite\n\n\nJulia is a high-level, high-performance dynamic programming language for numerical computing.\n\n\n\n\n\n\nlanguage\n\n\nlua\n\n\n-\n5.3.4\n\n\nWebsite\n\n\nLua is a powerful, efficient, lightweight, embeddable scripting language. It supports procedural programming, object-oriented programming, functional programming, data-driven programming, and data description.\n\n\n\n\n\n\nlanguage\n\n\nluarocks\n\n\n-\n2.4.3\n\n\nWebsite\n\n\nLuaRocks is the package manager for Lua modules.\n\n\n\n\n\n\nlanguage\n\n\nmanticore\n\n\n-\n20180301\n\n\nWebsite\n\n\nManticore is a high-level parallel programming language aimed at general-purpose applications running on multi-core processors.\n\n\n\n\n\n\nlanguage\n\n\nnodejs\n\n\n-\n8.9.4\n-\n9.5.0\n\n\nWebsite\n\n\nNode.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. It provides the npm package manager.\n\n\n\n\n\n\nlanguage\n\n\nperl\n\n\n-\n5.26.0\n\n\nWebsite\n\n\nPerl 5 is a highly capable, feature-rich programming language with over 29 years of development. \nUsage on Sherlock\n\n\n\n\n\n\nlanguage\n\n\npy-cython\n\n\n-\n0.27.3_py36\n-\n0.27.3_py27\n\n\nWebsite\n\n\nCython is an optimising static compiler for both the Python programming language and the extended Cython programming language (based on Pyrex).\n\n\n\n\n\n\nlanguage\n\n\npy-ipython\n\n\n-\n6.1.0_py36\n-\n5.4.1_py27\n\n\nWebsite\n\n\nIPython is a command shell for interactive computing in multiple programming languages, originally developed for the Python programming language.\n\n\n\n\n\n\nlanguage\n\n\npy-jupyter\n\n\n-\n1.0.0_py36\n-\n1.0.0_py27\n\n\nWebsite\n\n\nJupyter is a browser-based interactive notebook for programming, mathematics, and data science. It supports a number of languages via plugins.\n\n\n\n\n\n\nlanguage\n\n\npython\n\n\n-\n3.6.1\n-\n2.7.13\n\n\nWebsite\n\n\nPython is an interpreted, interactive, object-oriented programming language.\n\n\n\n\n\n\nlanguage\n\n\nruby\n\n\n-\n2.4.1\n\n\nWebsite\n\n\nA dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write.\n\n\n\n\n\n\nlibs\n\n\nant\n\n\n-\n1.10.1\n\n\nWebsite\n\n\nApache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other.\n\n\n\n\n\n\nlibs\n\n\nboost\n\n\n-\n1.64.0\n\n\nWebsite\n\n\nBoost is a set of libraries for the C++ programming language that provide support for tasks and structures such as linear algebra, pseudorandom number generation, multithreading, image processing, regular expressions, and unit testing.\n\n\n\n\n\n\nlibs\n\n\ncnmem\n\n\n-\n1.0.0\n\n\nWebsite\n\n\nCNMeM is a simple library to help the Deep Learning frameworks manage CUDA memory.\n\n\n\n\n\n\nlibs\n\n\ncub\n\n\n-\n1.7.3\n\n\nWebsite\n\n\nCUB is a flexible library of cooperative threadblock primitives and other utilities for CUDA kernel programming.\n\n\n\n\n\n\nlibs\n\n\ncutlass\n\n\n-\n0.1.0\n\n\nWebsite\n\n\nCUTLASS is a collection of CUDA C++ template abstractions for implementing high-performance matrix-multiplication (GEMM) at all levels and scales within CUDA.\n\n\n\n\n\n\nlibs\n\n\neigen\n\n\n-\n3.3.3\n\n\nWebsite\n\n\nEigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms.\n\n\n\n\n\n\nlibs\n\n\nlibctl\n\n\n-\n3.2.2\n\n\nWebsite\n\n\nlibctl is a library for supporting flexible control files in scientific simulations.\n\n\n\n\n\n\nlibs\n\n\nlibgpuarray\n\n\n-\n0.7.5\n\n\nWebsite\n\n\nLibrary to manipulate tensors on the GPU.\n\n\n\n\n\n\nlibs\n\n\nnccl\n\n\n-\n1.3.4\n-\n2.0.4\n-\n2.1.15\n\n\nWebsite\n\n\nNCCL (pronounced 'Nickel') is a stand-alone library of standard collective communication routines, such as all-gather, reduce, broadcast, etc., that have been optimized to achieve high bandwidth over PCIe.\n\n\n\n\n\n\nlibs\n\n\nopencv\n\n\n-\n3.3.0\n\n\nWebsite\n\n\nOpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library.\n\n\n\n\n\n\nlibs\n\n\npy-h5py\n\n\n-\n2.7.1_py27\n\n\nWebsite\n\n\nThe h5py package is a Pythonic interface to the HDF5 binary data format.\n\n\n\n\n\n\nlibs\n\n\npy-netcdf4\n\n\n-\n1.3.1_py36\n-\n1.3.1_py27\n\n\nWebsite\n\n\nnetcdf4-python is a Python interface to the netCDF C library.\n\n\n\n\n\n\nlibs\n\n\npy-numba\n\n\n-\n0.35.0_py36\n-\n0.35.0_py27\n\n\nWebsite\n\n\nNumba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python..\n\n\n\n\n\n\nlibs\n\n\npy-pycuda\n\n\n-\n2017.1.1_py27\n\n\nWebsite\n\n\nPyCUDA lets you access Nvidia\u2018s CUDA parallel computation API from Python.\n\n\n\n\n\n\nlibs\n\n\npy-scikit-image\n\n\n-\n0.13.0_py27\n\n\nWebsite\n\n\nscikit-image is a collection of algorithms for image processing.\n\n\n\n\n\n\nlibs\n\n\nswig\n\n\n-\n3.0.12\n\n\nWebsite\n\n\nSWIG is an interface compiler that connects programs written in C and C++ with scripting languages such as Perl, Python, Ruby, and Tcl.\n\n\n\n\n\n\nlibs\n\n\ntbb\n\n\n-\n2017.u2\n-\n2018.u1\n-\n2018\n\n\nWebsite\n\n\nIntel\u00ae Threading Building Blocks (Intel\u00ae TBB) is a widely used C++ library for shared-memory parallel programming and heterogeneous computing (intra-node distributed memory programming).\n\n\n\n\n\n\nlibs\n\n\nzeromq\n\n\n-\n4.2.2\n\n\nWebsite\n\n\nZeroMQ (also spelled \u00d8MQ, 0MQ or ZMQ) is a high-performance asynchronous messaging library, aimed at use in distributed or concurrent applications.\n\n\n\n\n\n\nmpi\n\n\nimpi\n\n\n-\n2017.u2\n-\n2018.u1\n-\n2018\n\n\nWebsite\n\n\nIntel\u00ae MPI Library is a multi-fabric message passing library that implements the Message Passing Interface, version 3.1 (MPI-3.1) specification.\n\n\n\n\n\n\nmpi\n\n\nopenmpi\n\n\n-\n2.0.2\n-\n3.0.1\n-\n2.1.1\n\n\nWebsite\n\n\nThe Open MPI Project is an open source Message Passing Interface implementation that is developed and maintained by a consortium of academic, research, and industry partners.\n\n\n\n\n\n\nnetworking\n\n\ngasnet\n\n\n-\n1.30.0\n\n\nWebsite\n\n\nGASNet is a language-independent, low-level networking layer that provides network-independent, high-performance communication primitives tailored for implementing parallel global address space SPMD languages and libraries.\n\n\n\n\n\n\nnetworking\n\n\nlibfabric\n\n\n-\n1.6.0\n\n\nWebsite\n\n\nThe Open Fabrics Interfaces (OFI) is a framework focused on exporting fabric communication services to applications. Libfabric is the library that defines and exports the user-space API of OFI.\n\n\n\n\n\n\nnetworking\n\n\nucx\n\n\n-\n1.2.1\n\n\nWebsite\n\n\nUCX is a communication library implementing high-performance messaging for MPI/PGAS frameworks.\n\n\n\n\n\n\nparser\n\n\nxerces-c\n\n\n-\n3.2.1\n\n\nWebsite\n\n\nXerces-C++ is a validating XML parser written in a portable subset of C++.\n\n\n\n\n\n\n\n\nMath\n#\n\n\n\n\n\n\n\n\nField\n\n\nModule\nname\n\n\nVersion(s)\n\n\nURL\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncomputational geometry\n\n\ncgal\n\n\n-\n4.10\n\n\nWebsite\n\n\nThe Computational Geometry Algorithms Library (CGAL) is a C++ library that aims to provide easy access to efficient and reliable algorithms in computational geometry.\n\n\n\n\n\n\ncomputational geometry\n\n\nqhull\n\n\n-\n2015.2\n\n\nWebsite\n\n\nQhull computes the convex hull, Delaunay triangulation, Voronoi diagram, halfspace intersection about a point, furthest-site Delaunay triangulation, and furthest-site Voronoi diagram.\n\n\n\n\n\n\ndeep learning\n\n\ncaffe2\n\n\n-\n0.8.1\n\n\nWebsite\n\n\nCaffe2 is a deep learning framework that provides an easy and straightforward way to experiment with deep learning and leverage community contributions of new models and algorithms.\n\n\n\n\n\n\ndeep learning\n\n\ncudnn\n\n\n-\n5.1\n-\n6.0\n-\n7.0.1\n-\n7.0.4\n-\n7.0.5\n-\n7.1.1\n\n\nWebsite\n\n\nNVIDIA cuDNN is a GPU-accelerated library of primitives for deep neural networks.\n\n\n\n\n\n\ndeep learning\n\n\npy-horovod\n\n\n-\n0.12.1_py36\n-\n0.12.1_py27\n\n\nWebsite\n\n\nHorovod is a distributed training framework for TensorFlow. The goal of Horovod is to make distributed Deep Learning fast and easy to use.\n\n\n\n\n\n\ndeep learning\n\n\npy-keras\n\n\n-\n2.0.8_py27\n-\n2.1.5_py36\n-\n2.1.5_py27\n\n\nWebsite\n\n\nKeras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.\n\n\n\n\n\n\ndeep learning\n\n\npy-onnx\n\n\n-\n1.0.1_py27\n\n\nWebsite\n\n\nONNX is a open format to represent deep learning models.\n\n\n\n\n\n\ndeep learning\n\n\npy-pytorch\n\n\n-\n0.2.0_py27\n-\n0.2.0_py36\n-\n0.3.0_py36\n-\n0.3.0_py27\n\n\nWebsite\n\n\nPyTorch is a deep learning framework that puts Python first.\n\n\n\n\n\n\ndeep learning\n\n\npy-tensorflow\n\n\n-\n1.4.0_py27\n-\n1.5.0_py27\n-\n1.5.0_py36\n-\n1.6.0_py36\n-\n1.7.0_py27\n-\n1.6.0_py27\n\n\nWebsite\n\n\nTensorFlow\u2122 is an open source software library for numerical computation using data flow graphs.\n\n\n\n\n\n\ndeep learning\n\n\npy-tensorlayer\n\n\n-\n1.6.3_py27\n\n\nWebsite\n\n\nTensorLayer is a Deep Learning (DL) and Reinforcement Learning (RL) library extended from Google TensorFlow.\n\n\n\n\n\n\ndeep learning\n\n\npy-theano\n\n\n-\n1.0.1_py27\n\n\nWebsite\n\n\nTheano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently.\n\n\n\n\n\n\ndeep learning\n\n\ntensorrt\n\n\n-\n3.0.1\n-\n3.0.4\n\n\nWebsite\n\n\nNVIDIA TensorRT\u2122 is a high-performance deep learning inference optimizer and runtime that delivers low latency, high-throughput inference for deep learning applications.\n\n\n\n\n\n\ndeep learning\n\n\ntorch\n\n\n-\n20180202\n\n\nWebsite\n\n\nTorch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first.\n\n\n\n\n\n\nlinear algebra\n\n\narmadillo\n\n\n-\n8.200.1\n\n\nWebsite\n\n\nArmadillo is a high quality linear algebra library (matrix maths) for the C++ language, aiming towards a good balance between speed and ease of use.\n\n\n\n\n\n\nmachine learning\n\n\npy-scikit-learn\n\n\n-\n0.19.1_py27\n\n\nWebsite\n\n\nScikit-learn is a free software machine learning library for the Python programming language.\n\n\n\n\n\n\nnumerical analysis\n\n\nmatlab\n\n\n-\nR2017a\n-\nR2017b\n\n\nWebsite\n\n\nMATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and fourth-generation programming language.\n\n\n\n\n\n\nnumerical analysis\n\n\noctave\n\n\n-\n4.2.1\n\n\nWebsite\n\n\nGNU Octave is a high-level language primarily intended for numerical computations.\n\n\n\n\n\n\nnumerical library\n\n\narpack\n\n\n-\n3.5.0\n\n\nWebsite\n\n\nCollection of Fortran77 subroutines designed to solve large scale eigenvalue problems.\n\n\n\n\n\n\nnumerical library\n\n\nfftw\n\n\n-\n3.3.6\n\n\nWebsite\n\n\nThe Fastest Fourier Transform in the West (FFTW) is a software library for computing discrete Fourier transforms (DFTs).\n\n\n\n\n\n\nnumerical library\n\n\nglpk\n\n\n-\n4.63\n\n\nWebsite\n\n\nThe GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems.\n\n\n\n\n\n\nnumerical library\n\n\ngmp\n\n\n-\n6.1.2\n\n\nWebsite\n\n\nGMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating-point numbers.\n\n\n\n\n\n\nnumerical library\n\n\ngsl\n\n\n-\n1.16\n-\n2.3\n\n\nWebsite\n\n\nThe GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting.\n\n\n\n\n\n\nnumerical library\n\n\nharminv\n\n\n-\n1.4.1\n\n\nWebsite\n\n\nharminv is a program designed to solve the problem of harmonic inversion: given a time series consisting of a sum of sinusoids (modes), extract their frequencies and amplitudes.\n\n\n\n\n\n\nnumerical library\n\n\nimkl\n\n\n-\n2017.u2\n-\n2018.u1\n-\n2018\n\n\nWebsite\n\n\nIntel Math Kernel Library (Intel MKL) is a library of optimized math routines for science, engineering, and financial applications. Core math functions include BLAS, LAPACK, ScaLAPACK, sparse solvers, fast Fourier transforms, and vector math.[3] The routines in MKL are hand-optimized specifically for Intel processors\n\n\n\n\n\n\nnumerical library\n\n\nlibxsmm\n\n\n-\n1.8.1\n\n\nWebsite\n\n\nLIBXSMM is a library for small dense and small sparse matrix-matrix multiplications as well as for deep learning primitives such as small convolutions\n\n\n\n\n\n\nnumerical library\n\n\nmetis\n\n\n-\n5.1.0\n\n\nWebsite\n\n\nMETIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices.\n\n\n\n\n\n\nnumerical library\n\n\nmpfr\n\n\n-\n3.1.5\n\n\nWebsite\n\n\nThe MPFR library is a C library for multiple-precision floating-point computations with correct rounding.\n\n\n\n\n\n\nnumerical library\n\n\nmumps\n\n\n-\n5.1.2\n\n\nWebsite\n\n\nA parallel sparse direct solver.\n\n\n\n\n\n\nnumerical library\n\n\nnagcl\n\n\n-\ncll6i26dcl\n\n\nWebsite\n\n\nThe NAG C Library is the largest and most comprehensive collection of mathematical and statistical algorithms for C and C++.\n\n\n\n\n\n\nnumerical library\n\n\nnagfl\n\n\n-\nfll6i26dcl\n\n\nWebsite\n\n\nThe NAG Fortran Library is the largest and most comprehensive collection of numerical and statistical algorithms in Fortran.\n\n\n\n\n\n\nnumerical library\n\n\nnagfs\n\n\n-\nfsl6i26dcl\n\n\nWebsite\n\n\nThe NAG Library for SMP \n Multicore is based on, and includes, the full functionality of the NAG Fortran Library.\n\n\n\n\n\n\nnumerical library\n\n\nnagmb\n\n\n-\nMBL6I25DNL\n\n\nWebsite\n\n\nThe NAG C Library is the largest and most comprehensive collection of mathematical and statistical algorithms for C and C++.\n\n\n\n\n\n\nnumerical library\n\n\nopenblas\n\n\n-\n0.2.19\n\n\nWebsite\n\n\nOpenBLAS is an optimized BLAS library\n\n\n\n\n\n\nnumerical library\n\n\nparmetis\n\n\n-\n4.0.3\n\n\nWebsite\n\n\nParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices.\n\n\n\n\n\n\nnumerical library\n\n\npy-pyublas\n\n\n-\n2017.1_py27\n\n\nWebsite\n\n\nPyUblas provides a seamless glue layer between Numpy and Boost.Ublas for use with Boost.Python.\n\n\n\n\n\n\nnumerical library\n\n\nqrupdate\n\n\n-\n1.1.2\n\n\nWebsite\n\n\nqrupdate is a Fortran library for fast updates of QR and Cholesky decompositions.\n\n\n\n\n\n\nnumerical library\n\n\nscalapack\n\n\n-\n2.0.2\n\n\nWebsite\n\n\nScaLAPACK is a library of high-performance linear algebra routines for parallel distributed memory machines.\n\n\n\n\n\n\nnumerical library\n\n\nscotch\n\n\n-\n6.0.4\n\n\nWebsite\n\n\nSoftware package and libraries for sequential and parallel graph partitioning, static mapping and clustering, sequential mesh and hypergraph partitioning, and sequential and parallel sparse matrix block ordering.\n\n\n\n\n\n\nnumerical library\n\n\nsuperlu\n\n\n-\n5.2.1\n\n\nWebsite\n\n\nSuperLU is a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations.\n\n\n\n\n\n\nnumerical library\n\n\nxblas\n\n\n-\n1.0.248\n\n\nWebsite\n\n\nExtra precise basic linear algebra subroutines.\n\n\n\n\n\n\noptimization\n\n\ngurobi\n\n\n-\n7.5.1\n\n\nWebsite\n\n\nThe Gurobi Optimizer is a commercial optimization solver for mathematical programming.\n\n\n\n\n\n\noptimization\n\n\nknitro\n\n\n-\n10.3.0\n\n\nWebsite\n\n\nArtelys Knitro is an optimization solver for difficult large-scale nonlinear problems.\n\n\n\n\n\n\nscientific computing\n\n\npy-scipystack\n\n\n-\n1.0_py36\n-\n1.0_py27\n\n\nWebsite\n\n\nThe SciPy Stack is a collection of open source software for scientific computing in Python. It provides the following packages: numpy, scipy, matplotlib, ipython, jupyter, pandas, sympy and nose.\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n-\n3.4.0\n\n\nWebsite\n\n\nR is a free software environment for statistical computing and graphics.\n\n\n\n\n\n\nstatistics\n\n\njags\n\n\n-\n4.3.0\n\n\nWebsite\n\n\nJust another Gibbs sampler (JAGS) is a program for simulation from Bayesian hierarchical models using Markov chain Monte Carlo (MCMC).\n\n\n\n\n\n\nstatistics\n\n\npy-rpy2\n\n\n-\n2.8.6_py27\n-\n2.9.2_py36\n\n\nWebsite\n\n\nrpy2 is an interface to R running embedded in a Python process.\n\n\n\n\n\n\nstatistics\n\n\nr-rstan\n\n\n-\n2.17.3\n\n\nWebsite\n\n\nRStan is the R interface to Stan, an open-source software for facilitating statistical inference at the frontiers of applied statistics.\n\n\n\n\n\n\nstatistics\n\n\nrstudio\n\n\n-\n1.1.423\n\n\nWebsite\n\n\nRStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.\n\n\n\n\n\n\nstatistics\n\n\nsas\n\n\n-\n9.4\n\n\nWebsite\n\n\nSAS is a software suite developed by SAS Institute for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics.\n\n\n\n\n\n\nstatistics\n\n\nstata\n\n\n-\n14\n-\n15\n\n\nWebsite\n\n\nStata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics.\n\n\n\n\n\n\nsymbolic\n\n\nlibmatheval\n\n\n-\n1.1.11\n\n\nWebsite\n\n\nGNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text.\n\n\n\n\n\n\n\n\nPhysics\n#\n\n\n\n\n\n\n\n\nField\n\n\nModule\nname\n\n\nVersion(s)\n\n\nURL\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nastronomy\n\n\nheasoft\n\n\n-\n6.22.1\n\n\nWebsite\n\n\nHEAsoft is a Unified Release of the FTOOLS (General and mission-specific tools to manipulate FITS files) and XANADU (High-level, multi-mission tasks for X-ray astronomical spectral, timing, and imaging data analysis) software packages.\n\n\n\n\n\n\ngeophysics\n\n\nopensees\n\n\n-\n2.5.0\n\n\nWebsite\n\n\nOpenSees is a software framework for developing applications to simulate the performance of structural and geotechnical systems subjected to earthquakes.\n\n\n\n\n\n\ngeoscience\n\n\ngdal\n\n\n-\n2.2.1\n\n\nWebsite\n\n\nGDAL is a translator library for raster and vector geospatial data formats.\n\n\n\n\n\n\ngeoscience\n\n\ngeos\n\n\n-\n3.6.2\n\n\nWebsite\n\n\nGEOS (Geometry Engine - Open Source) is a C++ port of Java Topology Suite (JTS).\n\n\n\n\n\n\ngeoscience\n\n\nproj\n\n\n-\n4.9.3\n\n\nWebsite\n\n\nproj.4 is a standard UNIX filter function which converts geographic longitude and latitude coordinates into cartesian coordinates (and vice versa.\n\n\n\n\n\n\ngeoscience\n\n\nudunits\n\n\n-\n2.2.26\n\n\nWebsite\n\n\nThe UDUNITS package from Unidata is a C-based package for the programatic handling of units of physical quantities.\n\n\n\n\n\n\nphotonics\n\n\nmeep\n\n\n-\n1.3\n\n\nWebsite\n\n\nMeep is a free finite-difference time-domain (FDTD) simulation software package to model electromagnetic systems.\n\n\n\n\n\n\nphotonics\n\n\nmpb\n\n\n-\n1.5\n\n\nWebsite\n\n\nMPB is a free software package for computing the band structures, or dispersion relations, and electromagnetic modes of periodic dielectric structures, on both serial and parallel computers.\n\n\n\n\n\n\n\n\nSystem\n#\n\n\n\n\n\n\n\n\nField\n\n\nModule\nname\n\n\nVersion(s)\n\n\nURL\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbecnhmark\n\n\nhp2p\n\n\n-\n3.2\n\n\nWebsite\n\n\nHeavy Peer To Peer: a MPI based benchmark for network diagnostic.\n\n\n\n\n\n\ncompression\n\n\nlz4\n\n\n-\n1.8.0\n\n\nWebsite\n\n\nLZ4 is lossless compression algorithm.\n\n\n\n\n\n\ncompression\n\n\nlzo\n\n\n-\n2.10\n\n\nWebsite\n\n\nLZO is a portable lossless data compression library written in ANSI C.\n\n\n\n\n\n\ncompression\n\n\nszip\n\n\n-\n2.1.1\n\n\nWebsite\n\n\nSzip compression software, providing lossless compression of scientific data, is an implementation of the extended-Rice lossless compression algorithm.\n\n\n\n\n\n\ncompression\n\n\nxz\n\n\n-\n5.2.3\n\n\nWebsite\n\n\nXZ Utils is free general-purpose data compression software with a high compression ratio.\n\n\n\n\n\n\ncompression\n\n\nzlib\n\n\n-\n1.2.11\n\n\nWebsite\n\n\nzlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system.\n\n\n\n\n\n\ncontainers\n\n\nsingularity\n\n\n-\n2.4.6\n\n\nWebsite\n\n\nSingularity is a container framework that enables users to package entire scientific workflows, software and libraries.\n\n\n\n\n\n\ndatabase\n\n\nbdb\n\n\n-\n6.2.32\n\n\nWebsite\n\n\nBerkeley DB (BDB) is a software library intended to provide a high-performance embedded database for key/value data.\n\n\n\n\n\n\ndatabase\n\n\nmariadb\n\n\n-\n10.2.11\n\n\nWebsite\n\n\nMariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL.\n\n\n\n\n\n\ndatabase\n\n\nsqlite\n\n\n-\n3.18.0\n\n\nWebsite\n\n\nSQLite is a self-contained, high-reliability, embedded, full-featured, public-domain, SQL database engine.\n\n\n\n\n\n\nfile management\n\n\nfpart\n\n\n-\n0.9.3\n\n\nWebsite\n\n\nfpart sorts files and packs them into partitions.\n\n\n\n\n\n\nfile transfer\n\n\ngdrive\n\n\n-\n2.1.0\n\n\nWebsite\n\n\ngdrive is a command line utility for interacting with Google Drive.\n\n\n\n\n\n\nfile transfer\n\n\nlftp\n\n\n-\n4.8.1\n\n\nWebsite\n\n\nLFTP is a sophisticated file transfer program supporting a number of network protocols (ftp, http, sftp, fish, torrent).\n\n\n\n\n\n\nfile transfer\n\n\nmpifileutils\n\n\n-\n20170210\n\n\nWebsite\n\n\nmpiFileUtils is a suite of MPI-based tools to manage large datasets, which may vary from large directory trees to large files.\n\n\n\n\n\n\nfile transfer\n\n\npy-globus-cli\n\n\n-\n1.2.0\n\n\nWebsite\n\n\nA command line wrapper over the Globus SDK for Python.\n\n\n\n\n\n\nfile transfer\n\n\nrclone\n\n\n-\n1.39\n\n\nWebsite\n\n\nRclone is a command line program to sync files and directories to and from: Google Drive, Amazon S3, Dropbox, Google Cloud Storage, Amazon Drive, Microsoft One Drive, Hubic, Backblaze B2, Yandex Disk, or the local filesystem.\n\n\n\n\n\n\nlanguage\n\n\ntcltk\n\n\n-\n8.6.6\n\n\nWebsite\n\n\nTcl (Tool Command Language) is a dynamic programming language, suitable for web and desktop applications, networking, administration, testing. Tk is a graphical user interface toolkit.\n\n\n\n\n\n\nlibs\n\n\napr\n\n\n-\n1.6.3\n\n\nWebsite\n\n\nThe Apache Portable Runtime is a supporting library for the Apache web server. It provides a set of APIs that map to the underlying operating system.\n\n\n\n\n\n\nlibs\n\n\napr-util\n\n\n-\n1.6.1\n\n\nWebsite\n\n\nThe Apache Portable Runtime is a supporting library for the Apache web server. It provides a set of APIs that map to the underlying operating system.\n\n\n\n\n\n\nlibs\n\n\natk\n\n\n-\n2.24.0\n\n\nWebsite\n\n\nATK is the Accessibility Toolkit. It provides a set of generic interfaces allowing accessibility technologies such as screen readers to interact with a graphical user interface.\n\n\n\n\n\n\nlibs\n\n\nbenchmark\n\n\n-\n1.2.0\n\n\nWebsite\n\n\nA microbenchmark support library\n\n\n\n\n\n\nlibs\n\n\ncairo\n\n\n-\n1.14.10\n\n\nWebsite\n\n\nCairo is a 2D graphics library with support for multiple output devices.\n\n\n\n\n\n\nlibs\n\n\ncups\n\n\n-\n2.2.4\n\n\nWebsite\n\n\nCUPS is the standards-based, open source printing system.\n\n\n\n\n\n\nlibs\n\n\ndbus\n\n\n-\n1.10.22\n\n\nWebsite\n\n\nD-Bus is a message bus system, a simple way for applications to talk to one another.\n\n\n\n\n\n\nlibs\n\n\nenchant\n\n\n-\n1.6.1\n-\n2.2.3\n\n\nWebsite\n\n\nEnchant is a library (and command-line program) that wraps a number of different spelling libraries and programs with a consistent interface.\n\n\n\n\n\n\nlibs\n\n\nfltk\n\n\n-\n1.3.4\n\n\nWebsite\n\n\nFLTK (pronounced 'fulltick') is a cross-platform C++ GUI toolkit.\n\n\n\n\n\n\nlibs\n\n\nfontconfig\n\n\n-\n2.12.4\n\n\nWebsite\n\n\nFontconfig is a library for configuring and customizing font access.\n\n\n\n\n\n\nlibs\n\n\nfreeglut\n\n\n-\n3.0.0\n\n\nWebsite\n\n\nFreeGLUT is a free-software/open-source alternative to the OpenGL Utility Toolkit (GLUT) library.\n\n\n\n\n\n\nlibs\n\n\nfreetype\n\n\n-\n2.8\n\n\nWebsite\n\n\nFreeType is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images).\n\n\n\n\n\n\nlibs\n\n\ngc\n\n\n-\n7.6.0\n\n\nWebsite\n\n\nThe Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new.\n\n\n\n\n\n\nlibs\n\n\ngconf\n\n\n-\n2.9.91\n\n\nWebsite\n\n\nGConf is a system for storing application preferences.\n\n\n\n\n\n\nlibs\n\n\ngdk-pixbuf\n\n\n-\n2.36.8\n\n\nWebsite\n\n\nThe GdkPixbuf library provides facilities for loading images in a variety of file formats.\n\n\n\n\n\n\nlibs\n\n\ngflags\n\n\n-\n2.2.1\n\n\nWebsite\n\n\nThe gflags package contains a C++ library that implements commandline flags processing.\n\n\n\n\n\n\nlibs\n\n\ngiflib\n\n\n-\n5.1.4\n\n\nWebsite\n\n\nGIFLIB is a package of portable tools and library routines for working with GIF images.\n\n\n\n\n\n\nlibs\n\n\nglib\n\n\n-\n2.52.3\n\n\nWebsite\n\n\nThe GLib library provides core non-graphical functionality such as high level data types, Unicode manipulation, and an object and type system to C programs.\n\n\n\n\n\n\nlibs\n\n\nglog\n\n\n-\n0.3.5\n\n\nWebsite\n\n\nC++ implementation of the Google logging module.\n\n\n\n\n\n\nlibs\n\n\ngnutls\n\n\n-\n3.5.9\n\n\nWebsite\n\n\nGnuTLS is a secure communications library implementing the SSL, TLS and DTLS protocols and technologies around them.\n\n\n\n\n\n\nlibs\n\n\ngobject-introspection\n\n\n-\n1.52.1\n\n\nWebsite\n\n\nGObject introspection is a middleware layer between C libraries (using GObject) and language bindings.\n\n\n\n\n\n\nlibs\n\n\ngoogletest\n\n\n-\n1.8.0\n\n\nWebsite\n\n\nGoogle Test is Google's C++ test framework.\n\n\n\n\n\n\nlibs\n\n\ngtk+\n\n\n-\n2.24.30\n-\n3.22.18\n\n\nWebsite\n\n\nGTK+, or the GIMP Toolkit, is a multi-platform toolkit for creating graphical user interfaces.\n\n\n\n\n\n\nlibs\n\n\nharfbuzz\n\n\n-\n1.4.8\n\n\nWebsite\n\n\nHarfBuzz is an OpenType text shaping engine.\n\n\n\n\n\n\nlibs\n\n\nhunspell\n\n\n-\n1.6.2\n\n\nWebsite\n\n\nHunspell is a spell checker.\n\n\n\n\n\n\nlibs\n\n\nhyphen\n\n\n-\n2.8.8\n\n\nWebsite\n\n\nHyphen is a hyphenation library to use converted TeX hyphenation patterns.\n\n\n\n\n\n\nlibs\n\n\nicu\n\n\n-\n59.1\n\n\nWebsite\n\n\nICU is a set of C/C++ and Java libraries providing Unicode and Globalization support for software applications.\n\n\n\n\n\n\nlibs\n\n\nlibepoxy\n\n\n-\n1.4.1\n\n\nWebsite\n\n\nEpoxy is a library for handling OpenGL function pointer management for you.\n\n\n\n\n\n\nlibs\n\n\nlibffi\n\n\n-\n3.2.1\n\n\nWebsite\n\n\nlibffi is a portable Foreign Function Interface library.\n\n\n\n\n\n\nlibs\n\n\nlibgcrypt\n\n\n-\n1.8.2\n\n\nWebsite\n\n\nLibgcrypt is a general purpose cryptographic library originally based on code from GnuPG.\n\n\n\n\n\n\nlibs\n\n\nlibgd\n\n\n-\n2.2.5\n\n\nWebsite\n\n\nGD is an open source code library for the dynamic creation of images by programmers.\n\n\n\n\n\n\nlibs\n\n\nlibgpg-error\n\n\n-\n1.27\n\n\nWebsite\n\n\nLibgpg-error is a small library that originally defined common error values for all GnuPG components.\n\n\n\n\n\n\nlibs\n\n\nlibidl\n\n\n-\n0.8.14\n\n\nWebsite\n\n\nThe libIDL package contains libraries for Interface Definition Language files. This is a specification for defining portable interfaces.\n\n\n\n\n\n\nlibs\n\n\nlibjpeg-turbo\n\n\n-\n1.5.1\n\n\nWebsite\n\n\nlibjpeg-turbo is a JPEG image codec that uses SIMD instructions (MMX, SSE2, AVX2, NEON, AltiVec) to accelerate baseline JPEG compression and decompression on x86, x86-64, ARM, and PowerPC systems\n\n\n\n\n\n\nlibs\n\n\nlibmng\n\n\n-\n2.0.3\n\n\nWebsite\n\n\nTHE reference library for reading, displaying, writing and examining Multiple-Image Network Graphics. MNG is the animation extension to the popular PNG image-format.\n\n\n\n\n\n\nlibs\n\n\nlibpng\n\n\n-\n1.2.57\n-\n1.6.29\n\n\nWebsite\n\n\nlibpng is the official PNG reference library. It supports almost all PNG features, is extensible, and has been extensively tested for over 20 years.\n\n\n\n\n\n\nlibs\n\n\nlibproxy\n\n\n-\n0.4.15\n\n\nWebsite\n\n\nlibproxy is a library that provides automatic proxy configuration management.\n\n\n\n\n\n\nlibs\n\n\nlibressl\n\n\n-\n2.5.3\n\n\nWebsite\n\n\nLibreSSL is a version of the TLS/crypto stack forked from OpenSSL in 2014, with goals of modernizing the codebase, improving security, and applying best practice development processes.\n\n\n\n\n\n\nlibs\n\n\nlibsoup\n\n\n-\n2.61.2\n\n\nWebsite\n\n\nlibsoup is an HTTP client/server library for GNOME.\n\n\n\n\n\n\nlibs\n\n\nlibtasn1\n\n\n-\n4.13\n\n\nWebsite\n\n\nLibtasn1 is the ASN.1 library used by GnuTLS, p11-kit and some other packages.\n\n\n\n\n\n\nlibs\n\n\nlibtiff\n\n\n-\n4.0.8\n\n\nWebsite\n\n\nlibtiff provides support for the Tag Image File Format (TIFF), a widely used format for storing image data.\n\n\n\n\n\n\nlibs\n\n\nlibunistring\n\n\n-\n0.9.7\n\n\nWebsite\n\n\nLibunistring provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard.\n\n\n\n\n\n\nlibs\n\n\nlibwebp\n\n\n-\n0.6.1\n\n\nWebsite\n\n\nWebP is a modern image format that provides superior lossless and lossy compression for images on the web.\n\n\n\n\n\n\nlibs\n\n\nlibxml2\n\n\n-\n2.9.4\n\n\nWebsite\n\n\nLibxml2 is a XML C parser and toolkit.\n\n\n\n\n\n\nlibs\n\n\nlibxslt\n\n\n-\n1.1.32\n\n\nWebsite\n\n\nLibxslt is the XSLT C library developed for the GNOME project. XSLT itself is a an XML language to define transformation for XML.\n\n\n\n\n\n\nlibs\n\n\nmesa\n\n\n-\n17.1.6\n\n\nWebsite\n\n\nMesa is an open-source implementation of the OpenGL, Vulkan and other specifications.\n\n\n\n\n\n\nlibs\n\n\nncurses\n\n\n-\n6.0\n\n\nWebsite\n\n\nThe ncurses (new curses) library is a free software emulation of curses in System V Release 4.0 (SVr4), and more.\n\n\n\n\n\n\nlibs\n\n\nnettle\n\n\n-\n3.3\n\n\nWebsite\n\n\nNettle is a cryptographic library that is designed to fit easily in more or less any context.\n\n\n\n\n\n\nlibs\n\n\norbit\n\n\n-\n2.14.19\n\n\nWebsite\n\n\nORBit2 is a CORBA 2.4-compliant Object Request Broker (ORB) featuring mature C, C++ and Python bindings.\n\n\n\n\n\n\nlibs\n\n\npango\n\n\n-\n1.40.10\n\n\nWebsite\n\n\nPango is a library for laying out and rendering of text, with an emphasis on internationalization.\n\n\n\n\n\n\nlibs\n\n\npcre\n\n\n-\n8.40\n\n\nWebsite\n\n\nThe PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5.\n\n\n\n\n\n\nlibs\n\n\npopt\n\n\n-\n1.16\n\n\nWebsite\n\n\nLibrary for parsing command line options.\n\n\n\n\n\n\nlibs\n\n\npy-lmdb\n\n\n-\n0.93\n\n\nWebsite\n\n\nUniversal Python binding for the LMDB 'Lightning' Database.\n\n\n\n\n\n\nlibs\n\n\npy-mako\n\n\n-\n1.0.7_py36\n-\n1.0.7_py27\n\n\nWebsite\n\n\nMako is a template library written in Python. It provides a familiar, non-XML syntax which compiles into Python modules for maximum performance.\n\n\n\n\n\n\nlibs\n\n\npy-pyqt5\n\n\n-\n5.9.1_py36\n\n\nWebsite\n\n\nPyQt5 is a comprehensive set of Python bindings for Qt v5.\n\n\n\n\n\n\nlibs\n\n\nreadline\n\n\n-\n7.0\n\n\nWebsite\n\n\nThe GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in.\n\n\n\n\n\n\nlibs\n\n\nsnappy\n\n\n-\n1.1.7\n\n\nWebsite\n\n\nA fast compressor/decompressor.\n\n\n\n\n\n\nresource monitoring\n\n\nremora\n\n\n-\n1.8.2\n\n\nWebsite\n\n\nRemora is a tool to monitor runtime resource utilization.\n\n\n\n\n\n\nscm\n\n\ngit\n\n\n-\n2.12.2\n\n\nWebsite\n\n\nGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.\n\n\n\n\n\n\nscm\n\n\ngit-lfs\n\n\n-\n2.4.0\n\n\nWebsite\n\n\nGit Large File Storage (LFS) replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server.\n\n\n\n\n\n\nscm\n\n\nsubversion\n\n\n-\n1.9.7\n\n\nWebsite\n\n\nSubversion is an open source version control system.\n\n\n\n\n\n\ntools\n\n\ncurl\n\n\n-\n7.54.0\n\n\nWebsite\n\n\ncurl is an open source command line tool and library for transferring data with URL syntax.\n\n\n\n\n\n\ntools\n\n\nexpat\n\n\n-\n2.2.3\n\n\nWebsite\n\n\nExpat is a stream-oriented XML parser library written in C.\n\n\n\n\n\n\ntools\n\n\ngraphicsmagick\n\n\n-\n1.3.26\n\n\nWebsite\n\n\nGraphicsMagick is the swiss army knife of image processing.\n\n\n\n\n\n\ntools\n\n\nimagemagick\n\n\n-\n7.0.7-2\n\n\nWebsite\n\n\nImageMagick is a free and open-source software suite for displaying, converting, and editing raster image and vector image files.\n\n\n\n\n\n\ntools\n\n\nleveldb\n\n\n-\n1.20\n\n\nWebsite\n\n\nSymas LMDB is an extraordinarily fast, memory-efficient database we developed for the Symas OpenLDAP Project.\n\n\n\n\n\n\ntools\n\n\nlmdb\n\n\n-\n0.9.21\n\n\nWebsite\n\n\nSymas LMDB is an extraordinarily fast, memory-efficient database we developed for the Symas OpenLDAP Project.\n\n\n\n\n\n\ntools\n\n\nmotif\n\n\n-\n2.3.7\n\n\nWebsite\n\n\nMotif is the toolkit for the Common Desktop Environment.\n\n\n\n\n\n\ntools\n\n\nparallel\n\n\n-\n20180122\n\n\nWebsite\n\n\nGNU parallel is a shell tool for executing jobs in parallel using one or more computers.\n\n\n\n\n\n\ntools\n\n\nqt\n\n\n-\n5.9.1\n\n\nWebsite\n\n\nQT is a cross-platform application framework that is used for developing application software that can be run on various software and hardware platforms.\n\n\n\n\n\n\ntools\n\n\nrocksdb\n\n\n-\n5.7.3\n\n\nWebsite\n\n\nA library that provides an embeddable, persistent key-value store for fast storage.\n\n\n\n\n\n\ntools\n\n\nx11\n\n\n-\n7.7\n\n\nWebsite\n\n\nThe X.Org project provides an open source implementation of the X Window System.\n\n\n\n\n\n\ntools\n\n\nxkeyboard-config\n\n\n-\n2.21\n\n\nWebsite\n\n\nThe non-arch keyboard configuration database for X Window.\n\n\n\n\n\n\n\n\nViz\n#\n\n\n\n\n\n\n\n\nField\n\n\nModule\nname\n\n\nVersion(s)\n\n\nURL\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngraphs\n\n\ngraphviz\n\n\n-\n2.40.1\n\n\nWebsite\n\n\nGraphviz is open source graph visualization software.\n\n\n\n\n\n\nmolecular visualization\n\n\npymol\n\n\n-\n1.8.6.2\n\n\nWebsite\n\n\nPyMOL is a Python-enhanced molecular graphics tool.\n\n\n\n\n\n\nplotting\n\n\ngnuplot\n\n\n-\n5.2.0\n\n\nWebsite\n\n\nGnuplot is a portable command-line driven graphing utility for Linux, OS/2, MS Windows, OSX, VMS, and many other platforms.\n\n\n\n\n\n\nplotting\n\n\npy-matplotlib\n\n\n-\n2.1.0_py27\n-\n2.1.2_py36\n-\n2.0.2_py27\n\n\nWebsite\n\n\nMatplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n\n\n\n\n\n\nplotting\n\n\npy-plotly\n\n\n-\n2.4.1_py27\n\n\nWebsite\n\n\nPlotly's Python graphing library makes interactive, publication-quality graphs online.\n\n\n\n\n\n\nremote display\n\n\nvirtualgl\n\n\n-\n2.5.2\n\n\nWebsite\n\n\nVirtualGL is an open source toolkit that gives any Unix or Linux remote display software the ability to run OpenGL applications with full 3D hardware acceleration.", 
            "title": "List"
        }, 
        {
            "location": "/docs/software/list/#software-list", 
            "text": "The full list of software centrally installed and managed on Sherlock is in the\ntables below.   Work in progress  Software installations on Sherlock are an ever ongoing process. We're\ncontinuously adding new software to the list. If you're looking for\nsomething that is not in the list, please take a look here  for options.   Software modules on Sherlock are organized in  categories , by scientific\nfield. It means that you will have to first load a category module before\ngetting access to individual modules.  The  math  and  devel  categories are\nloaded by default. See the  Modules page  for further details and\nexamples.", 
            "title": "Software list"
        }, 
        {
            "location": "/docs/software/list/#categories", 
            "text": "As of Wednesday, April 18 2018, we provide 331 software packages, in 7 categories, covering 45 fields of science:   biology   computational biology, cryo-em, genomics, neurology, phylogenetics  chemistry   computational chemistry, molecular dynamics, quantum chemistry  devel   build, compiler, data, language, libs, mpi, networking, parser  math   computational geometry, deep learning, linear algebra, machine learning, numerical analysis, numerical library, optimization, scientific computing, statistics, symbolic  physics   astronomy, geophysics, geoscience, photonics  system   becnhmark, compression, containers, database, file management, file transfer, language, libs, resource monitoring, scm, tools  viz   graphs, molecular visualization, plotting, remote display    Licensed software  Access to software modules marked with   in the tables\nbelow is restricted to properly licensed user groups.  The SRCC is not funded to provide commercial software on Sherlock and\nresearchers are responsible for the costs of purchasing and renewing\ncommercial software licenses. For more information, please feel free to contact us  and see the  Stanford\nSoftware Licensing page  for\npurchasing information.    \n.lic  { color: darkred; }\n.lic:after {\n    content: attr(class);\n}", 
            "title": "Categories"
        }, 
        {
            "location": "/docs/software/list/#biology", 
            "text": "Field  Module name  Version(s)  URL  Description      computational biology  imp  - 2.8.0  Website  IMP's broad goal is to contribute to a comprehensive structural characterization of biomolecules ranging in size and complexity from small peptides to large macromolecular assemblies, by integrating data from diverse biochemical and biophysical experiments.    computational biology  py-biopython  - 1.70_py27  Website  Biopython is a set of freely available tools for biological computation written in Python.    computational biology  rosetta  - 3.8  Website  Rosetta is the premier software suite for modeling macromolecular structures. As a flexible, multi-purpose application, it includes tools for structure prediction, design, and remodeling of proteins and nucleic acids.    cryo-em  eman2  - 2.2  Website  EMAN2 is a broadly based greyscale scientific image processing suite with a primary focus on processing data from transmission electron microscopes.    cryo-em  relion  - 2.0.3 - 2.1  Website  RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a stand-alone computer program that employs an empirical Bayesian approach to refinement of (multiple) 3D reconstructions or 2D class averages in electron cryo-microscopy (cryo-EM).    genomics  angsd  - 0.919  Website  ANGSD is a software for analyzing next generation sequencing data.    genomics  bcftools  - 1.6  Website  BCFtools is a program for variant calling and manipulating files in the Variant Call Format (VCF) and its binary counterpart BCF.    genomics  bedtools  - 2.27.1  Website  The bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks.    genomics  bowtie  - 1.2.2  Website  Bowtie is an ultrafast, memory-efficient short read aligner.    genomics  bowtie2  - 2.3.4.1  Website  Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences.    genomics  bwa  - 0.7.17  Website  BWA (Burrows-Wheeler Aligner) is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome.    genomics  cufflinks  - 2.2.1  Website  Cufflinks assembles transcripts, estimates their abundances, and tests for differential expression and regulation in RNA-Seq samples.    genomics  fastx_toolkit  - 0.0.14  Website  The FASTX-Toolkit is a collection of command line tools for Short-Reads FASTA/FASTQ files preprocessing.    genomics  hisat2  - 2.1.0  Website  HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes (as well as to a single reference genome).    genomics  htslib  - 1.6  Website  C library for high-throughput sequencing data formats.    genomics  ncbi-blast+  - 2.6.0  Website  NCBI BLAST+ is a suite of command-line tools to run BLAST (Basic Local Alignment Search Tool), an algorithm for comparing primary biological sequence information.    genomics  plink  - 1.07 - 1.90b5.3 - 2.0a1  Website  PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner.    genomics  py-bx-python  - 0.8.1_py27  Website  Tools for manipulating biological data, particularly multiple sequence alignments.    genomics  py-macs2  - 2.1.1_py27  Website  MACS (Model-based Analysis of ChIP-Seq) implements a novel ChIP-Seq analysis method.    genomics  samtools  - 1.6  Website  Tools (written in C using htslib) for manipulating next-generation sequencing data.    genomics  star  - 2.5.4b  Website  STAR: ultrafast universal RNA-seq aligner.    genomics  tophat  - 2.1.1  Website  TopHat is a fast splice junction mapper for RNA-Seq reads.    genomics  vcftools  - 0.1.15  Website  VCFtools is a program package designed for working with VCF files, such as those generated by the 1000 Genomes Project.    neurology  afni  - 17.2.07 - 18.0.09  Website  AFNI (Analysis of Functional NeuroImages) is a set of C programs for processing, analyzing, and displaying functional MRI (FMRI) data - a technique for mapping human brain activity.    neurology  ants  - 2.1.0  Website  ANTs computes high-dimensional mappings to capture the statistics of brain structure and function.    neurology  dcm2niix  - 1.0.20171215  Website  dcm2niix is a program esigned to convert neuroimaging data from the DICOM format to the NIfTI format.    neurology  freesurfer  - 6.0.0  Website  An open source software suite for processing and analyzing (human) brain MRI images.    neurology  fsl  - 5.0.10  Website  FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data.    neurology  mricron  - 20160502  Website  MRIcron is a cross-platform NIfTI format image viewer.    neurology  mrtrix  - 0.3.16  Website  MRtrix3 provides a set of tools to perform various types of diffusion MRI analyses, from various forms of tractography through to next-generation group-level analyses.    neurology  py-mdt  - 0.10.9_py36  Website  The Maastricht Diffusion Toolbox, MDT, is a framework and library for parallelized (GPU and multi-core CPU) diffusion Magnetic Resonance Imaging (MRI) modeling.    neurology  spm  - 12  Website  The SPM software package has been designed for the analysis of brain imaging data sequences. The sequences can be a series of images from different cohorts, or time-series from the same subject.    phylogenetics  py-ete  - 3.0.0_py27  Website  A Python framework for the analysis and visualization of trees.", 
            "title": "Biology"
        }, 
        {
            "location": "/docs/software/list/#chemistry", 
            "text": "Field  Module name  Version(s)  URL  Description      computational chemistry  gaussian  - g16  Website  Gaussian is a general purpose computational chemistry software package.    computational chemistry  libint  - 1.1.4 - 2.0.3  Website  Libint computes molecular integrals.    computational chemistry  libxc  - 3.0.0  Website  Libxc is a library of exchange-correlation functionals for density-functional theory.    computational chemistry  nwchem  - 6.8  Website  NWChem is an ab initio computational chemistry software package which also includes quantum chemical and molecular dynamics functionality.    computational chemistry  py-ase  - 3.14.1_py27  Website  The Atomic Simulation Environment (ASE) is a set of tools and Python modules for setting up, manipulating, running, visualizing and analyzing atomistic simulations.    computational chemistry  schrodinger  - 2017-3  Website  Schr\u00f6dinger Suites (Small-molecule Drug Discovery Suite, Material Science Suite, Biologics Suite) provide a set of molecular modelling software.    computational chemistry  vasp  - 5.4.1  Website  The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.    molecular dynamics  gromacs  - 2016.3 - 2018  Website  GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.    molecular dynamics  lammps  - 20180316  Website  LAMMPS is a classical molecular dynamics code that models an ensemble of particles in a liquid, solid, or gaseous state.    molecular dynamics  openmm  - 7.1.1  Website  A high performance toolkit for molecular simulation.    molecular dynamics  plumed  - 2.3.2  Website  PLUMED is an open source library for free energy calculations in molecular systems.    molecular dynamics  py-raspa2  - 2.0.3_py27  Website  RASPA2 is a general purpose classical simulation package that can be used for the simulation of molecules in gases, fluids, zeolites, aluminosilicates, metal-organic frameworks, carbon nanotubes and external fields.    molecular dynamics  quip  - 20170901  Website  The QUIP package is a collection of software tools to carry out molecular dynamics simulations.    quantum chemistry  cp2k  - 4.1  Website  CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems.", 
            "title": "Chemistry"
        }, 
        {
            "location": "/docs/software/list/#devel", 
            "text": "Field  Module name  Version(s)  URL  Description      build  cmake  - 3.8.1  Website  CMake is an extensible, open-source system that manages the build process in an operating system and in a compiler-independent manner.    build  scons  - 2.5.1_py36 - 2.5.1_py27  Website  SCons is an Open Source software construction tool.    compiler  gcc  - 7.1.0 - 7.3.0 - 6.3.0  Website  The GNU Compiler Collection includes front ends for C, C++, Fortran, Java, and Go, as well as libraries for these languages (libstdc++, libgcj,...).    compiler  icc  - 2017.u2 - 2018.u1 - 2018  Website  Intel C++ Compiler, also known as icc or icl, is a group of C and C++ compilers from Intel    compiler  ifort  - 2017.u2 - 2018.u1 - 2018  Website  Intel Fortran Compiler, also known as ifort, is a group of Fortran compilers from Intel    compiler  llvm  - 3.8.1 - 4.0.0 - 5.0.0  Website  The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. Clang is an LLVM native C/C++/Objective-C compiler,    compiler  nagfor  - npl6a61na  Website  The NAG Fortran Compiler is a full standard implementation of the ISO Fortran 95 language with the addition of all of Fortran 2003, most of Fortran 2008 and OpenMP 3.0 and 3.1.    compiler  pgi  - 17.4  Website  PGI compilers and tools, including Open MPI (Community Edition).    compiler  smlnj  - 110.81  Website  Standard ML of New Jersey (abbreviated SML/NJ) is a compiler for the Standard ML '97 programming language.    data  h5utils  - 1.12.1  Website  h5utils is a set of utilities for visualization and conversion of scientific data in the free, portable HDF5 format.    data  hdf5  - 1.10.0p1  Website  HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data.    data  hiredis  - 0.13.3  Website  Hiredis is a minimalistic C client library for the Redis database.    data  ncl  - 6.4.0  Website  NCL is a free interpreted language designed specifically for scientific data processing and visualization.    data  netcdf  - 4.4.1.1  Website  NetCDF is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.    data  pnetcdf  - 1.8.1  Website  Parallel netCDF (PnetCDF) is a parallel I/O library for accessing NetCDF files in CDF-1, 2, and 5 formats.    data  protobuf  - 3.4.0  Website  Protocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data.    data  redis  - 4.0.1  Website  Redis is an open source, in-memory data structure store, used as a database, cache and message broker.    language  cuda  - 8.0.61 - 9.0.176 - 9.1.85  Website  CUDA is a parallel computing platform and application programming interface (API) model created by Nvidia. It allows software developers and software engineers to use a CUDA-enabled graphics processing unit (GPU) for general purpose processing.    language  go  - 1.9  Website  Go is an open source programming language that makes it easy to build simple, reliable, and efficient software.    language  guile  - 2.2.2  Website  GNU Guile is the preferred extension system for the GNU Project, which features an implementation of the Scheme programming language.    language  java  - 1.8.0_131  Website  Java is a general-purpose computer programming language that is concurrent, class-based, object-oriented,[14] and specifically designed to have as few implementation dependencies as possible.    language  julia  - 0.5.1 - 0.6  Website  Julia is a high-level, high-performance dynamic programming language for numerical computing.    language  lua  - 5.3.4  Website  Lua is a powerful, efficient, lightweight, embeddable scripting language. It supports procedural programming, object-oriented programming, functional programming, data-driven programming, and data description.    language  luarocks  - 2.4.3  Website  LuaRocks is the package manager for Lua modules.    language  manticore  - 20180301  Website  Manticore is a high-level parallel programming language aimed at general-purpose applications running on multi-core processors.    language  nodejs  - 8.9.4 - 9.5.0  Website  Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. It provides the npm package manager.    language  perl  - 5.26.0  Website  Perl 5 is a highly capable, feature-rich programming language with over 29 years of development.  Usage on Sherlock    language  py-cython  - 0.27.3_py36 - 0.27.3_py27  Website  Cython is an optimising static compiler for both the Python programming language and the extended Cython programming language (based on Pyrex).    language  py-ipython  - 6.1.0_py36 - 5.4.1_py27  Website  IPython is a command shell for interactive computing in multiple programming languages, originally developed for the Python programming language.    language  py-jupyter  - 1.0.0_py36 - 1.0.0_py27  Website  Jupyter is a browser-based interactive notebook for programming, mathematics, and data science. It supports a number of languages via plugins.    language  python  - 3.6.1 - 2.7.13  Website  Python is an interpreted, interactive, object-oriented programming language.    language  ruby  - 2.4.1  Website  A dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write.    libs  ant  - 1.10.1  Website  Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other.    libs  boost  - 1.64.0  Website  Boost is a set of libraries for the C++ programming language that provide support for tasks and structures such as linear algebra, pseudorandom number generation, multithreading, image processing, regular expressions, and unit testing.    libs  cnmem  - 1.0.0  Website  CNMeM is a simple library to help the Deep Learning frameworks manage CUDA memory.    libs  cub  - 1.7.3  Website  CUB is a flexible library of cooperative threadblock primitives and other utilities for CUDA kernel programming.    libs  cutlass  - 0.1.0  Website  CUTLASS is a collection of CUDA C++ template abstractions for implementing high-performance matrix-multiplication (GEMM) at all levels and scales within CUDA.    libs  eigen  - 3.3.3  Website  Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms.    libs  libctl  - 3.2.2  Website  libctl is a library for supporting flexible control files in scientific simulations.    libs  libgpuarray  - 0.7.5  Website  Library to manipulate tensors on the GPU.    libs  nccl  - 1.3.4 - 2.0.4 - 2.1.15  Website  NCCL (pronounced 'Nickel') is a stand-alone library of standard collective communication routines, such as all-gather, reduce, broadcast, etc., that have been optimized to achieve high bandwidth over PCIe.    libs  opencv  - 3.3.0  Website  OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library.    libs  py-h5py  - 2.7.1_py27  Website  The h5py package is a Pythonic interface to the HDF5 binary data format.    libs  py-netcdf4  - 1.3.1_py36 - 1.3.1_py27  Website  netcdf4-python is a Python interface to the netCDF C library.    libs  py-numba  - 0.35.0_py36 - 0.35.0_py27  Website  Numba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python..    libs  py-pycuda  - 2017.1.1_py27  Website  PyCUDA lets you access Nvidia\u2018s CUDA parallel computation API from Python.    libs  py-scikit-image  - 0.13.0_py27  Website  scikit-image is a collection of algorithms for image processing.    libs  swig  - 3.0.12  Website  SWIG is an interface compiler that connects programs written in C and C++ with scripting languages such as Perl, Python, Ruby, and Tcl.    libs  tbb  - 2017.u2 - 2018.u1 - 2018  Website  Intel\u00ae Threading Building Blocks (Intel\u00ae TBB) is a widely used C++ library for shared-memory parallel programming and heterogeneous computing (intra-node distributed memory programming).    libs  zeromq  - 4.2.2  Website  ZeroMQ (also spelled \u00d8MQ, 0MQ or ZMQ) is a high-performance asynchronous messaging library, aimed at use in distributed or concurrent applications.    mpi  impi  - 2017.u2 - 2018.u1 - 2018  Website  Intel\u00ae MPI Library is a multi-fabric message passing library that implements the Message Passing Interface, version 3.1 (MPI-3.1) specification.    mpi  openmpi  - 2.0.2 - 3.0.1 - 2.1.1  Website  The Open MPI Project is an open source Message Passing Interface implementation that is developed and maintained by a consortium of academic, research, and industry partners.    networking  gasnet  - 1.30.0  Website  GASNet is a language-independent, low-level networking layer that provides network-independent, high-performance communication primitives tailored for implementing parallel global address space SPMD languages and libraries.    networking  libfabric  - 1.6.0  Website  The Open Fabrics Interfaces (OFI) is a framework focused on exporting fabric communication services to applications. Libfabric is the library that defines and exports the user-space API of OFI.    networking  ucx  - 1.2.1  Website  UCX is a communication library implementing high-performance messaging for MPI/PGAS frameworks.    parser  xerces-c  - 3.2.1  Website  Xerces-C++ is a validating XML parser written in a portable subset of C++.", 
            "title": "Devel"
        }, 
        {
            "location": "/docs/software/list/#math", 
            "text": "Field  Module name  Version(s)  URL  Description      computational geometry  cgal  - 4.10  Website  The Computational Geometry Algorithms Library (CGAL) is a C++ library that aims to provide easy access to efficient and reliable algorithms in computational geometry.    computational geometry  qhull  - 2015.2  Website  Qhull computes the convex hull, Delaunay triangulation, Voronoi diagram, halfspace intersection about a point, furthest-site Delaunay triangulation, and furthest-site Voronoi diagram.    deep learning  caffe2  - 0.8.1  Website  Caffe2 is a deep learning framework that provides an easy and straightforward way to experiment with deep learning and leverage community contributions of new models and algorithms.    deep learning  cudnn  - 5.1 - 6.0 - 7.0.1 - 7.0.4 - 7.0.5 - 7.1.1  Website  NVIDIA cuDNN is a GPU-accelerated library of primitives for deep neural networks.    deep learning  py-horovod  - 0.12.1_py36 - 0.12.1_py27  Website  Horovod is a distributed training framework for TensorFlow. The goal of Horovod is to make distributed Deep Learning fast and easy to use.    deep learning  py-keras  - 2.0.8_py27 - 2.1.5_py36 - 2.1.5_py27  Website  Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.    deep learning  py-onnx  - 1.0.1_py27  Website  ONNX is a open format to represent deep learning models.    deep learning  py-pytorch  - 0.2.0_py27 - 0.2.0_py36 - 0.3.0_py36 - 0.3.0_py27  Website  PyTorch is a deep learning framework that puts Python first.    deep learning  py-tensorflow  - 1.4.0_py27 - 1.5.0_py27 - 1.5.0_py36 - 1.6.0_py36 - 1.7.0_py27 - 1.6.0_py27  Website  TensorFlow\u2122 is an open source software library for numerical computation using data flow graphs.    deep learning  py-tensorlayer  - 1.6.3_py27  Website  TensorLayer is a Deep Learning (DL) and Reinforcement Learning (RL) library extended from Google TensorFlow.    deep learning  py-theano  - 1.0.1_py27  Website  Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently.    deep learning  tensorrt  - 3.0.1 - 3.0.4  Website  NVIDIA TensorRT\u2122 is a high-performance deep learning inference optimizer and runtime that delivers low latency, high-throughput inference for deep learning applications.    deep learning  torch  - 20180202  Website  Torch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first.    linear algebra  armadillo  - 8.200.1  Website  Armadillo is a high quality linear algebra library (matrix maths) for the C++ language, aiming towards a good balance between speed and ease of use.    machine learning  py-scikit-learn  - 0.19.1_py27  Website  Scikit-learn is a free software machine learning library for the Python programming language.    numerical analysis  matlab  - R2017a - R2017b  Website  MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and fourth-generation programming language.    numerical analysis  octave  - 4.2.1  Website  GNU Octave is a high-level language primarily intended for numerical computations.    numerical library  arpack  - 3.5.0  Website  Collection of Fortran77 subroutines designed to solve large scale eigenvalue problems.    numerical library  fftw  - 3.3.6  Website  The Fastest Fourier Transform in the West (FFTW) is a software library for computing discrete Fourier transforms (DFTs).    numerical library  glpk  - 4.63  Website  The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems.    numerical library  gmp  - 6.1.2  Website  GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating-point numbers.    numerical library  gsl  - 1.16 - 2.3  Website  The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting.    numerical library  harminv  - 1.4.1  Website  harminv is a program designed to solve the problem of harmonic inversion: given a time series consisting of a sum of sinusoids (modes), extract their frequencies and amplitudes.    numerical library  imkl  - 2017.u2 - 2018.u1 - 2018  Website  Intel Math Kernel Library (Intel MKL) is a library of optimized math routines for science, engineering, and financial applications. Core math functions include BLAS, LAPACK, ScaLAPACK, sparse solvers, fast Fourier transforms, and vector math.[3] The routines in MKL are hand-optimized specifically for Intel processors    numerical library  libxsmm  - 1.8.1  Website  LIBXSMM is a library for small dense and small sparse matrix-matrix multiplications as well as for deep learning primitives such as small convolutions    numerical library  metis  - 5.1.0  Website  METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices.    numerical library  mpfr  - 3.1.5  Website  The MPFR library is a C library for multiple-precision floating-point computations with correct rounding.    numerical library  mumps  - 5.1.2  Website  A parallel sparse direct solver.    numerical library  nagcl  - cll6i26dcl  Website  The NAG C Library is the largest and most comprehensive collection of mathematical and statistical algorithms for C and C++.    numerical library  nagfl  - fll6i26dcl  Website  The NAG Fortran Library is the largest and most comprehensive collection of numerical and statistical algorithms in Fortran.    numerical library  nagfs  - fsl6i26dcl  Website  The NAG Library for SMP   Multicore is based on, and includes, the full functionality of the NAG Fortran Library.    numerical library  nagmb  - MBL6I25DNL  Website  The NAG C Library is the largest and most comprehensive collection of mathematical and statistical algorithms for C and C++.    numerical library  openblas  - 0.2.19  Website  OpenBLAS is an optimized BLAS library    numerical library  parmetis  - 4.0.3  Website  ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices.    numerical library  py-pyublas  - 2017.1_py27  Website  PyUblas provides a seamless glue layer between Numpy and Boost.Ublas for use with Boost.Python.    numerical library  qrupdate  - 1.1.2  Website  qrupdate is a Fortran library for fast updates of QR and Cholesky decompositions.    numerical library  scalapack  - 2.0.2  Website  ScaLAPACK is a library of high-performance linear algebra routines for parallel distributed memory machines.    numerical library  scotch  - 6.0.4  Website  Software package and libraries for sequential and parallel graph partitioning, static mapping and clustering, sequential mesh and hypergraph partitioning, and sequential and parallel sparse matrix block ordering.    numerical library  superlu  - 5.2.1  Website  SuperLU is a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations.    numerical library  xblas  - 1.0.248  Website  Extra precise basic linear algebra subroutines.    optimization  gurobi  - 7.5.1  Website  The Gurobi Optimizer is a commercial optimization solver for mathematical programming.    optimization  knitro  - 10.3.0  Website  Artelys Knitro is an optimization solver for difficult large-scale nonlinear problems.    scientific computing  py-scipystack  - 1.0_py36 - 1.0_py27  Website  The SciPy Stack is a collection of open source software for scientific computing in Python. It provides the following packages: numpy, scipy, matplotlib, ipython, jupyter, pandas, sympy and nose.    statistics  R  - 3.4.0  Website  R is a free software environment for statistical computing and graphics.    statistics  jags  - 4.3.0  Website  Just another Gibbs sampler (JAGS) is a program for simulation from Bayesian hierarchical models using Markov chain Monte Carlo (MCMC).    statistics  py-rpy2  - 2.8.6_py27 - 2.9.2_py36  Website  rpy2 is an interface to R running embedded in a Python process.    statistics  r-rstan  - 2.17.3  Website  RStan is the R interface to Stan, an open-source software for facilitating statistical inference at the frontiers of applied statistics.    statistics  rstudio  - 1.1.423  Website  RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.    statistics  sas  - 9.4  Website  SAS is a software suite developed by SAS Institute for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics.    statistics  stata  - 14 - 15  Website  Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics.    symbolic  libmatheval  - 1.1.11  Website  GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text.", 
            "title": "Math"
        }, 
        {
            "location": "/docs/software/list/#physics", 
            "text": "Field  Module name  Version(s)  URL  Description      astronomy  heasoft  - 6.22.1  Website  HEAsoft is a Unified Release of the FTOOLS (General and mission-specific tools to manipulate FITS files) and XANADU (High-level, multi-mission tasks for X-ray astronomical spectral, timing, and imaging data analysis) software packages.    geophysics  opensees  - 2.5.0  Website  OpenSees is a software framework for developing applications to simulate the performance of structural and geotechnical systems subjected to earthquakes.    geoscience  gdal  - 2.2.1  Website  GDAL is a translator library for raster and vector geospatial data formats.    geoscience  geos  - 3.6.2  Website  GEOS (Geometry Engine - Open Source) is a C++ port of Java Topology Suite (JTS).    geoscience  proj  - 4.9.3  Website  proj.4 is a standard UNIX filter function which converts geographic longitude and latitude coordinates into cartesian coordinates (and vice versa.    geoscience  udunits  - 2.2.26  Website  The UDUNITS package from Unidata is a C-based package for the programatic handling of units of physical quantities.    photonics  meep  - 1.3  Website  Meep is a free finite-difference time-domain (FDTD) simulation software package to model electromagnetic systems.    photonics  mpb  - 1.5  Website  MPB is a free software package for computing the band structures, or dispersion relations, and electromagnetic modes of periodic dielectric structures, on both serial and parallel computers.", 
            "title": "Physics"
        }, 
        {
            "location": "/docs/software/list/#system", 
            "text": "Field  Module name  Version(s)  URL  Description      becnhmark  hp2p  - 3.2  Website  Heavy Peer To Peer: a MPI based benchmark for network diagnostic.    compression  lz4  - 1.8.0  Website  LZ4 is lossless compression algorithm.    compression  lzo  - 2.10  Website  LZO is a portable lossless data compression library written in ANSI C.    compression  szip  - 2.1.1  Website  Szip compression software, providing lossless compression of scientific data, is an implementation of the extended-Rice lossless compression algorithm.    compression  xz  - 5.2.3  Website  XZ Utils is free general-purpose data compression software with a high compression ratio.    compression  zlib  - 1.2.11  Website  zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system.    containers  singularity  - 2.4.6  Website  Singularity is a container framework that enables users to package entire scientific workflows, software and libraries.    database  bdb  - 6.2.32  Website  Berkeley DB (BDB) is a software library intended to provide a high-performance embedded database for key/value data.    database  mariadb  - 10.2.11  Website  MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL.    database  sqlite  - 3.18.0  Website  SQLite is a self-contained, high-reliability, embedded, full-featured, public-domain, SQL database engine.    file management  fpart  - 0.9.3  Website  fpart sorts files and packs them into partitions.    file transfer  gdrive  - 2.1.0  Website  gdrive is a command line utility for interacting with Google Drive.    file transfer  lftp  - 4.8.1  Website  LFTP is a sophisticated file transfer program supporting a number of network protocols (ftp, http, sftp, fish, torrent).    file transfer  mpifileutils  - 20170210  Website  mpiFileUtils is a suite of MPI-based tools to manage large datasets, which may vary from large directory trees to large files.    file transfer  py-globus-cli  - 1.2.0  Website  A command line wrapper over the Globus SDK for Python.    file transfer  rclone  - 1.39  Website  Rclone is a command line program to sync files and directories to and from: Google Drive, Amazon S3, Dropbox, Google Cloud Storage, Amazon Drive, Microsoft One Drive, Hubic, Backblaze B2, Yandex Disk, or the local filesystem.    language  tcltk  - 8.6.6  Website  Tcl (Tool Command Language) is a dynamic programming language, suitable for web and desktop applications, networking, administration, testing. Tk is a graphical user interface toolkit.    libs  apr  - 1.6.3  Website  The Apache Portable Runtime is a supporting library for the Apache web server. It provides a set of APIs that map to the underlying operating system.    libs  apr-util  - 1.6.1  Website  The Apache Portable Runtime is a supporting library for the Apache web server. It provides a set of APIs that map to the underlying operating system.    libs  atk  - 2.24.0  Website  ATK is the Accessibility Toolkit. It provides a set of generic interfaces allowing accessibility technologies such as screen readers to interact with a graphical user interface.    libs  benchmark  - 1.2.0  Website  A microbenchmark support library    libs  cairo  - 1.14.10  Website  Cairo is a 2D graphics library with support for multiple output devices.    libs  cups  - 2.2.4  Website  CUPS is the standards-based, open source printing system.    libs  dbus  - 1.10.22  Website  D-Bus is a message bus system, a simple way for applications to talk to one another.    libs  enchant  - 1.6.1 - 2.2.3  Website  Enchant is a library (and command-line program) that wraps a number of different spelling libraries and programs with a consistent interface.    libs  fltk  - 1.3.4  Website  FLTK (pronounced 'fulltick') is a cross-platform C++ GUI toolkit.    libs  fontconfig  - 2.12.4  Website  Fontconfig is a library for configuring and customizing font access.    libs  freeglut  - 3.0.0  Website  FreeGLUT is a free-software/open-source alternative to the OpenGL Utility Toolkit (GLUT) library.    libs  freetype  - 2.8  Website  FreeType is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images).    libs  gc  - 7.6.0  Website  The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new.    libs  gconf  - 2.9.91  Website  GConf is a system for storing application preferences.    libs  gdk-pixbuf  - 2.36.8  Website  The GdkPixbuf library provides facilities for loading images in a variety of file formats.    libs  gflags  - 2.2.1  Website  The gflags package contains a C++ library that implements commandline flags processing.    libs  giflib  - 5.1.4  Website  GIFLIB is a package of portable tools and library routines for working with GIF images.    libs  glib  - 2.52.3  Website  The GLib library provides core non-graphical functionality such as high level data types, Unicode manipulation, and an object and type system to C programs.    libs  glog  - 0.3.5  Website  C++ implementation of the Google logging module.    libs  gnutls  - 3.5.9  Website  GnuTLS is a secure communications library implementing the SSL, TLS and DTLS protocols and technologies around them.    libs  gobject-introspection  - 1.52.1  Website  GObject introspection is a middleware layer between C libraries (using GObject) and language bindings.    libs  googletest  - 1.8.0  Website  Google Test is Google's C++ test framework.    libs  gtk+  - 2.24.30 - 3.22.18  Website  GTK+, or the GIMP Toolkit, is a multi-platform toolkit for creating graphical user interfaces.    libs  harfbuzz  - 1.4.8  Website  HarfBuzz is an OpenType text shaping engine.    libs  hunspell  - 1.6.2  Website  Hunspell is a spell checker.    libs  hyphen  - 2.8.8  Website  Hyphen is a hyphenation library to use converted TeX hyphenation patterns.    libs  icu  - 59.1  Website  ICU is a set of C/C++ and Java libraries providing Unicode and Globalization support for software applications.    libs  libepoxy  - 1.4.1  Website  Epoxy is a library for handling OpenGL function pointer management for you.    libs  libffi  - 3.2.1  Website  libffi is a portable Foreign Function Interface library.    libs  libgcrypt  - 1.8.2  Website  Libgcrypt is a general purpose cryptographic library originally based on code from GnuPG.    libs  libgd  - 2.2.5  Website  GD is an open source code library for the dynamic creation of images by programmers.    libs  libgpg-error  - 1.27  Website  Libgpg-error is a small library that originally defined common error values for all GnuPG components.    libs  libidl  - 0.8.14  Website  The libIDL package contains libraries for Interface Definition Language files. This is a specification for defining portable interfaces.    libs  libjpeg-turbo  - 1.5.1  Website  libjpeg-turbo is a JPEG image codec that uses SIMD instructions (MMX, SSE2, AVX2, NEON, AltiVec) to accelerate baseline JPEG compression and decompression on x86, x86-64, ARM, and PowerPC systems    libs  libmng  - 2.0.3  Website  THE reference library for reading, displaying, writing and examining Multiple-Image Network Graphics. MNG is the animation extension to the popular PNG image-format.    libs  libpng  - 1.2.57 - 1.6.29  Website  libpng is the official PNG reference library. It supports almost all PNG features, is extensible, and has been extensively tested for over 20 years.    libs  libproxy  - 0.4.15  Website  libproxy is a library that provides automatic proxy configuration management.    libs  libressl  - 2.5.3  Website  LibreSSL is a version of the TLS/crypto stack forked from OpenSSL in 2014, with goals of modernizing the codebase, improving security, and applying best practice development processes.    libs  libsoup  - 2.61.2  Website  libsoup is an HTTP client/server library for GNOME.    libs  libtasn1  - 4.13  Website  Libtasn1 is the ASN.1 library used by GnuTLS, p11-kit and some other packages.    libs  libtiff  - 4.0.8  Website  libtiff provides support for the Tag Image File Format (TIFF), a widely used format for storing image data.    libs  libunistring  - 0.9.7  Website  Libunistring provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard.    libs  libwebp  - 0.6.1  Website  WebP is a modern image format that provides superior lossless and lossy compression for images on the web.    libs  libxml2  - 2.9.4  Website  Libxml2 is a XML C parser and toolkit.    libs  libxslt  - 1.1.32  Website  Libxslt is the XSLT C library developed for the GNOME project. XSLT itself is a an XML language to define transformation for XML.    libs  mesa  - 17.1.6  Website  Mesa is an open-source implementation of the OpenGL, Vulkan and other specifications.    libs  ncurses  - 6.0  Website  The ncurses (new curses) library is a free software emulation of curses in System V Release 4.0 (SVr4), and more.    libs  nettle  - 3.3  Website  Nettle is a cryptographic library that is designed to fit easily in more or less any context.    libs  orbit  - 2.14.19  Website  ORBit2 is a CORBA 2.4-compliant Object Request Broker (ORB) featuring mature C, C++ and Python bindings.    libs  pango  - 1.40.10  Website  Pango is a library for laying out and rendering of text, with an emphasis on internationalization.    libs  pcre  - 8.40  Website  The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5.    libs  popt  - 1.16  Website  Library for parsing command line options.    libs  py-lmdb  - 0.93  Website  Universal Python binding for the LMDB 'Lightning' Database.    libs  py-mako  - 1.0.7_py36 - 1.0.7_py27  Website  Mako is a template library written in Python. It provides a familiar, non-XML syntax which compiles into Python modules for maximum performance.    libs  py-pyqt5  - 5.9.1_py36  Website  PyQt5 is a comprehensive set of Python bindings for Qt v5.    libs  readline  - 7.0  Website  The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in.    libs  snappy  - 1.1.7  Website  A fast compressor/decompressor.    resource monitoring  remora  - 1.8.2  Website  Remora is a tool to monitor runtime resource utilization.    scm  git  - 2.12.2  Website  Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.    scm  git-lfs  - 2.4.0  Website  Git Large File Storage (LFS) replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server.    scm  subversion  - 1.9.7  Website  Subversion is an open source version control system.    tools  curl  - 7.54.0  Website  curl is an open source command line tool and library for transferring data with URL syntax.    tools  expat  - 2.2.3  Website  Expat is a stream-oriented XML parser library written in C.    tools  graphicsmagick  - 1.3.26  Website  GraphicsMagick is the swiss army knife of image processing.    tools  imagemagick  - 7.0.7-2  Website  ImageMagick is a free and open-source software suite for displaying, converting, and editing raster image and vector image files.    tools  leveldb  - 1.20  Website  Symas LMDB is an extraordinarily fast, memory-efficient database we developed for the Symas OpenLDAP Project.    tools  lmdb  - 0.9.21  Website  Symas LMDB is an extraordinarily fast, memory-efficient database we developed for the Symas OpenLDAP Project.    tools  motif  - 2.3.7  Website  Motif is the toolkit for the Common Desktop Environment.    tools  parallel  - 20180122  Website  GNU parallel is a shell tool for executing jobs in parallel using one or more computers.    tools  qt  - 5.9.1  Website  QT is a cross-platform application framework that is used for developing application software that can be run on various software and hardware platforms.    tools  rocksdb  - 5.7.3  Website  A library that provides an embeddable, persistent key-value store for fast storage.    tools  x11  - 7.7  Website  The X.Org project provides an open source implementation of the X Window System.    tools  xkeyboard-config  - 2.21  Website  The non-arch keyboard configuration database for X Window.", 
            "title": "System"
        }, 
        {
            "location": "/docs/software/list/#viz", 
            "text": "Field  Module name  Version(s)  URL  Description      graphs  graphviz  - 2.40.1  Website  Graphviz is open source graph visualization software.    molecular visualization  pymol  - 1.8.6.2  Website  PyMOL is a Python-enhanced molecular graphics tool.    plotting  gnuplot  - 5.2.0  Website  Gnuplot is a portable command-line driven graphing utility for Linux, OS/2, MS Windows, OSX, VMS, and many other platforms.    plotting  py-matplotlib  - 2.1.0_py27 - 2.1.2_py36 - 2.0.2_py27  Website  Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.    plotting  py-plotly  - 2.4.1_py27  Website  Plotly's Python graphing library makes interactive, publication-quality graphs online.    remote display  virtualgl  - 2.5.2  Website  VirtualGL is an open source toolkit that gives any Unix or Linux remote display software the ability to run OpenGL applications with full 3D hardware acceleration.", 
            "title": "Viz"
        }, 
        {
            "location": "/docs/software/modules/", 
            "text": "Environment modules\n#\n\n\nSoftware is provided on Sherlock under the form of loadable \nenvironment\nmodules\n.\n\n\n\n\nSoftware is only accessible via modules\n\n\nThe use of a module system means that most software is not accessible by\ndefault and has to be loaded using the \nmodule\n command. This mechanism\nallows us to provide multiple versions of the same software concurrently,\nand gives users the possibility to easily switch between software versions.\n\n\n\n\nSherlock uses \nLmod\n to manage software installations. The modules\nsystem helps setting up the user's shell environment to give access to\napplications, and make running and compiling software easier. It also allows us\nto provide multiple versions of the same software, that would otherwise\nconflict with each other, and abstract things from the OS sometimes rigid\nversions and dependencies.\n\n\nWhen you first log into Sherlock, you'll be presented with a default, bare bone\nenvironment with minimal software available. The module system is used to\nmanage the user environment and to \nactivate\n software packages on demand.\nIn order to use software installed on Sherlock, you must first load the\ncorresponding software module.\n\n\nWhen you load a module, the system will set or modify your user environment\nvariables to enable access to the software package provided by that module. For\ninstance, the \n$PATH\n environment variable might be updated so that appropriate\nexecutables for that package can be used.\n\n\nModule categories\n#\n\n\nModules on Sherlock are organized by scientific field, in distinct categories.\nThis is to limit the information overload that can result when displaying the\nfull list of available modules. Given the large diversity of the Sherlock user\npopulation,  all users are not be interested in the same kind of software, and\nhigh-energy physicists may not want to see their screens cluttered with the\nlatest bioinformatics packages.\n\n\n\n\nModule categories\n\n\nYou will first have to load a category module before getting access to\nindividual modules. The \nmath\n and \ndevel\n categories are loaded by\ndefault, and modules in those categories can be loaded directly\n\n\n\n\nFor instance, to be able to load the \ngromacs\n module, you'll first need to\nload the \nchemistry\n module. This can be done in a single command, by\nspecifying first the category, then the actual application module name:\n\n$ module load chemistry gromacs\n\n\nThe \nmath\n and \ndevel\n categories, which are loaded by default, provide direct\naccess to compilers, languages, and MPI and numerical libraries.\n\n\nFor a complete list of software odule categories, please refer to the \nlist of\navailable software\n\n\n\n\nSearching for a module\n\n\nTo know how to access a module, you can use the \nmodule spider\n\nmodule_name\n command. It will search through all the installed modules,\neven if they're masked, and display instructions to load them. See the\n\nExamples\n section for details.\n\n\n\n\nModule usage\n#\n\n\nThe most common \nmodule\n commands are outlined in the following table.\n\nmodule\n commands may be shortened with the \nml\n alias, with slightly different\nsemantics.\n\n\n\n\nModule names auto-completion\n\n\nThe \nmodule\n command supports auto-completion, so you can just start typing\nthe name of a module, and press \nTab\n to let the shell automatically\ncomplete the module name and/or version.\n\n\n\n\n\n\n\n\n\n\nModule\ncommand\n\n\nShort\nversion\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmodule avail\n\n\nml av\n\n\nList\navailable\nsoftware\n1\n\n\n\n\n\n\nmodule spider gromacs\n\n\nml spider gromacs\n\n\nSearch for particular software\n\n\n\n\n\n\nmodule keyword blas\n\n\nml key blas\n\n\nSearch for \nblas\n in module names and descriptions\n\n\n\n\n\n\nmodule whatis gcc\n\n\nml whatis gcc\n\n\nDisplay information about the \ngcc\n module\n\n\n\n\n\n\nmodule help gcc\n\n\nml help gcc\n\n\nDisplay module specific help\n\n\n\n\n\n\nmodule load gcc\n\n\nml gcc\n\n\nLoad a module to use the associated software\n\n\n\n\n\n\nmodule load gsl/2.3\n\n\nml gsl/2.3\n\n\nLoad specific version of a module\n\n\n\n\n\n\nmodule unload gcc\n\n\nml -gcc\n\n\nUnload a module\n\n\n\n\n\n\nmodule swap gcc icc\n\n\nml -gcc icc\n\n\nSwap a module (unload \ngcc\n and replace it with \nicc\n)\n\n\n\n\n\n\nmodule purge\n\n\nml purge\n\n\nRemove all modules\n2\n\n\n\n\n\n\nmodule save foo\n\n\nml save foo\n\n\nSave the state of all loaded modules in a collection named \nfoo\n\n\n\n\n\n\nmodule restore foo\n\n\nml restore foo\n\n\nRestore the state of saved modules from the \nfoo\n collection\n\n\n\n\n\n\n\n\nAdditional module sub-commands are documented in the \nmodule help\n command. For\ncomplete reference, please refer to the \nofficial Lmod\ndocumentation\n.\n\n\nModule properties\n#\n\n\n\n\nMultiple versions\n\n\nWhen multiple versions of the same module exist, \nmodule\n will load the one\nmarked as \nDefault (D)\n. For the sake of reproducibility, we recommend\nalways specifying the module version you want to load, as defaults may\nevolve over time.\n\n\n\n\nTo quickly see some of the modules characteristics, \nmodule avail\n will display\ncolored property attributes next to the module names. The main module\nproperties are:\n\n\n\n\nS\n: Module is sticky, requires \n--force\n to unload or purge\n\n\nL\n: Indicate currently loaded module\n\n\nD\n: Default module that will be loaded when multiple versions are available\n\n\nr\n: Restricted access, typically software under license.  \nContact\n  us\n for details\n\n\ng\n: GPU-accelerated software, will only run on GPU nodes\n\n\nm\n: Software supports parallel execution using MPI\n\n\n\n\nSearching for modules\n#\n\n\nYou can search through all the available modules for either:\n\n\n\n\na module name (if you already know it), using \nmodule spider\n\n\nany string within modules names and descriptions, using \nmodule keyword\n\n\n\n\nFor instance, if you want to know how to load the \ngromacs\n module, you can do:\n\n$ module spider gromacs\n\n\nIf you don't know the module name, or want to list all the modules that contain\na specific string of characters in their name or description, you can use\n\nmodule keyword\n. For instance, the following command will list all the modules\nproviding a BLAS library:\n\n$ module keyword blas\n\n\nExamples\n#\n\n\nListing\n#\n\n\nTo list all the modules that can be loaded, you can do:\n\n\n$ ml av\n\n-- math -- numerical libraries, statistics, deep-learning, computer science ---\n   R/3.4.0             gsl/1.16             openblas/0.2.19\n   cudnn/5.1  (g)      gsl/2.3       (D)    py-scipystack/1.0_py27 (D)\n   cudnn/6.0  (g,D)    imkl/2017.u2         py-scipystack/1.0_py36\n   fftw/3.3.6          matlab/R2017a (r)\n\n------------------ devel -- compilers, MPI, languages, libs -------------------\n   boost/1.64.0          icc/2017.u2           python/2.7.13    (D)\n   cmake/3.8.1           ifort/2017.u2         python/3.6.1\n   cuda/8.0.61    (g)    impi/2017.u2   (m)    scons/2.5.1_py27 (D)\n   eigen/3.3.3           java/1.8.0_131        scons/2.5.1_py36\n   gcc/6.3.0      (D)    julia/0.5.1           sqlite/3.18.0\n   gcc/7.1.0             llvm/4.0.0            tbb/2017.u2\n   h5utils/1.12.1        nccl/1.3.4     (g)    tcltk/8.6.6\n   hdf5/1.10.0p1         openmpi/2.0.2  (m)\n\n-------------- categories -- load to make more modules available --------------\n   biology      devel (S,L)    physics    system\n   chemistry    math  (S,L)    staging    viz\n\n  Where:\n   S:  Module is Sticky, requires --force to unload or purge\n   r:  Restricted access\n   g:  GPU support\n   L:  Module is loaded\n   m:  MPI support\n   D:  Default Module\n\nUse \nmodule spider\n to find all possible modules.\nUse \nmodule keyword key1 key2 ...\n to search for all possible modules matching\nany of the \nkeys\n.\n\n\n\nSearching\n#\n\n\nTo search for a specific string in modules names and descriptions, you can run:\n\n\n$ module keyword numpy\n---------------------------------------------------------------------------\n\nThe following modules match your search criteria: \nnumpy\n\n---------------------------------------------------------------------------\n\n  py-scipystack: py-scipystack/1.0_py27, py-scipystack/1.0_py36\n    The SciPy Stack is a collection of open source software for scientific\n    computing in Python. It provides the following packages: numpy, scipy,\n    matplotlib, ipython, jupyter, pandas, sympy and nose.\n\n---------------------------------------------------------------------------\n[...]\n$ ml key compiler\n---------------------------------------------------------------------------\n\nThe following modules match your search criteria: \ncompiler\n\n---------------------------------------------------------------------------\n\n  cmake: cmake/3.8.1\n    CMake is an extensible, open-source system that manages the build\n    process in an operating system and in a compiler-independent manner.\n\n  gcc: gcc/6.3.0, gcc/7.1.0\n    The GNU Compiler Collection includes front ends for C, C++, Fortran,\n    Java, and Go, as well as libraries for these languages (libstdc++,\n    libgcj,...).\n\n  icc: icc/2017.u2\n    Intel C++ Compiler, also known as icc or icl, is a group of C and C++\n    compilers from Intel\n\n  ifort: ifort/2017.u2\n    Intel Fortran Compiler, also known as ifort, is a group of Fortran\n    compilers from Intel\n\n  llvm: llvm/4.0.0\n    The LLVM Project is a collection of modular and reusable compiler and\n    toolchain technologies. Clang is an LLVM native C/C++/Objective-C\n    compiler,\n\n---------------------------------------------------------------------------\n\nTo get information about a specific module, especially how to load it, the\nfollowing command can be used:\n\n$ module spider gromacs\n\n-------------------------------------------------------------------------------\n  gromacs: gromacs/2016.3\n-------------------------------------------------------------------------------\n    Description:\n      GROMACS is a versatile package to perform molecular dynamics, i.e.\n      simulate the Newtonian equations of motion for systems with hundreds to\n      millions of particles.\n\n    Properties:\n      GPU support      MPI support\n\n    You will need to load all module(s) on any one of the lines below before\n    the \ngromacs/2016.3\n module is available to load.\n\n      chemistry\n\n\nLoading\n#\n\n\nLoading a category module allows to get access to field-specific software:\n\n\n$ ml chemistry\n$ ml av\n\n------------- chemistry -- quantum chemistry, molecular dynamics --------------\n   gromacs/2016.3 (g,m)    vasp/5.4.1 (g,r,m)\n\n-- math -- numerical libraries, statistics, deep-learning, computer science ---\n   R/3.4.0             gsl/1.16             openblas/0.2.19\n   cudnn/5.1  (g)      gsl/2.3       (D)    py-scipystack/1.0_py27 (D)\n   cudnn/6.0  (g,D)    imkl/2017.u2         py-scipystack/1.0_py36\n   fftw/3.3.6          matlab/R2017a (r)\n\n------------------ devel -- compilers, MPI, languages, libs -------------------\n   boost/1.64.0          icc/2017.u2           python/2.7.13    (D)\n   cmake/3.8.1           ifort/2017.u2         python/3.6.1\n   cuda/8.0.61    (g)    impi/2017.u2   (m)    scons/2.5.1_py27 (D)\n   eigen/3.3.3           java/1.8.0_131        scons/2.5.1_py36\n   gcc/6.3.0      (D)    julia/0.5.1           sqlite/3.18.0\n   gcc/7.1.0             llvm/4.0.0            tbb/2017.u2\n   h5utils/1.12.1        nccl/1.3.4     (g)    tcltk/8.6.6\n   hdf5/1.10.0p1         openmpi/2.0.2  (m)\n\n-------------- categories -- load to make more modules available --------------\n   biology          devel (S,L)    physics    system\n   chemistry (L)    math  (S,L)    staging    viz\n\n[...]\n\n\n\nReseting the modules environment\n#\n\n\nIf you want to reset your modules environment as it was when you initially\nconnected to Sherlock, you can use the \nml reset\n command: it will remove all\nthe modules you have loaded, and restore the original state where only the\n\nmath\n and \ndevel\n categories are accessible.\n\n\nIf you want to remove \nall\n modules from your environment, including the\ndefault \nmath\n and \ndevel\n modules, you can use \nml --force purge\n.\n\n\nLoading modules in jobs\n#\n\n\nIn order for an application running in a Slurm job to have access to any\nnecessary module-provided software packages, we recommend loading those modules\nin the job script directly. Since Slurm propagates all user environment\nvariables by default, this is not strictly necessary, as jobs will inherit the\nmodules loaded at submission time. But to make sure things are reproducible and\navoid issues, it is preferable to explicitly load the modules in the batch\nscripts.\n\n\nmodule load\n commands should be placed right after \n#SBATCH\n directives and\nbefore the actual executable calls. For instance:\n\n\n#!/bin/bash\n# SBATCH ...\n# SBATCH ...\n# SBATCH ...\n\nml reset\nml load gromacs/2016.3\n\nsrun gmx_mpi ...\n\n\n\nCustom modules\n#\n\n\nUsers are welcome and encouraged to build and install their own software on\nSherlock. To that end, and to facilitate usage or sharing of their custom\nsoftware installations, they can create their own module repositories.\n\n\nSee the \nSoftware Installation page\n for more details.\n\n\n\n\n\n\n\n\n\n\nIf a module is not listed here, it might be unavailable in the\n  loaded modules categories, and require loading another category module.\n  Search for not-listed software using the \nmodule spider\n command.\n\n\n\n\n\n\nThe \nmath\n and \ndevel\n category modules will not be unloaded\n  with \nmodule purge\n as they are \"sticky\". If a user wants to unload a sticky\n  module, they must specify the \n--force\n option.", 
            "title": "Modules"
        }, 
        {
            "location": "/docs/software/modules/#environment-modules", 
            "text": "Software is provided on Sherlock under the form of loadable  environment\nmodules .   Software is only accessible via modules  The use of a module system means that most software is not accessible by\ndefault and has to be loaded using the  module  command. This mechanism\nallows us to provide multiple versions of the same software concurrently,\nand gives users the possibility to easily switch between software versions.   Sherlock uses  Lmod  to manage software installations. The modules\nsystem helps setting up the user's shell environment to give access to\napplications, and make running and compiling software easier. It also allows us\nto provide multiple versions of the same software, that would otherwise\nconflict with each other, and abstract things from the OS sometimes rigid\nversions and dependencies.  When you first log into Sherlock, you'll be presented with a default, bare bone\nenvironment with minimal software available. The module system is used to\nmanage the user environment and to  activate  software packages on demand.\nIn order to use software installed on Sherlock, you must first load the\ncorresponding software module.  When you load a module, the system will set or modify your user environment\nvariables to enable access to the software package provided by that module. For\ninstance, the  $PATH  environment variable might be updated so that appropriate\nexecutables for that package can be used.", 
            "title": "Environment modules"
        }, 
        {
            "location": "/docs/software/modules/#module-categories", 
            "text": "Modules on Sherlock are organized by scientific field, in distinct categories.\nThis is to limit the information overload that can result when displaying the\nfull list of available modules. Given the large diversity of the Sherlock user\npopulation,  all users are not be interested in the same kind of software, and\nhigh-energy physicists may not want to see their screens cluttered with the\nlatest bioinformatics packages.   Module categories  You will first have to load a category module before getting access to\nindividual modules. The  math  and  devel  categories are loaded by\ndefault, and modules in those categories can be loaded directly   For instance, to be able to load the  gromacs  module, you'll first need to\nload the  chemistry  module. This can be done in a single command, by\nspecifying first the category, then the actual application module name: $ module load chemistry gromacs  The  math  and  devel  categories, which are loaded by default, provide direct\naccess to compilers, languages, and MPI and numerical libraries.  For a complete list of software odule categories, please refer to the  list of\navailable software   Searching for a module  To know how to access a module, you can use the  module spider module_name  command. It will search through all the installed modules,\neven if they're masked, and display instructions to load them. See the Examples  section for details.", 
            "title": "Module categories"
        }, 
        {
            "location": "/docs/software/modules/#module-usage", 
            "text": "The most common  module  commands are outlined in the following table. module  commands may be shortened with the  ml  alias, with slightly different\nsemantics.   Module names auto-completion  The  module  command supports auto-completion, so you can just start typing\nthe name of a module, and press  Tab  to let the shell automatically\ncomplete the module name and/or version.      Module command  Short version  Description      module avail  ml av  List available software 1    module spider gromacs  ml spider gromacs  Search for particular software    module keyword blas  ml key blas  Search for  blas  in module names and descriptions    module whatis gcc  ml whatis gcc  Display information about the  gcc  module    module help gcc  ml help gcc  Display module specific help    module load gcc  ml gcc  Load a module to use the associated software    module load gsl/2.3  ml gsl/2.3  Load specific version of a module    module unload gcc  ml -gcc  Unload a module    module swap gcc icc  ml -gcc icc  Swap a module (unload  gcc  and replace it with  icc )    module purge  ml purge  Remove all modules 2    module save foo  ml save foo  Save the state of all loaded modules in a collection named  foo    module restore foo  ml restore foo  Restore the state of saved modules from the  foo  collection     Additional module sub-commands are documented in the  module help  command. For\ncomplete reference, please refer to the  official Lmod\ndocumentation .", 
            "title": "Module usage"
        }, 
        {
            "location": "/docs/software/modules/#module-properties", 
            "text": "Multiple versions  When multiple versions of the same module exist,  module  will load the one\nmarked as  Default (D) . For the sake of reproducibility, we recommend\nalways specifying the module version you want to load, as defaults may\nevolve over time.   To quickly see some of the modules characteristics,  module avail  will display\ncolored property attributes next to the module names. The main module\nproperties are:   S : Module is sticky, requires  --force  to unload or purge  L : Indicate currently loaded module  D : Default module that will be loaded when multiple versions are available  r : Restricted access, typically software under license.   Contact\n  us  for details  g : GPU-accelerated software, will only run on GPU nodes  m : Software supports parallel execution using MPI", 
            "title": "Module properties"
        }, 
        {
            "location": "/docs/software/modules/#searching-for-modules", 
            "text": "You can search through all the available modules for either:   a module name (if you already know it), using  module spider  any string within modules names and descriptions, using  module keyword   For instance, if you want to know how to load the  gromacs  module, you can do: $ module spider gromacs  If you don't know the module name, or want to list all the modules that contain\na specific string of characters in their name or description, you can use module keyword . For instance, the following command will list all the modules\nproviding a BLAS library: $ module keyword blas", 
            "title": "Searching for modules"
        }, 
        {
            "location": "/docs/software/modules/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/docs/software/modules/#listing", 
            "text": "To list all the modules that can be loaded, you can do:  $ ml av\n\n-- math -- numerical libraries, statistics, deep-learning, computer science ---\n   R/3.4.0             gsl/1.16             openblas/0.2.19\n   cudnn/5.1  (g)      gsl/2.3       (D)    py-scipystack/1.0_py27 (D)\n   cudnn/6.0  (g,D)    imkl/2017.u2         py-scipystack/1.0_py36\n   fftw/3.3.6          matlab/R2017a (r)\n\n------------------ devel -- compilers, MPI, languages, libs -------------------\n   boost/1.64.0          icc/2017.u2           python/2.7.13    (D)\n   cmake/3.8.1           ifort/2017.u2         python/3.6.1\n   cuda/8.0.61    (g)    impi/2017.u2   (m)    scons/2.5.1_py27 (D)\n   eigen/3.3.3           java/1.8.0_131        scons/2.5.1_py36\n   gcc/6.3.0      (D)    julia/0.5.1           sqlite/3.18.0\n   gcc/7.1.0             llvm/4.0.0            tbb/2017.u2\n   h5utils/1.12.1        nccl/1.3.4     (g)    tcltk/8.6.6\n   hdf5/1.10.0p1         openmpi/2.0.2  (m)\n\n-------------- categories -- load to make more modules available --------------\n   biology      devel (S,L)    physics    system\n   chemistry    math  (S,L)    staging    viz\n\n  Where:\n   S:  Module is Sticky, requires --force to unload or purge\n   r:  Restricted access\n   g:  GPU support\n   L:  Module is loaded\n   m:  MPI support\n   D:  Default Module\n\nUse  module spider  to find all possible modules.\nUse  module keyword key1 key2 ...  to search for all possible modules matching\nany of the  keys .", 
            "title": "Listing"
        }, 
        {
            "location": "/docs/software/modules/#searching", 
            "text": "To search for a specific string in modules names and descriptions, you can run:  $ module keyword numpy\n---------------------------------------------------------------------------\n\nThe following modules match your search criteria:  numpy \n---------------------------------------------------------------------------\n\n  py-scipystack: py-scipystack/1.0_py27, py-scipystack/1.0_py36\n    The SciPy Stack is a collection of open source software for scientific\n    computing in Python. It provides the following packages: numpy, scipy,\n    matplotlib, ipython, jupyter, pandas, sympy and nose.\n\n---------------------------------------------------------------------------\n[...]\n$ ml key compiler\n---------------------------------------------------------------------------\n\nThe following modules match your search criteria:  compiler \n---------------------------------------------------------------------------\n\n  cmake: cmake/3.8.1\n    CMake is an extensible, open-source system that manages the build\n    process in an operating system and in a compiler-independent manner.\n\n  gcc: gcc/6.3.0, gcc/7.1.0\n    The GNU Compiler Collection includes front ends for C, C++, Fortran,\n    Java, and Go, as well as libraries for these languages (libstdc++,\n    libgcj,...).\n\n  icc: icc/2017.u2\n    Intel C++ Compiler, also known as icc or icl, is a group of C and C++\n    compilers from Intel\n\n  ifort: ifort/2017.u2\n    Intel Fortran Compiler, also known as ifort, is a group of Fortran\n    compilers from Intel\n\n  llvm: llvm/4.0.0\n    The LLVM Project is a collection of modular and reusable compiler and\n    toolchain technologies. Clang is an LLVM native C/C++/Objective-C\n    compiler,\n\n--------------------------------------------------------------------------- \nTo get information about a specific module, especially how to load it, the\nfollowing command can be used: $ module spider gromacs\n\n-------------------------------------------------------------------------------\n  gromacs: gromacs/2016.3\n-------------------------------------------------------------------------------\n    Description:\n      GROMACS is a versatile package to perform molecular dynamics, i.e.\n      simulate the Newtonian equations of motion for systems with hundreds to\n      millions of particles.\n\n    Properties:\n      GPU support      MPI support\n\n    You will need to load all module(s) on any one of the lines below before\n    the  gromacs/2016.3  module is available to load.\n\n      chemistry", 
            "title": "Searching"
        }, 
        {
            "location": "/docs/software/modules/#loading", 
            "text": "Loading a category module allows to get access to field-specific software:  $ ml chemistry\n$ ml av\n\n------------- chemistry -- quantum chemistry, molecular dynamics --------------\n   gromacs/2016.3 (g,m)    vasp/5.4.1 (g,r,m)\n\n-- math -- numerical libraries, statistics, deep-learning, computer science ---\n   R/3.4.0             gsl/1.16             openblas/0.2.19\n   cudnn/5.1  (g)      gsl/2.3       (D)    py-scipystack/1.0_py27 (D)\n   cudnn/6.0  (g,D)    imkl/2017.u2         py-scipystack/1.0_py36\n   fftw/3.3.6          matlab/R2017a (r)\n\n------------------ devel -- compilers, MPI, languages, libs -------------------\n   boost/1.64.0          icc/2017.u2           python/2.7.13    (D)\n   cmake/3.8.1           ifort/2017.u2         python/3.6.1\n   cuda/8.0.61    (g)    impi/2017.u2   (m)    scons/2.5.1_py27 (D)\n   eigen/3.3.3           java/1.8.0_131        scons/2.5.1_py36\n   gcc/6.3.0      (D)    julia/0.5.1           sqlite/3.18.0\n   gcc/7.1.0             llvm/4.0.0            tbb/2017.u2\n   h5utils/1.12.1        nccl/1.3.4     (g)    tcltk/8.6.6\n   hdf5/1.10.0p1         openmpi/2.0.2  (m)\n\n-------------- categories -- load to make more modules available --------------\n   biology          devel (S,L)    physics    system\n   chemistry (L)    math  (S,L)    staging    viz\n\n[...]", 
            "title": "Loading"
        }, 
        {
            "location": "/docs/software/modules/#reseting-the-modules-environment", 
            "text": "If you want to reset your modules environment as it was when you initially\nconnected to Sherlock, you can use the  ml reset  command: it will remove all\nthe modules you have loaded, and restore the original state where only the math  and  devel  categories are accessible.  If you want to remove  all  modules from your environment, including the\ndefault  math  and  devel  modules, you can use  ml --force purge .", 
            "title": "Reseting the modules environment"
        }, 
        {
            "location": "/docs/software/modules/#loading-modules-in-jobs", 
            "text": "In order for an application running in a Slurm job to have access to any\nnecessary module-provided software packages, we recommend loading those modules\nin the job script directly. Since Slurm propagates all user environment\nvariables by default, this is not strictly necessary, as jobs will inherit the\nmodules loaded at submission time. But to make sure things are reproducible and\navoid issues, it is preferable to explicitly load the modules in the batch\nscripts.  module load  commands should be placed right after  #SBATCH  directives and\nbefore the actual executable calls. For instance:  #!/bin/bash\n# SBATCH ...\n# SBATCH ...\n# SBATCH ...\n\nml reset\nml load gromacs/2016.3\n\nsrun gmx_mpi ...", 
            "title": "Loading modules in jobs"
        }, 
        {
            "location": "/docs/software/modules/#custom-modules", 
            "text": "Users are welcome and encouraged to build and install their own software on\nSherlock. To that end, and to facilitate usage or sharing of their custom\nsoftware installations, they can create their own module repositories.  See the  Software Installation page  for more details.      If a module is not listed here, it might be unavailable in the\n  loaded modules categories, and require loading another category module.\n  Search for not-listed software using the  module spider  command.    The  math  and  devel  category modules will not be unloaded\n  with  module purge  as they are \"sticky\". If a user wants to unload a sticky\n  module, they must specify the  --force  option.", 
            "title": "Custom modules"
        }, 
        {
            "location": "/docs/software/install/", 
            "text": "Software installation requests\n\n\nFor more information about software installation requests, please see \nthe\nSoftware Overview page\n\n\n\n\nIf the software package or version you need is not available in the \nlist of\nprovided software\n, you may compile and install it yourself. The\nrecommended location for user-installed software is the\n\n$PI_HOME\n \ngroup shared directory\n, which is snapshotted and\nreplicated off-site, and can easily be shared with members of a research group.\n\n\n\n\n Work in progress \n\n\nThis page is a work in progress and is not complete yet. We are actively\nworking on adding more content and information.", 
            "title": "Installation"
        }, 
        {
            "location": "/docs/software/using/perl/", 
            "text": "Perl modules in your environment\n#\n\n\nPerl provides a framework allowing users to easily extend the language by\ninstalling new modules in their local environment. The Comprehensive Perl\nArchive Network (CPAN\n1\n) is an archive of over 25,000 distributions of\nsoftware written in Perl, as well as documentation for it. It is searchable at\n\nhttp://metacpan.org\n or \nhttp://search.cpan.org\n and mirrored in over 270\nlocations around the world.\n\n\nTo install Perl modules from CPAN, we recommend using the (provided)\n\nApp::cpanminus\n module and \nlocal::lib\n\nmodules:\n\n\n\n\nApp::cpanminus\n is a popular alternative CPAN client that can be used to\n  manage Perl distributions. It has many great features, including uninstalling\n  modules.\n\n\nlocal::lib\n allows users to install Perl modules in the directory of their\n  choice (typically their home directory) without administrative privileges.\n\n\n\n\nBoth are already installed on Sherlock, and are automatically enabled and\nconfigured when you load the \nperl\n module. You don't need to add anything in\nyour \n~/.bashrc\n file, the Sherlock \nperl\n module will automatically create\neverything that is required so you can directly run a command to install Perl\nmodules locally.\n\n\nInstallation\n#\n\n\n\n\nPerl modules installation is only necessary once\n\n\nYou only need to install Perl modules once on Sherlock. Since fielsystems\nare shared, modules installed on one node will immediately be available on\nall nodes on the cluster.\n\n\n\n\nAs an example, to install the \nDateTime::TimeZone\n module, you can do the\nfollowing:\n\n\n$ ml perl\n$ cpanm DateTime::TimeZone\n\n\n\nUsage\n#\n\n\nOnce installed, you can use the Perl modules directly, no specific options or\nsyntax is required.\n\n\nFor instance, to check that the \nDateTime::TimeZone\n module is correctly\ninstalled:\n\n\n$ perl -MDateTime::TimeZone -e 'print $DateTime::TimeZone::VERSION . \n\\n\n';\n2.13\n\n\n\nUninstallation\n#\n\n\nTo uninstall a Perl module:\n\n\n$ cpanm -U DateTime::TimeZone\n\n\n\n\n\n\n\n\n\n\n\nCPAN can denote either the archive network itself, or the Perl program\n  that acts as an interface to the network and as an automated software\n  installer (somewhat like a package manager). Most software on CPAN is free\n  and open source.", 
            "title": "Perl"
        }, 
        {
            "location": "/docs/software/using/perl/#perl-modules-in-your-environment", 
            "text": "Perl provides a framework allowing users to easily extend the language by\ninstalling new modules in their local environment. The Comprehensive Perl\nArchive Network (CPAN 1 ) is an archive of over 25,000 distributions of\nsoftware written in Perl, as well as documentation for it. It is searchable at http://metacpan.org  or  http://search.cpan.org  and mirrored in over 270\nlocations around the world.  To install Perl modules from CPAN, we recommend using the (provided) App::cpanminus  module and  local::lib \nmodules:   App::cpanminus  is a popular alternative CPAN client that can be used to\n  manage Perl distributions. It has many great features, including uninstalling\n  modules.  local::lib  allows users to install Perl modules in the directory of their\n  choice (typically their home directory) without administrative privileges.   Both are already installed on Sherlock, and are automatically enabled and\nconfigured when you load the  perl  module. You don't need to add anything in\nyour  ~/.bashrc  file, the Sherlock  perl  module will automatically create\neverything that is required so you can directly run a command to install Perl\nmodules locally.", 
            "title": "Perl modules in your environment"
        }, 
        {
            "location": "/docs/software/using/perl/#installation", 
            "text": "Perl modules installation is only necessary once  You only need to install Perl modules once on Sherlock. Since fielsystems\nare shared, modules installed on one node will immediately be available on\nall nodes on the cluster.   As an example, to install the  DateTime::TimeZone  module, you can do the\nfollowing:  $ ml perl\n$ cpanm DateTime::TimeZone", 
            "title": "Installation"
        }, 
        {
            "location": "/docs/software/using/perl/#usage", 
            "text": "Once installed, you can use the Perl modules directly, no specific options or\nsyntax is required.  For instance, to check that the  DateTime::TimeZone  module is correctly\ninstalled:  $ perl -MDateTime::TimeZone -e 'print $DateTime::TimeZone::VERSION .  \\n ';\n2.13", 
            "title": "Usage"
        }, 
        {
            "location": "/docs/software/using/perl/#uninstallation", 
            "text": "To uninstall a Perl module:  $ cpanm -U DateTime::TimeZone      CPAN can denote either the archive network itself, or the Perl program\n  that acts as an interface to the network and as an automated software\n  installer (somewhat like a package manager). Most software on CPAN is free\n  and open source.", 
            "title": "Uninstallation"
        }, 
        {
            "location": "/docs/advanced-topics/connection/", 
            "text": "Advanced connection options\n#\n\n\nLogin nodes\n#\n\n\nSherlock login nodes are regrouped behind a single DNS alias:\n\nlogin.sherlock.stanford.edu\n.\n\n\nThis alias provides a load-balanced login environment, and the assurance that\nyou will be connected to the least loaded login node when you connect to\nSherlock.\n\n\nIf for any reason, you want to directly connect to a specific login node\nand bypass the automatic load-balanced dispatching of new connections\n(which we don't recommend), you can use that login node's hostname\nexplicitly. For instance:\n\n\n$ ssh \nsunetid\n@ln01.sherlock.stanford.edu\n\n\n\n\nThis can be useful if you run long-standing processes on the login nodes, such\nas \nscreen\n or \ntmux\n sessions. To find them back when you\nreconnect to Sherlock, you will indeed need to login to the same login node you\nstarted them on.\n\n\nThe drawback is that by connecting to a specific login node, you will forfeit\nthe load-balancing benefits, which could result in a crowded environment, or\neven in login errors in case that specific login node is unavailable.\n\n\nAuthentication methods\n#\n\n\n\n\nPublic-key authentication\n\n\nSSH public-key authentication is \nnot\n supported on Sherlock.\n\n\n\n\nPassword \n(recommended)\n#\n\n\nThe recommended way to authenticate to Sherlock is to simply use your SUNet ID\nand password, as described in the \nConnecting\n page.\n\n\nPasswords are not stored on Sherlock. Sherlock login nodes will delegate\npassword authentication to the \nUniversity central Kerberos\nservice\n.\n\n\nGSSAPI\n#\n\n\nFor compatibility with previous generations of Sherlock, GSSAPI\n1\n\nauthentication is still allowed, and could be considered a more convenient\noption, as this mechanism doesn't require entering your password for each\nconnection.\n\n\nGSSAPI authentication relies on a token system, where users obtain Kerberos\nticket-granting tickets, transmit them via SSH to the server they want to\nconnect to, which will, in turn, verify their validity. That way, passwords are\nnever stored locally, and never transit over the network. That's why Kerberos\nis usually considered the most secure method to authenticate.\n\n\nTo connect using GSSAPI on Sherlock, you'll need to go through a few\nsteps\n2\n:\n\n\n\n\n\n\nmake sure the Kerberos user tools are installed on your local machine.\n   You'll need the \nkinit\n (and optionally \nklist\n and \nkdestroy\n) utilities.\n   Please refer to your OS documentation to install them if required.\n\n\n\n\n\n\ndownload and install the Stanford \nkrb5.conf\n file, which contains\n   information about the Stanford Kerberos environment:\n\n\n$ sudo curl -o /etc/krb5.conf https://web.stanford.edu/dept/its/support/kerberos/dist/krb5.conf\n\n\n\n\n\n\n\nconfigure your SSH client, by modifying (or creating if it doesn't\n   exist already) the \n.ssh/config\n file in your home directory on your local\n   machine. Using a text editor, you can add the following lines to your\n   \n~/.ssh/config\n file (indentation is important):\n\n\nHost login.sherlock.stanford.edu\n    GSSAPIDelegateCredentials yes\n    GSSAPIAuthentication yes\n\n\n\n\n\n\n\nOnce everything is in place (you only need to do this once), you'll be able to\ntest that your Kerberos installation works by running \nkinit\n\nsunetid\n@stanford.edu\n. You should get a password prompt, and upon success,\nyou'll be able to list your Kerberos credentials with the \nklist\n command:\n\n\n$ kinit kilian@stanford.edu\nPassword for kilian@stanford.edu:\n$ klist\nTicket cache: FILE:/tmp/krb5cc_215845_n4S4I6KgyM\nDefault principal: kilian@stanford.edu\n\nValid starting     Expires            Service principal\n07/28/17 17:33:54  07/29/17 18:33:32  krbtgt/stanford.edu@stanford.edu\n        renew until 08/04/17 17:33:32\n\n\n\n\n\nKerberos ticket expiration\n\n\nKerberos tickets have a 25-hour lifetime. So you'll need to run the \nkinit\n\ncommand pretty much once a day to continue being able to authenticate to\nSherlock.\n\n\n\n\nPlease note that when your Kerberos ticket expire, existing Sherlock\nconnections will \nnot\n be interrupted. So you'll be able to keep connections\nopen to Sherlock for several days without any issue.\n\n\nYou're now ready to connect to Sherlock using GSSAPI. Simply SSH as usual:\n\n\n$ ssh \nsunetid\n@login.sherlock.stanford.edu\n\n\n\nand if everything goes well, you should directly see the two-factor (Duo)\nprompt, without having to enter your password.\n\n\nIf you want to destroy your Kerberos ticket before its expiration, you can use\nthe \nkdestroy\n command.\n\n\nSSH options\n#\n\n\nOpenSSH offers a variety of configuration options that you can use in\n\n~/.ssh/config\n on your local computer. The following section describe some of\nthe options you can use with Sherlock that may make connecting and transferring\nfiles more convenient.\n\n\nAvoiding multiple Duo prompts\n#\n\n\nIn order to avoid getting a second-factor (Duo) prompt every time you want to\nopen a new connection to Sherlock, you can take advantage of the multiplexing\nfeatures provided by OpenSSH.\n\n\nSimply add the following lines to your \n~/.ssh/config\n file on your local\nmachine to activate the \nControlMaster\n option. If you already have a \nHost\nlogin.sherlock.stanford.edu\n block in your configuration file, simply add the\n\nControl*\n option lines in the same block.\n\n\nHost login.sherlock.stanford.edu\n    ControlMaster auto\n    ControlPersist yes\n    ControlPath ~/.ssh/%l%r@%h:%p\n\n\n\nIt will allow SSH to re-use an existing connection to Sherlock each time you\nopen a new session (create a new SSH connection), thus avoiding subsequent 2FA\nprompts once the initial connection is established.\n\n\nThe slight disadvantage of this approach is that once you have a connection\nopen to one of Sherlock's login nodes, all your subsequent connections will be\nusing the same login node. This will somewhat defeat the purpose of the load-balancing mechanism\nused by the login nodes.\n\n\nConnecting from abroad\n#\n\n\n\n\nVPN\n\n\nAs a good security practice, we always recommend to use the \nStanford\nVPN\n when connecting from untrusted networks.\n\n\n\n\nAccess to Sherlock is not restricted to campus, meaning that you can connect to\nSherlock from pretty much anywhere, including when traveling abroad.  We don't\nrestrict inbound SSH connections to any specific IP address range or\ngeographical location, so you shouldn't have any issue to reach the login nodes\nfrom anywhere.\n\n\nRegarding two-step authentication, University IT provides \nalternate\nauthentication options\n when phone service or Duo Mobile push\nnotifications are not available.\n\n\n\n\n\n\n\n\n\n\nThe Generic Security Service Application Program Interface (GSSAPI,\n  also GSS-API) is an application programming interface for programs to access\n  security services. It allows program to interact with security services such\n  as Kerberos for user authentication.\n\n\n\n\n\n\nThose instructions should work on Linux \n and MacOs\n  \n computers. For Windows \n, we recommend using the WSL,\n  as described in the \nPrerequisites\n page.", 
            "title": "Connection"
        }, 
        {
            "location": "/docs/advanced-topics/connection/#advanced-connection-options", 
            "text": "", 
            "title": "Advanced connection options"
        }, 
        {
            "location": "/docs/advanced-topics/connection/#login-nodes", 
            "text": "Sherlock login nodes are regrouped behind a single DNS alias: login.sherlock.stanford.edu .  This alias provides a load-balanced login environment, and the assurance that\nyou will be connected to the least loaded login node when you connect to\nSherlock.  If for any reason, you want to directly connect to a specific login node\nand bypass the automatic load-balanced dispatching of new connections\n(which we don't recommend), you can use that login node's hostname\nexplicitly. For instance:  $ ssh  sunetid @ln01.sherlock.stanford.edu  This can be useful if you run long-standing processes on the login nodes, such\nas  screen  or  tmux  sessions. To find them back when you\nreconnect to Sherlock, you will indeed need to login to the same login node you\nstarted them on.  The drawback is that by connecting to a specific login node, you will forfeit\nthe load-balancing benefits, which could result in a crowded environment, or\neven in login errors in case that specific login node is unavailable.", 
            "title": "Login nodes"
        }, 
        {
            "location": "/docs/advanced-topics/connection/#authentication-methods", 
            "text": "Public-key authentication  SSH public-key authentication is  not  supported on Sherlock.", 
            "title": "Authentication methods"
        }, 
        {
            "location": "/docs/advanced-topics/connection/#password-recommended", 
            "text": "The recommended way to authenticate to Sherlock is to simply use your SUNet ID\nand password, as described in the  Connecting  page.  Passwords are not stored on Sherlock. Sherlock login nodes will delegate\npassword authentication to the  University central Kerberos\nservice .", 
            "title": "Password (recommended)"
        }, 
        {
            "location": "/docs/advanced-topics/connection/#gssapi", 
            "text": "For compatibility with previous generations of Sherlock, GSSAPI 1 \nauthentication is still allowed, and could be considered a more convenient\noption, as this mechanism doesn't require entering your password for each\nconnection.  GSSAPI authentication relies on a token system, where users obtain Kerberos\nticket-granting tickets, transmit them via SSH to the server they want to\nconnect to, which will, in turn, verify their validity. That way, passwords are\nnever stored locally, and never transit over the network. That's why Kerberos\nis usually considered the most secure method to authenticate.  To connect using GSSAPI on Sherlock, you'll need to go through a few\nsteps 2 :    make sure the Kerberos user tools are installed on your local machine.\n   You'll need the  kinit  (and optionally  klist  and  kdestroy ) utilities.\n   Please refer to your OS documentation to install them if required.    download and install the Stanford  krb5.conf  file, which contains\n   information about the Stanford Kerberos environment:  $ sudo curl -o /etc/krb5.conf https://web.stanford.edu/dept/its/support/kerberos/dist/krb5.conf    configure your SSH client, by modifying (or creating if it doesn't\n   exist already) the  .ssh/config  file in your home directory on your local\n   machine. Using a text editor, you can add the following lines to your\n    ~/.ssh/config  file (indentation is important):  Host login.sherlock.stanford.edu\n    GSSAPIDelegateCredentials yes\n    GSSAPIAuthentication yes    Once everything is in place (you only need to do this once), you'll be able to\ntest that your Kerberos installation works by running  kinit sunetid @stanford.edu . You should get a password prompt, and upon success,\nyou'll be able to list your Kerberos credentials with the  klist  command:  $ kinit kilian@stanford.edu\nPassword for kilian@stanford.edu:\n$ klist\nTicket cache: FILE:/tmp/krb5cc_215845_n4S4I6KgyM\nDefault principal: kilian@stanford.edu\n\nValid starting     Expires            Service principal\n07/28/17 17:33:54  07/29/17 18:33:32  krbtgt/stanford.edu@stanford.edu\n        renew until 08/04/17 17:33:32   Kerberos ticket expiration  Kerberos tickets have a 25-hour lifetime. So you'll need to run the  kinit \ncommand pretty much once a day to continue being able to authenticate to\nSherlock.   Please note that when your Kerberos ticket expire, existing Sherlock\nconnections will  not  be interrupted. So you'll be able to keep connections\nopen to Sherlock for several days without any issue.  You're now ready to connect to Sherlock using GSSAPI. Simply SSH as usual:  $ ssh  sunetid @login.sherlock.stanford.edu  and if everything goes well, you should directly see the two-factor (Duo)\nprompt, without having to enter your password.  If you want to destroy your Kerberos ticket before its expiration, you can use\nthe  kdestroy  command.", 
            "title": "GSSAPI"
        }, 
        {
            "location": "/docs/advanced-topics/connection/#ssh-options", 
            "text": "OpenSSH offers a variety of configuration options that you can use in ~/.ssh/config  on your local computer. The following section describe some of\nthe options you can use with Sherlock that may make connecting and transferring\nfiles more convenient.", 
            "title": "SSH options"
        }, 
        {
            "location": "/docs/advanced-topics/connection/#avoiding-multiple-duo-prompts", 
            "text": "In order to avoid getting a second-factor (Duo) prompt every time you want to\nopen a new connection to Sherlock, you can take advantage of the multiplexing\nfeatures provided by OpenSSH.  Simply add the following lines to your  ~/.ssh/config  file on your local\nmachine to activate the  ControlMaster  option. If you already have a  Host\nlogin.sherlock.stanford.edu  block in your configuration file, simply add the Control*  option lines in the same block.  Host login.sherlock.stanford.edu\n    ControlMaster auto\n    ControlPersist yes\n    ControlPath ~/.ssh/%l%r@%h:%p  It will allow SSH to re-use an existing connection to Sherlock each time you\nopen a new session (create a new SSH connection), thus avoiding subsequent 2FA\nprompts once the initial connection is established.  The slight disadvantage of this approach is that once you have a connection\nopen to one of Sherlock's login nodes, all your subsequent connections will be\nusing the same login node. This will somewhat defeat the purpose of the load-balancing mechanism\nused by the login nodes.", 
            "title": "Avoiding multiple Duo prompts"
        }, 
        {
            "location": "/docs/advanced-topics/connection/#connecting-from-abroad", 
            "text": "VPN  As a good security practice, we always recommend to use the  Stanford\nVPN  when connecting from untrusted networks.   Access to Sherlock is not restricted to campus, meaning that you can connect to\nSherlock from pretty much anywhere, including when traveling abroad.  We don't\nrestrict inbound SSH connections to any specific IP address range or\ngeographical location, so you shouldn't have any issue to reach the login nodes\nfrom anywhere.  Regarding two-step authentication, University IT provides  alternate\nauthentication options  when phone service or Duo Mobile push\nnotifications are not available.      The Generic Security Service Application Program Interface (GSSAPI,\n  also GSS-API) is an application programming interface for programs to access\n  security services. It allows program to interact with security services such\n  as Kerberos for user authentication.    Those instructions should work on Linux   and MacOs\n    computers. For Windows  , we recommend using the WSL,\n  as described in the  Prerequisites  page.", 
            "title": "Connecting from abroad"
        }
    ]
}